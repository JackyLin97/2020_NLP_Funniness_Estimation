{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"“Funniness_Estimation_4.5v.ipynb”的副本","provenance":[{"file_id":"1EAfUp-S3MRA_3USHoY4YAjHtMstV45tg","timestamp":1595434377776},{"file_id":"1FNPK0jKMkTKdUjHFdhRie9F70dfSTSlg","timestamp":1594741265423}],"toc_visible":true,"authorship_tag":"ABX9TyNkND/NHo1+e8IEMM9qtyev"},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"92e9968adbe948859c7ad0119edda9f7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a38409d144e6447eb6976d24f9196d27","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b46e2a8fe5fe483594de40c1e5043965","IPY_MODEL_2c544699235b470995bbb58c122eb365"]}},"a38409d144e6447eb6976d24f9196d27":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b46e2a8fe5fe483594de40c1e5043965":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5b7520e06cf643d4b6ab1302098745db","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":760289,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":760289,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_73828769c4aa45a087d3fcc50d7d7ecf"}},"2c544699235b470995bbb58c122eb365":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8189a06434504f4fb617b693dd0f7340","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 760k/760k [00:00&lt;00:00, 769kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7dc9302a23974176bbf090807eacb260"}},"5b7520e06cf643d4b6ab1302098745db":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"73828769c4aa45a087d3fcc50d7d7ecf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8189a06434504f4fb617b693dd0f7340":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7dc9302a23974176bbf090807eacb260":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1fee630ee7204d0f8ff5cfc764bf40dc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_13d198cc56bb4abdadb3b03aeac2289e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6a95c520b2ac4ce5b4ca94561f10f22f","IPY_MODEL_f4a7ce13fcda47ecb5270f7df900848b"]}},"13d198cc56bb4abdadb3b03aeac2289e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6a95c520b2ac4ce5b4ca94561f10f22f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9cd88fdf261c4f6a9cc180b70d8fe055","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9c5396aac920480aac7e2a50ba99a189"}},"f4a7ce13fcda47ecb5270f7df900848b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6e542ecf51a04399a8ac76379fc09312","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 546kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e617ce6441c3462082b3bd2d5a4e1685"}},"9cd88fdf261c4f6a9cc180b70d8fe055":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9c5396aac920480aac7e2a50ba99a189":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6e542ecf51a04399a8ac76379fc09312":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e617ce6441c3462082b3bd2d5a4e1685":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"IoAuoSxKZogJ","colab_type":"text"},"source":["# Funniness Estimation System  v4.5"]},{"cell_type":"code","metadata":{"id":"oO_Pzm1cLKf4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"executionInfo":{"status":"ok","timestamp":1595448499015,"user_tz":-60,"elapsed":28821,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"160b7e7f-f047-4aa5-8906-67dc4e104351"},"source":["\"\"\"\n","@author: Ziyang Lin\n","         zlin19@sheffield.ac.uk\n","         University of Sheffield, UK\n","\"\"\"\n","\n","'''\n","A system for\n","\"Assessing the Funniness of Edited News Headlines (SemEval-2020)\" task 2.\n","'''\n","\n","import random\n","\n","import pandas as pd\n","import numpy as np\n","\n","import os\n","import re\n","import time\n","import math\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","from torchtext import data\n","from torchtext import datasets\n","import torch.utils.data as tud\n","\n","from google.colab import drive \n","drive.mount('/content/gdrive')\n","\n","import nltk\n","nltk.download('punkt')\n","from nltk import word_tokenize\n","\n","\n","# fix the seeds to get consistent results before every training\n","# loop in what follows\n","def fix_seed(seed=1234):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","\n","\n","# Helper function to print the run time\n","def run_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LchxscdyVAfV","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595448517202,"user_tz":-60,"elapsed":726,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}}},"source":["log_num_r = 0\n","real_task_stats = []\n","\n","def add_real_task_stats(real_task_stats, log_num_r, MODEL_NAME, BATCH_SIZE, N_EPOCHS, LRATE, FRATE, EPS, WU, WDECAY, train_loss, train_accuracy, val_loss, val_accuracy, test_accuracy):\n","    log_num_r += 1\n","\n","    real_task_stats.append(\n","        {\n","            'log': log_num_r,\n","            'Model Name': MODEL_NAME,\n","            'Batch Size': BATCH_SIZE,\n","            'N_Epochs': N_EPOCHS,\n","            'lr': LRATE,\n","            'fr': FRATE,\n","            'eps': EPS,\n","            'wu': WU,\n","            'wd': WDECAY,\n","            'Training Loss': train_loss,\n","            'Training Accur.': train_accuracy,\n","            'Valid. Loss': val_loss,\n","            'Valid. Accur.': val_accuracy,\n","            'Testing Accur.': test_accuracy\n","        }\n","    )\n","    \n","    return real_task_stats, log_num_r"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yn3ifntYKxSA","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595448519417,"user_tz":-60,"elapsed":850,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}}},"source":["log_num_f = 0\n","fake_task_stats = []\n","\n","def add_fake_task_stats(fake_task_stats, log_num_f, MODEL_NAME, BATCH_SIZE, N_EPOCHS, LRATE, FRATE, EPS, WU, WDECAY, train_loss, val_loss):\n","    log_num_f += 1\n","\n","    fake_task_stats.append(\n","        {\n","            'log': log_num_f,\n","            'Model Name': MODEL_NAME,\n","            'Batch Size': BATCH_SIZE,\n","            'N_Epochs': N_EPOCHS,\n","            'lr': LRATE,\n","            'fr': FRATE,\n","            'eps': EPS,\n","            'wu': WU,\n","            'wd': WDECAY,\n","            'Training Loss': train_loss,\n","            'Valid. Loss': val_loss\n","        }\n","    )\n","    \n","    return fake_task_stats, log_num_f"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"circaXFnHUnq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":615},"executionInfo":{"status":"ok","timestamp":1595448527374,"user_tz":-60,"elapsed":7427,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"f82eccf6-10b5-4876-de03-9c8373bb66e1"},"source":["!pip install transformers"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n","\u001b[K     |████████████████████████████████| 778kB 8.0MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 23.7MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Collecting tokenizers==0.8.1.rc1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 45.2MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Collecting sentencepiece!=0.1.92\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 42.3MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=b3ba2a3f5e88dc655df3786fc78cf7da21b57f134a755a4a634ea7680adf62f9\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w1adxCqDFnGE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595448527375,"user_tz":-60,"elapsed":2383,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"0823ac3c-a1ed-4663-da96-55444e7a3738"},"source":["# do computation on a GPU if possible \n","if torch.cuda.is_available():\n","  torch.backends.cudnn.deterministic = True\n","  DEVICE='cuda:0'\n","else:\n","  DEVICE='cpu'\n","\n","print('Device is', DEVICE)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Device is cuda:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FcLgdOc_yGsF","colab_type":"text"},"source":["# Preprocessing Datasets"]},{"cell_type":"markdown","metadata":{"id":"IMmXYdN9GXNR","colab_type":"text"},"source":["## Read data from csv files"]},{"cell_type":"code","metadata":{"id":"ooKOsrf8MKrO","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595448531124,"user_tz":-60,"elapsed":2066,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}}},"source":["train_loc = 'gdrive/My Drive/subtask-2/train.csv'\n","dev_loc = 'gdrive/My Drive/subtask-2/dev.csv'\n","test_loc = 'gdrive/My Drive/subtask-2/test.csv'\n","train = pd.read_csv(train_loc)  \n","valid = pd.read_csv(dev_loc)\n","test = pd.read_csv(test_loc)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"WAA7mYMzMK1M","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595448532412,"user_tz":-60,"elapsed":588,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}}},"source":["def get_edited_headlines_list(headls_words):\n","    # list of new edited headlines\n","    headls_list = []\n","    \n","    for origin_headl, new_word in headls_words:\n","      # pattern\n","      p = re.compile(r'\\<(.*?)\\/\\>')\n","      # get the new edited headline\n","      new_headl = p.sub(new_word, origin_headl)\n","      # add it to the list\n","      headls_list.append(new_headl)\n","\n","    return headls_list\n","\n","\n","def processed_data_to_lists(train):\n","    headls_words_1 = [(origin_headl_1, new_word_1) for (origin_headl_1, new_word_1) in zip(train.original1.to_list(), train.edit1.to_list())]\n","    headls_words_2 = [(origin_headl_2, new_word_2) for (origin_headl_2, new_word_2) in zip(train.original2.to_list(), train.edit2.to_list())]\n","    \n","    labels_list = train.label.to_list()\n","\n","    meanGrade1_list = train.meanGrade1.to_list()\n","    meanGrade2_list = train.meanGrade2.to_list()\n","    meanGrade_list = meanGrade1_list + meanGrade2_list\n","\n","    meanGrade1_list = train.meanGrade1.to_list()\n","    meanGrade2_list = train.meanGrade2.to_list()\n","    meanGrade_list = meanGrade1_list + meanGrade2_list\n","\n","    new_word1_list = train.edit1.to_list()\n","    new_word2_list = train.edit2.to_list()\n","    new_word_list = new_word1_list + new_word2_list\n","    \n","    headls_1 = get_edited_headlines_list(headls_words_1)\n","    headls_2 = get_edited_headlines_list(headls_words_2)\n","\n","    return headls_1, headls_2, labels_list, meanGrade_list, new_word_list\n","\n"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cPEAi_Cf1Iqt","colab_type":"text"},"source":["## Get lists of headlines and list of labels"]},{"cell_type":"code","metadata":{"id":"kSgjtdHRMK-q","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595448533846,"user_tz":-60,"elapsed":479,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}}},"source":["train_headls_1, train_headls_2, train_labels_list, train_meanGrade_list, train_new_word_list = processed_data_to_lists(train)\n","valid_headls_1, valid_headls_2, valid_labels_list, valid_meanGrade_list, valid_new_word_list = processed_data_to_lists(valid)\n","test_headls_1, test_headls_2, test_labels_list, test_meanGrade_list, test_new_word_list = processed_data_to_lists(test)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"-PH_wE8JArmh","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595448534616,"user_tz":-60,"elapsed":644,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}}},"source":["train_headls = train_headls_1 + train_headls_2\n","valid_headls = valid_headls_1 + valid_headls_2\n","test_headls = test_headls_1 + test_headls_2"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"LEvZgqhzFDYg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595464881390,"user_tz":-60,"elapsed":874,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"3b9169a3-240a-4fcb-a669-2ef67500ac40"},"source":["# extra data for training\n","\n","train_loc_extra = 'gdrive/My Drive/subtask-2/train_funlines.csv'\n","train_extra = pd.read_csv(train_loc_extra)\n","train_headls_1_extra, train_headls_2_extra, train_labels_list_extra, train_meanGrade_list_extra, train_new_word_list_extra = processed_data_to_lists(train_extra)\n","\n","train_headls_1 = train_headls_1 + train_headls_1_extra\n","train_headls_2 = train_headls_2 + train_headls_2_extra\n","train_meanGrade_list = train_meanGrade_list + train_meanGrade_list_extra\n","train_labels_list = train_labels_list + train_labels_list_extra\n","train_new_word_list = train_new_word_list + train_new_word_list_extra\n","\n","train_headls = train_headls + train_headls_1_extra + train_headls_2_extra\n","\n","len(train_headls_1)"],"execution_count":248,"outputs":[{"output_type":"execute_result","data":{"text/plain":["15255"]},"metadata":{"tags":[]},"execution_count":248}]},{"cell_type":"code","metadata":{"id":"JkgCDCDPAsYp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["92e9968adbe948859c7ad0119edda9f7","a38409d144e6447eb6976d24f9196d27","b46e2a8fe5fe483594de40c1e5043965","2c544699235b470995bbb58c122eb365","5b7520e06cf643d4b6ab1302098745db","73828769c4aa45a087d3fcc50d7d7ecf","8189a06434504f4fb617b693dd0f7340","7dc9302a23974176bbf090807eacb260"]},"executionInfo":{"status":"ok","timestamp":1595448537867,"user_tz":-60,"elapsed":3306,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"25dae934-ea5d-46f7-a329-80b58ff85842"},"source":["from transformers import AlbertTokenizer\n","\n","# Load the ALBERT tokenizer.\n","tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2', do_lower_case=True)"],"execution_count":11,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"92e9968adbe948859c7ad0119edda9f7","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=760289.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5onc99rJTYGg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["1fee630ee7204d0f8ff5cfc764bf40dc","13d198cc56bb4abdadb3b03aeac2289e","6a95c520b2ac4ce5b4ca94561f10f22f","f4a7ce13fcda47ecb5270f7df900848b","9cd88fdf261c4f6a9cc180b70d8fe055","9c5396aac920480aac7e2a50ba99a189","6e542ecf51a04399a8ac76379fc09312","e617ce6441c3462082b3bd2d5a4e1685"]},"executionInfo":{"status":"ok","timestamp":1595448539073,"user_tz":-60,"elapsed":3763,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"f8afd431-33eb-4bb5-8985-eaa4af1ffcb1"},"source":["from transformers import BertTokenizer\n","\n","# Load the BERT tokenizer.\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"execution_count":12,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1fee630ee7204d0f8ff5cfc764bf40dc","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EKNmwLrzudWC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1595464623699,"user_tz":-60,"elapsed":763,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"8e5fbb56-538a-4ae3-e3f1-f460d7074e0f"},"source":["print(' Original: ', train_headls_1[0])\n","\n","print('Tokenized: ', tokenizer.tokenize(train_headls_1[0]))\n","\n","print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_headls_1[0])))"],"execution_count":245,"outputs":[{"output_type":"stream","text":[" Original:  \" Gene Cernan , Last Dancer on the Moon , Dies at 82 \"\n","Tokenized:  ['\"', 'gene', 'ce', '##rna', '##n', ',', 'last', 'dancer', 'on', 'the', 'moon', ',', 'dies', 'at', '82', '\"']\n","Token IDs:  [1000, 4962, 8292, 12789, 2078, 1010, 2197, 8033, 2006, 1996, 4231, 1010, 8289, 2012, 6445, 1000]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"De9MytTHAITV","colab_type":"text"},"source":["## Max sequence length for pre-trained LMs"]},{"cell_type":"code","metadata":{"id":"Fb5crsr9udgo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1595464888147,"user_tz":-60,"elapsed":728,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"619c8b84-a5ce-4263-bea7-1d441102bb63"},"source":["max_one_len = 0\n","\n","\"\"\"for headl in train_headls:\n","    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n","    input_ids = tokenizer.encode(headl, add_special_tokens=True)\n","    # Update the maximum sentence length.\n","    max_one_len = max(max_one_len, len(input_ids))\n","\n","print('Max sequence length for two sentences: ', (max_one_len-1)*2)\n","print('Max sequence length for one sentence: ', max_one_len)\"\"\"\n","\n","\n","for headl in train_headls:\n","    headl = headl.split()\n","    max_one_len = max(len(headl), max_one_len)\n","\n","print('Max sequence length for two headlines: ', max_one_len*2 + 3 )\n","print('Max sequence length for new \\'headlines + new words\\': ', max_one_len + 4 )\n"],"execution_count":249,"outputs":[{"output_type":"stream","text":["Max sequence length for two headlines:  57\n","Max sequence length for new 'headlines + new words':  31\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oE6vMueW_kf8","colab_type":"text"},"source":["## Get encoded inputs for pre-trained LMs"]},{"cell_type":"code","metadata":{"id":"DJGCW4pC2XG7","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595464999897,"user_tz":-60,"elapsed":21081,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}}},"source":["# prepare encoded inputs for real task\n","train_encoded_inputs = tokenizer(train_headls_1, train_headls_2, padding='max_length', max_length=57, truncation=True, return_tensors=\"pt\")\n","valid_encoded_inputs = tokenizer(valid_headls_1, valid_headls_2, padding='max_length', max_length=57, truncation=True, return_tensors=\"pt\")\n","test_encoded_inputs = tokenizer(test_headls_1, test_headls_2, padding='max_length', max_length=57, truncation=True, return_tensors=\"pt\")\n","\n","# prepare encoded inputs for fake task\n","pre_train_encoded = tokenizer(train_headls, train_new_word_list, padding='max_length', max_length=31, truncation=True, return_tensors=\"pt\")\n","pre_valid_encoded = tokenizer(valid_headls, valid_new_word_list, padding='max_length', max_length=31, truncation=True, return_tensors=\"pt\")"],"execution_count":250,"outputs":[]},{"cell_type":"code","metadata":{"id":"WjUbZTv15O4q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595465005173,"user_tz":-60,"elapsed":743,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"b418e1ea-20bd-4954-c201-54f1f15b0a23"},"source":["# get input_ids, attention_mask, token_type_ids and labels for real task\n","train_input_ids = train_encoded_inputs['input_ids']\n","train_attention_mask = train_encoded_inputs['attention_mask']\n","train_token_type_ids = train_encoded_inputs['token_type_ids']\n","train_labels = torch.tensor(train_labels_list)\n","\n","valid_input_ids = valid_encoded_inputs['input_ids']\n","valid_attention_mask = valid_encoded_inputs['attention_mask']\n","valid_token_type_ids = valid_encoded_inputs['token_type_ids']\n","valid_labels = torch.tensor(valid_labels_list)\n","\n","test_input_ids = test_encoded_inputs['input_ids']\n","test_attention_mask = test_encoded_inputs['attention_mask']\n","test_token_type_ids = test_encoded_inputs['token_type_ids']\n","test_labels = torch.tensor(test_labels_list)\n","\n","\n","# get input_ids, attention_mask and labels for fake task\n","pre_train_input_ids = pre_train_encoded['input_ids']\n","pre_train_attention_mask = pre_train_encoded['attention_mask']\n","pre_train_token_type_ids = pre_train_encoded['token_type_ids']\n","pre_train_labels = torch.tensor(train_meanGrade_list)\n","\n","pre_valid_input_ids = pre_valid_encoded['input_ids']\n","pre_valid_attention_mask = pre_valid_encoded['attention_mask']\n","pre_valid_token_type_ids = pre_valid_encoded['token_type_ids']\n","pre_valid_labels = torch.tensor(valid_meanGrade_list)\n","\n","pre_train_input_ids[0]\n","pre_train_labels"],"execution_count":251,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1.2000, 0.6000, 0.6000,  ..., 1.4000, 1.4000, 2.2000])"]},"metadata":{"tags":[]},"execution_count":251}]},{"cell_type":"markdown","metadata":{"id":"W_G4N1_nHDBS","colab_type":"text"},"source":["## Prepare mini-batches"]},{"cell_type":"code","metadata":{"id":"JFBTt-rLGwOW","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595465014350,"user_tz":-60,"elapsed":629,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}}},"source":["class BERT_Dataset(tud.Dataset):\n","    def __init__(self, x1, x2, x3, y1):\n","        self.len = x1.shape[0]\n","\n","        self.x1_data = x1.to(DEVICE)\n","        self.x2_data = x2.to(DEVICE)\n","        self.x3_data = x3.to(DEVICE)\n","        self.y1_data = y1.to(DEVICE)\n","\n","\n","    def __getitem__(self, index):\n","        return self.x1_data[index], self.x2_data[index], self.x3_data[index], self.y1_data[index]\n","\n","\n","    def __len__(self):\n","        return self.len\n","\n","\n","class Pre_BERT_Dataset(tud.Dataset):\n","    def __init__(self, x1, x2, y1):\n","        self.len = x1.shape[0]\n","\n","        self.x1_data = x1.to(DEVICE)\n","        self.x2_data = x2.to(DEVICE)\n","        self.y1_data = y1.to(DEVICE)\n","\n","\n","    def __getitem__(self, index):\n","        return self.x1_data[index], self.x2_data[index], self.y1_data[index]\n","\n","\n","    def __len__(self):\n","        return self.len"],"execution_count":252,"outputs":[]},{"cell_type":"code","metadata":{"id":"GAv8-n4iFcN0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1595465015884,"user_tz":-60,"elapsed":842,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"3ba1efe6-a68f-4fdf-8c72-9cfaa3877859"},"source":["fix_seed()\n","# Batching for BERT\n","BATCH_SIZE = 16\n","\n","# For real task\n","train_dataset = BERT_Dataset(train_input_ids, train_attention_mask, train_token_type_ids, train_labels)\n","valid_dataset = BERT_Dataset(valid_input_ids, valid_attention_mask, valid_token_type_ids, valid_labels)\n","test_dataset = BERT_Dataset(test_input_ids, test_attention_mask, test_token_type_ids, test_labels)\n","\n","train_dataloader = tud.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","valid_dataloader = tud.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","test_dataloader = tud.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","\n","# For fake task\n","pre_train_dataset = BERT_Dataset(pre_train_input_ids, pre_train_attention_mask, pre_train_token_type_ids, pre_train_labels)\n","pre_valid_dataset = BERT_Dataset(pre_valid_input_ids, pre_valid_attention_mask, pre_valid_token_type_ids, pre_valid_labels)\n","\n","pre_train_dataloader = tud.DataLoader(pre_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","pre_valid_dataloader = tud.DataLoader(pre_valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","\n","\n","##### demo #####\n","print(train_dataloader)\n","\n","for x1, x2, x3, y1 in train_dataloader:\n","    demo_x1 = x1\n","    demo_x2 = x2\n","    demo_x3 = x3\n","    demo_y1 = y1\n","    break\n","    \n","print(x1.shape)\n","print(x2.shape)\n","print(x3.shape)\n","print(y1.shape)\n","print(len(train_dataloader))"],"execution_count":253,"outputs":[{"output_type":"stream","text":["<torch.utils.data.dataloader.DataLoader object at 0x7f1e3e147be0>\n","torch.Size([16, 57])\n","torch.Size([16, 57])\n","torch.Size([16, 57])\n","torch.Size([16])\n","954\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"V7SP8TiKLcL2","colab_type":"text"},"source":["# Training Preparation "]},{"cell_type":"markdown","metadata":{"id":"ry7hbzFyLFz6","colab_type":"text"},"source":["## Define accuracy"]},{"cell_type":"code","metadata":{"id":"3IozWES_NudS","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595465026612,"user_tz":-60,"elapsed":696,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}}},"source":["def accuracy(preds, y):\n","    \"\"\"\n","    returns accuracy per batch\n","    \"\"\"\n","\n","    class_preds =  torch.argmax(F.softmax(preds, dim = 1), 1)\n","    correct = (class_preds == y).float() # convert into float for division \n","    acc = correct.sum() / len(correct)\n","    return acc"],"execution_count":254,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nx-ADOt3MlHg","colab_type":"text"},"source":["## Define train and evaluate"]},{"cell_type":"code","metadata":{"id":"oL8RT5k2Bn5W","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595465028421,"user_tz":-60,"elapsed":706,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}}},"source":["# define train_BERT and evaluate_BERT for real task\n","def train_BERT(model, train_dataloader, valid_dataloader, optimizer, scheduler, criterion, N_EPOCHS):\n","    fix_seed()\n","    model = model.to(DEVICE)\n","    model_list = []\n","    # Measure the total time for the whole run.\n","    t0 = time.time()\n","\n","    for epoch in range(N_EPOCHS):\n","    \n","        start_time = time.time()\n","        # To ensure the dropout is \"turned on\" while training\n","        model.train()\n","        \n","        epoch_loss = 0\n","        epoch_acc = 0\n","    \n","        for input_ids_batch, attention_mask_batch, token_type_ids_batch, labels in train_dataloader:\n","                        \n","            # Zero the gradients\n","            optimizer.zero_grad()\n","            # shape(input_ids_batch) = [B, T]\n","            # shape(attention_mask_batch) = [B, T]\n","            # shape(labels) = [B]\n","\n","            # get the output\n","            predictions = model(input_ids_batch,\n","                                attention_mask_batch,\n","                                token_type_ids_batch)\n","            \n","            # calculate the loss\n","            loss = criterion(predictions.view(-1, 3), labels.view(-1))\n","\n","            # calculate training accuracy\n","            acc = accuracy(predictions, labels)\n","                      \n","            # calculate the gradient of each parameter\n","            loss.backward()\n","        \n","            # update the parameters using the gradients and optimizer algorithm \n","            optimizer.step()\n","\n","            # update the learning rate\n","            scheduler.step()\n","            \n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","            \n","        average_epoch_loss = epoch_loss / len(train_dataloader)\n","        average_epoch_acc = epoch_acc / len(train_dataloader)\n","        \n","        end_time = time.time()\n","               \n","        epoch_mins, epoch_secs = run_time(start_time, end_time)\n","    \n","        average_epoch_valid_loss, average_epoch_valid_acc = evaluate_BERT(model, criterion, valid_dataloader)\n","\n","        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","        print(f'\\tTrain Loss: {average_epoch_loss:.5f} | Train Acc: {average_epoch_acc*100:.4f}%')\n","        print(f'\\t Val. Loss: {average_epoch_valid_loss:.5f} |  Val. Acc: {average_epoch_valid_acc*100:.4f}%')\n","\n","        model_list.append(model)\n","\n","    print(\"\")\n","    print(\"***Completed***\")\n","    total_mins, total_secs = run_time(t0, time.time())\n","    print(f'Total time spent: {total_mins}m {total_secs}s')\n","\n","    return average_epoch_loss, average_epoch_acc, average_epoch_valid_loss, average_epoch_valid_acc, model_list\n","\n","def evaluate_BERT(model, criterion, dataloader):\n","    fix_seed()\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    # Turn on evaluate mode. This de-activates dropout. \n","    model.eval()\n","\n","    # We do not compute gradients within this block, i.e. no training\n","    with torch.no_grad():\n","\n","        for input_ids_batch, attention_mask_batch, token_type_ids_batch, labels in dataloader:\n","            \n","            # get the output\n","            predictions = model(input_ids_batch,\n","                            attention_mask_batch,\n","                            token_type_ids_batch)\n","\n","            loss = criterion(predictions.view(-1, 3), labels.view(-1))\n","            acc = accuracy(predictions, labels)\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","\n","    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)"],"execution_count":255,"outputs":[]},{"cell_type":"code","metadata":{"id":"7tVyUsUDAt3d","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595465033663,"user_tz":-60,"elapsed":882,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}}},"source":["# define train_BERT and evaluate_BERT for fake regression task\n","def pre_train_BERT(model, train_dataloader, valid_dataloader, optimizer, scheduler, criterion, N_EPOCHS):\n","    fix_seed()\n","    model = model.to(DEVICE)\n","    model_list = []\n","    t0 = time.time()\n","\n","    for epoch in range(N_EPOCHS):    \n","        start_time = time.time()\n","        model.train()       \n","        epoch_loss = 0\n","        epoch_acc = 0\n","    \n","        for input_ids_batch, attention_mask_batch, token_type_ids_batch, labels in train_dataloader:                       \n","            optimizer.zero_grad()\n","\n","            predictions = model(input_ids_batch,\n","                                attention_mask_batch,\n","                                token_type_ids_batch)\n","            \n","            loss = criterion(predictions.view(-1), labels.view(-1))                      \n","            loss.backward()        \n","            optimizer.step()\n","            scheduler.step()          \n","            epoch_loss += loss.item()\n","            \n","        average_epoch_loss = epoch_loss / len(train_dataloader)\n","        \n","        end_time = time.time()               \n","        epoch_mins, epoch_secs = run_time(start_time, end_time)\n","    \n","        average_epoch_valid_loss = pre_evaluate_BERT(model, criterion, valid_dataloader)\n","\n","        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","        print(f'\\tTrain Loss: {average_epoch_loss:.5f} ')\n","        print(f'\\t Val. Loss: {average_epoch_valid_loss:.5f} ')\n","        \n","        model_list.append(model)\n","\n","    print(\"\")\n","    print(\"***Completed***\")\n","    total_mins, total_secs = run_time(t0, time.time())\n","    print(f'Total time spent: {total_mins}m {total_secs}s')\n","\n","    return average_epoch_loss, average_epoch_valid_loss, model_list\n","\n","def pre_evaluate_BERT(model, criterion, dataloader):\n","    fix_seed()\n","    epoch_loss = 0\n","    model.eval()\n","\n","    with torch.no_grad():\n","\n","        for input_ids_batch, attention_mask_batch, token_type_ids_batch, labels in dataloader:\n","            \n","            predictions = model(input_ids_batch,\n","                                attention_mask_batch,\n","                                token_type_ids_batch)\n","\n","            loss = criterion(predictions.view(-1), labels.view(-1))\n","            epoch_loss += loss.item()\n","\n","    return epoch_loss / len(dataloader)"],"execution_count":256,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gk-lxAmtJIjE","colab_type":"text"},"source":["## Define models"]},{"cell_type":"code","metadata":{"id":"yHcJOucfJN3X","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595459905024,"user_tz":-60,"elapsed":778,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}}},"source":["from transformers import AlbertModel, BertModel, ElectraModel, AdamW\n","\n","class AlbertModel_FakeTask(nn.Module):\n","    def __init__(self):\n","        super(AlbertModel_FakeTask, self).__init__()\n","        self.albert = AlbertModel.from_pretrained('albert-base-v2')\n","        self.linear = nn.Linear(768, 1)\n","\n","    def forward(self, source, mask):\n","        output = self.albert(source, attention_mask=mask)\n","        # take the last hidden state\n","        hidden = output[0]\n","        # take the representations for CLS\n","        all_cls = hidden[:, 0, :]\n","        # pass to linear layer to get the score for each sentence(cls here)\n","        pred = self.linear(all_cls)\n","\n","        return pred\n","\n","\n","class AlbertModel_Real(nn.Module):\n","    def __init__(self, albert_model):\n","        super(AlbertModel_Real, self).__init__()\n","        self.albert = albert_model\n","        self.classifier = nn.Linear(768, 3)\n","\n","    def forward(self, source, mask, type_ids):\n","        output = self.albert(source, attention_mask=mask, token_type_ids=type_ids)\n","        # take the last hidden state\n","        hidden = output[0]\n","        # take the representations for CLS\n","        all_cls = hidden[:, 0, :]\n","        # pass to linear layer to get the prediction for every sentence pairs(cls here)\n","        pred = self.classifier(all_cls)\n","\n","        return pred\n","\n","\n","class BertModel_FakeTask(nn.Module):\n","    def __init__(self):\n","        super(BertModel_FakeTask, self).__init__()\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","        self.linear = nn.Linear(768, 1)\n","\n","    def forward(self, source, mask, type_ids):\n","        output = self.bert(source, attention_mask=mask, token_type_ids=type_ids)\n","        # take the last hidden state\n","        hidden = output[0]\n","        # take the representations for CLS\n","        all_cls = hidden[:, 0, :]\n","        # pass to linear layer to get the score for each sentence(cls here)\n","        pred = self.linear(all_cls)\n","\n","        return pred\n","\n","\n","class BertModel_Real(nn.Module):\n","    def __init__(self, bert_model, dropout_prob):\n","        super(BertModel_Real, self).__init__()\n","        self.bert = bert_model\n","        self.dropout = nn.Dropout(dropout_prob)\n","        self.classifier = nn.Linear(768, 3)\n","\n","    def forward(self, source, mask, type_ids):\n","        output = self.bert(source, attention_mask=mask, token_type_ids=type_ids)\n","        # take the last hidden state\n","        hidden = output[0]\n","        # take the representations for CLS\n","        all_cls = hidden[:, 0, :]\n","\n","        all_cls = self.dropout(all_cls)\n","\n","        # pass to linear layer to get the prediction for every sentence pairs(cls here)\n","        pred = self.classifier(all_cls)\n","\n","        return pred        "],"execution_count":214,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O_F_bL7Ro4_s","colab_type":"text"},"source":["# Fake Task"]},{"cell_type":"code","metadata":{"id":"9eb57bSMcDKp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1595452458002,"user_tz":-60,"elapsed":4486,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"cf7e6d4f-a269-42a9-dd50-e5806d1baae6"},"source":["fix_seed()\n","# Create the pre_model for the fake task\n","pre_model_al = AlbertModel_FakeTask()\n","\n","pre_model_ber = BertModel_FakeTask()\n","\n","\n","param_names_list = [n for n, p in pre_model_ber.named_parameters()]\n","param_names_list"],"execution_count":96,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['bert.embeddings.word_embeddings.weight',\n"," 'bert.embeddings.position_embeddings.weight',\n"," 'bert.embeddings.token_type_embeddings.weight',\n"," 'bert.embeddings.LayerNorm.weight',\n"," 'bert.embeddings.LayerNorm.bias',\n"," 'bert.encoder.layer.0.attention.self.query.weight',\n"," 'bert.encoder.layer.0.attention.self.query.bias',\n"," 'bert.encoder.layer.0.attention.self.key.weight',\n"," 'bert.encoder.layer.0.attention.self.key.bias',\n"," 'bert.encoder.layer.0.attention.self.value.weight',\n"," 'bert.encoder.layer.0.attention.self.value.bias',\n"," 'bert.encoder.layer.0.attention.output.dense.weight',\n"," 'bert.encoder.layer.0.attention.output.dense.bias',\n"," 'bert.encoder.layer.0.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.0.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.0.intermediate.dense.weight',\n"," 'bert.encoder.layer.0.intermediate.dense.bias',\n"," 'bert.encoder.layer.0.output.dense.weight',\n"," 'bert.encoder.layer.0.output.dense.bias',\n"," 'bert.encoder.layer.0.output.LayerNorm.weight',\n"," 'bert.encoder.layer.0.output.LayerNorm.bias',\n"," 'bert.encoder.layer.1.attention.self.query.weight',\n"," 'bert.encoder.layer.1.attention.self.query.bias',\n"," 'bert.encoder.layer.1.attention.self.key.weight',\n"," 'bert.encoder.layer.1.attention.self.key.bias',\n"," 'bert.encoder.layer.1.attention.self.value.weight',\n"," 'bert.encoder.layer.1.attention.self.value.bias',\n"," 'bert.encoder.layer.1.attention.output.dense.weight',\n"," 'bert.encoder.layer.1.attention.output.dense.bias',\n"," 'bert.encoder.layer.1.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.1.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.1.intermediate.dense.weight',\n"," 'bert.encoder.layer.1.intermediate.dense.bias',\n"," 'bert.encoder.layer.1.output.dense.weight',\n"," 'bert.encoder.layer.1.output.dense.bias',\n"," 'bert.encoder.layer.1.output.LayerNorm.weight',\n"," 'bert.encoder.layer.1.output.LayerNorm.bias',\n"," 'bert.encoder.layer.2.attention.self.query.weight',\n"," 'bert.encoder.layer.2.attention.self.query.bias',\n"," 'bert.encoder.layer.2.attention.self.key.weight',\n"," 'bert.encoder.layer.2.attention.self.key.bias',\n"," 'bert.encoder.layer.2.attention.self.value.weight',\n"," 'bert.encoder.layer.2.attention.self.value.bias',\n"," 'bert.encoder.layer.2.attention.output.dense.weight',\n"," 'bert.encoder.layer.2.attention.output.dense.bias',\n"," 'bert.encoder.layer.2.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.2.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.2.intermediate.dense.weight',\n"," 'bert.encoder.layer.2.intermediate.dense.bias',\n"," 'bert.encoder.layer.2.output.dense.weight',\n"," 'bert.encoder.layer.2.output.dense.bias',\n"," 'bert.encoder.layer.2.output.LayerNorm.weight',\n"," 'bert.encoder.layer.2.output.LayerNorm.bias',\n"," 'bert.encoder.layer.3.attention.self.query.weight',\n"," 'bert.encoder.layer.3.attention.self.query.bias',\n"," 'bert.encoder.layer.3.attention.self.key.weight',\n"," 'bert.encoder.layer.3.attention.self.key.bias',\n"," 'bert.encoder.layer.3.attention.self.value.weight',\n"," 'bert.encoder.layer.3.attention.self.value.bias',\n"," 'bert.encoder.layer.3.attention.output.dense.weight',\n"," 'bert.encoder.layer.3.attention.output.dense.bias',\n"," 'bert.encoder.layer.3.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.3.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.3.intermediate.dense.weight',\n"," 'bert.encoder.layer.3.intermediate.dense.bias',\n"," 'bert.encoder.layer.3.output.dense.weight',\n"," 'bert.encoder.layer.3.output.dense.bias',\n"," 'bert.encoder.layer.3.output.LayerNorm.weight',\n"," 'bert.encoder.layer.3.output.LayerNorm.bias',\n"," 'bert.encoder.layer.4.attention.self.query.weight',\n"," 'bert.encoder.layer.4.attention.self.query.bias',\n"," 'bert.encoder.layer.4.attention.self.key.weight',\n"," 'bert.encoder.layer.4.attention.self.key.bias',\n"," 'bert.encoder.layer.4.attention.self.value.weight',\n"," 'bert.encoder.layer.4.attention.self.value.bias',\n"," 'bert.encoder.layer.4.attention.output.dense.weight',\n"," 'bert.encoder.layer.4.attention.output.dense.bias',\n"," 'bert.encoder.layer.4.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.4.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.4.intermediate.dense.weight',\n"," 'bert.encoder.layer.4.intermediate.dense.bias',\n"," 'bert.encoder.layer.4.output.dense.weight',\n"," 'bert.encoder.layer.4.output.dense.bias',\n"," 'bert.encoder.layer.4.output.LayerNorm.weight',\n"," 'bert.encoder.layer.4.output.LayerNorm.bias',\n"," 'bert.encoder.layer.5.attention.self.query.weight',\n"," 'bert.encoder.layer.5.attention.self.query.bias',\n"," 'bert.encoder.layer.5.attention.self.key.weight',\n"," 'bert.encoder.layer.5.attention.self.key.bias',\n"," 'bert.encoder.layer.5.attention.self.value.weight',\n"," 'bert.encoder.layer.5.attention.self.value.bias',\n"," 'bert.encoder.layer.5.attention.output.dense.weight',\n"," 'bert.encoder.layer.5.attention.output.dense.bias',\n"," 'bert.encoder.layer.5.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.5.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.5.intermediate.dense.weight',\n"," 'bert.encoder.layer.5.intermediate.dense.bias',\n"," 'bert.encoder.layer.5.output.dense.weight',\n"," 'bert.encoder.layer.5.output.dense.bias',\n"," 'bert.encoder.layer.5.output.LayerNorm.weight',\n"," 'bert.encoder.layer.5.output.LayerNorm.bias',\n"," 'bert.encoder.layer.6.attention.self.query.weight',\n"," 'bert.encoder.layer.6.attention.self.query.bias',\n"," 'bert.encoder.layer.6.attention.self.key.weight',\n"," 'bert.encoder.layer.6.attention.self.key.bias',\n"," 'bert.encoder.layer.6.attention.self.value.weight',\n"," 'bert.encoder.layer.6.attention.self.value.bias',\n"," 'bert.encoder.layer.6.attention.output.dense.weight',\n"," 'bert.encoder.layer.6.attention.output.dense.bias',\n"," 'bert.encoder.layer.6.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.6.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.6.intermediate.dense.weight',\n"," 'bert.encoder.layer.6.intermediate.dense.bias',\n"," 'bert.encoder.layer.6.output.dense.weight',\n"," 'bert.encoder.layer.6.output.dense.bias',\n"," 'bert.encoder.layer.6.output.LayerNorm.weight',\n"," 'bert.encoder.layer.6.output.LayerNorm.bias',\n"," 'bert.encoder.layer.7.attention.self.query.weight',\n"," 'bert.encoder.layer.7.attention.self.query.bias',\n"," 'bert.encoder.layer.7.attention.self.key.weight',\n"," 'bert.encoder.layer.7.attention.self.key.bias',\n"," 'bert.encoder.layer.7.attention.self.value.weight',\n"," 'bert.encoder.layer.7.attention.self.value.bias',\n"," 'bert.encoder.layer.7.attention.output.dense.weight',\n"," 'bert.encoder.layer.7.attention.output.dense.bias',\n"," 'bert.encoder.layer.7.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.7.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.7.intermediate.dense.weight',\n"," 'bert.encoder.layer.7.intermediate.dense.bias',\n"," 'bert.encoder.layer.7.output.dense.weight',\n"," 'bert.encoder.layer.7.output.dense.bias',\n"," 'bert.encoder.layer.7.output.LayerNorm.weight',\n"," 'bert.encoder.layer.7.output.LayerNorm.bias',\n"," 'bert.encoder.layer.8.attention.self.query.weight',\n"," 'bert.encoder.layer.8.attention.self.query.bias',\n"," 'bert.encoder.layer.8.attention.self.key.weight',\n"," 'bert.encoder.layer.8.attention.self.key.bias',\n"," 'bert.encoder.layer.8.attention.self.value.weight',\n"," 'bert.encoder.layer.8.attention.self.value.bias',\n"," 'bert.encoder.layer.8.attention.output.dense.weight',\n"," 'bert.encoder.layer.8.attention.output.dense.bias',\n"," 'bert.encoder.layer.8.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.8.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.8.intermediate.dense.weight',\n"," 'bert.encoder.layer.8.intermediate.dense.bias',\n"," 'bert.encoder.layer.8.output.dense.weight',\n"," 'bert.encoder.layer.8.output.dense.bias',\n"," 'bert.encoder.layer.8.output.LayerNorm.weight',\n"," 'bert.encoder.layer.8.output.LayerNorm.bias',\n"," 'bert.encoder.layer.9.attention.self.query.weight',\n"," 'bert.encoder.layer.9.attention.self.query.bias',\n"," 'bert.encoder.layer.9.attention.self.key.weight',\n"," 'bert.encoder.layer.9.attention.self.key.bias',\n"," 'bert.encoder.layer.9.attention.self.value.weight',\n"," 'bert.encoder.layer.9.attention.self.value.bias',\n"," 'bert.encoder.layer.9.attention.output.dense.weight',\n"," 'bert.encoder.layer.9.attention.output.dense.bias',\n"," 'bert.encoder.layer.9.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.9.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.9.intermediate.dense.weight',\n"," 'bert.encoder.layer.9.intermediate.dense.bias',\n"," 'bert.encoder.layer.9.output.dense.weight',\n"," 'bert.encoder.layer.9.output.dense.bias',\n"," 'bert.encoder.layer.9.output.LayerNorm.weight',\n"," 'bert.encoder.layer.9.output.LayerNorm.bias',\n"," 'bert.encoder.layer.10.attention.self.query.weight',\n"," 'bert.encoder.layer.10.attention.self.query.bias',\n"," 'bert.encoder.layer.10.attention.self.key.weight',\n"," 'bert.encoder.layer.10.attention.self.key.bias',\n"," 'bert.encoder.layer.10.attention.self.value.weight',\n"," 'bert.encoder.layer.10.attention.self.value.bias',\n"," 'bert.encoder.layer.10.attention.output.dense.weight',\n"," 'bert.encoder.layer.10.attention.output.dense.bias',\n"," 'bert.encoder.layer.10.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.10.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.10.intermediate.dense.weight',\n"," 'bert.encoder.layer.10.intermediate.dense.bias',\n"," 'bert.encoder.layer.10.output.dense.weight',\n"," 'bert.encoder.layer.10.output.dense.bias',\n"," 'bert.encoder.layer.10.output.LayerNorm.weight',\n"," 'bert.encoder.layer.10.output.LayerNorm.bias',\n"," 'bert.encoder.layer.11.attention.self.query.weight',\n"," 'bert.encoder.layer.11.attention.self.query.bias',\n"," 'bert.encoder.layer.11.attention.self.key.weight',\n"," 'bert.encoder.layer.11.attention.self.key.bias',\n"," 'bert.encoder.layer.11.attention.self.value.weight',\n"," 'bert.encoder.layer.11.attention.self.value.bias',\n"," 'bert.encoder.layer.11.attention.output.dense.weight',\n"," 'bert.encoder.layer.11.attention.output.dense.bias',\n"," 'bert.encoder.layer.11.attention.output.LayerNorm.weight',\n"," 'bert.encoder.layer.11.attention.output.LayerNorm.bias',\n"," 'bert.encoder.layer.11.intermediate.dense.weight',\n"," 'bert.encoder.layer.11.intermediate.dense.bias',\n"," 'bert.encoder.layer.11.output.dense.weight',\n"," 'bert.encoder.layer.11.output.dense.bias',\n"," 'bert.encoder.layer.11.output.LayerNorm.weight',\n"," 'bert.encoder.layer.11.output.LayerNorm.bias',\n"," 'bert.pooler.dense.weight',\n"," 'bert.pooler.dense.bias',\n"," 'linear.weight',\n"," 'linear.bias']"]},"metadata":{"tags":[]},"execution_count":96}]},{"cell_type":"markdown","metadata":{"id":"dfnCDZLLUuVj","colab_type":"text"},"source":["## Hyperparameters"]},{"cell_type":"code","metadata":{"id":"dlF41zJHKurw","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595452474949,"user_tz":-60,"elapsed":929,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}}},"source":["# Hyperparameters for the fake task LM:\n","\n","# Number of training epochs. The BERT authors recommend between 2 and 4.\n","N_EPOCHS = 1\n","\n","LRATE = 5e-3\n","FRATE = 2e-5\n","EPS = 1e-8\n","WU = 0.1\n","WDECAY = 0.01\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","TOTSTEPS = len(train_dataloader) * N_EPOCHS\n","WUSTEPS = int(TOTSTEPS * WU)\n","\n","# Apply weight decay to all parameters other than bias and layer normalization terms\n","# Optimize the parameters of the head layer by the learning rate\n","# Optimize the parameters of the pretrain LM by the fine-tuning rate\n","no_decay = ['bias', 'LayerNorm.weight']\n","named_parameters = pre_model_ber.named_parameters()\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in named_parameters if not any(nd in n for nd in no_decay)], 'weight_decay': WDECAY},\n","    {'params': [p for n, p in named_parameters if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n","    {'params': [p for n, p in named_parameters if \"albert\" not in n], 'lr': LRATE}\n","]\n","#    {'params': [p for n, p in named_parameters if \"albert\" in n], 'lr': FRATE},"],"execution_count":101,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Y55KlMtWSvV","colab_type":"text"},"source":["## Optimizer & learning rate scheduler"]},{"cell_type":"code","metadata":{"id":"vg-o8bvyBoDx","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595452474950,"user_tz":-60,"elapsed":566,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}}},"source":["# Create the optimizer, \n","# the epsilon parameter is a very small number to prevent any division by zero\n","optimizer = AdamW(optimizer_grouped_parameters, lr=FRATE, eps = EPS)"],"execution_count":102,"outputs":[]},{"cell_type":"code","metadata":{"id":"J6uz9uWdBoH6","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595452475217,"user_tz":-60,"elapsed":659,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}}},"source":["from transformers import get_linear_schedule_with_warmup\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = WUSTEPS,\n","                                            num_training_steps = TOTSTEPS)"],"execution_count":103,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zWK17_ZgEY8H","colab_type":"text"},"source":["## Define RMSE"]},{"cell_type":"code","metadata":{"id":"h4o3ue7mEi09","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595452475490,"user_tz":-60,"elapsed":475,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}}},"source":["# define rmse\n","def rmse(predictions, labels):\n","    loss = torch.sqrt(((predictions - labels)**2).mean())\n","\n","    return loss"],"execution_count":104,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uv3-MI3pX2tr","colab_type":"text"},"source":["## Fake task training"]},{"cell_type":"code","metadata":{"id":"j7uePPEGBoMu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1595452630918,"user_tz":-60,"elapsed":154921,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"b6db9ff4-448e-413f-d193-4e51b3a42798"},"source":["criterion_r = rmse\n","\n","train_loss, val_loss, model_list = pre_train_BERT(pre_model_ber,\n","                                                  pre_train_dataloader,\n","                                                  pre_valid_dataloader,\n","                                                  optimizer,\n","                                                  scheduler,\n","                                                  criterion_r,\n","                                                  N_EPOCHS)"],"execution_count":105,"outputs":[{"output_type":"stream","text":["Epoch: 01 | Epoch Time: 2m 24s\n","\tTrain Loss: 0.52952 \n","\t Val. Loss: 0.52524 \n","\n","***Completed***\n","Total time spent: 2m 34s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bKSdErzjMKAs","colab_type":"text"},"source":["## Fake task logging"]},{"cell_type":"code","metadata":{"id":"pOl5Sa_0MRb2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":142},"executionInfo":{"status":"ok","timestamp":1595453472671,"user_tz":-60,"elapsed":721,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"e0641466-8f6a-4a90-91e2-7bb6f119621e"},"source":["B1 = \"bert-base-uncased\"\n","A2 = \"albert-base-v2\"\n","E = \"electra\"\n","\n","fake_task_stats, log_num_f = add_fake_task_stats(fake_task_stats, \n","                                             log_num_f,\n","                                             B1,\n","                                             BATCH_SIZE, \n","                                             N_EPOCHS,\n","                                             \"{:.0e}\".format(LRATE),\n","                                             \"{:.0e}\".format(FRATE), \n","                                             \"{:.0e}\".format(EPS), \n","                                             WU,\n","                                             WDECAY, \n","                                             train_loss,\n","                                             val_loss\n","                                             )\n","\n","pd.set_option('precision', 5)\n","df_stats = pd.DataFrame(data=fake_task_stats)\n","df_stats = df_stats.set_index('log')\n","# Display the table.\n","df_stats"],"execution_count":106,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Model Name</th>\n","      <th>Batch Size</th>\n","      <th>N_Epochs</th>\n","      <th>lr</th>\n","      <th>fr</th>\n","      <th>eps</th>\n","      <th>wu</th>\n","      <th>wd</th>\n","      <th>Training Loss</th>\n","      <th>Valid. Loss</th>\n","    </tr>\n","    <tr>\n","      <th>log</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>bert-base-uncased</td>\n","      <td>16</td>\n","      <td>3</td>\n","      <td>5e-03</td>\n","      <td>2e-05</td>\n","      <td>1e-08</td>\n","      <td>0.1</td>\n","      <td>0.01</td>\n","      <td>0.32183</td>\n","      <td>0.54243</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>bert-base-uncased</td>\n","      <td>16</td>\n","      <td>1</td>\n","      <td>5e-03</td>\n","      <td>2e-05</td>\n","      <td>1e-08</td>\n","      <td>0.1</td>\n","      <td>0.01</td>\n","      <td>0.52952</td>\n","      <td>0.52524</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            Model Name  Batch Size  N_Epochs  ...    wd Training Loss Valid. Loss\n","log                                           ...                                \n","1    bert-base-uncased          16         3  ...  0.01       0.32183     0.54243\n","2    bert-base-uncased          16         1  ...  0.01       0.52952     0.52524\n","\n","[2 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":106}]},{"cell_type":"code","metadata":{"id":"zgpu6uhSNcsp","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595453479297,"user_tz":-60,"elapsed":748,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}}},"source":["log_loc = 'gdrive/My Drive/subtask-2/log_fake_task2.csv'\n","df_stats.to_csv(log_loc, index=False)"],"execution_count":107,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QYJz0PZOpLca","colab_type":"text"},"source":["# Real Task"]},{"cell_type":"code","metadata":{"id":"KioNHudYrNYp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1595469668676,"user_tz":-60,"elapsed":860,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"bc8b6214-9a64-4656-daea-bdba37833d17"},"source":["import copy\n","\n","fix_seed()\n","# Create the model for the real task\n","albert_model = pre_model_al.albert\n","real_model_al = AlbertModel_Real(albert_model)\n","\n","bert_model = copy.deepcopy(pre_model_ber.bert)\n","drop_prob = 0.1\n","real_model_ber = BertModel_Real(bert_model, drop_prob)\n","\n","\"\"\"param_names_list = [n for n, p in real_model_ber.named_parameters()]\n","param_names_list\"\"\""],"execution_count":297,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'param_names_list = [n for n, p in real_model_ber.named_parameters()]\\nparam_names_list'"]},"metadata":{"tags":[]},"execution_count":297}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QmsapKVgqq59"},"source":["## Hyperparameters"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Spc2ME9Fqu5w","colab":{},"executionInfo":{"status":"ok","timestamp":1595466054261,"user_tz":-60,"elapsed":491,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}}},"source":["# Hyperparameters for the real task model:\n","\n","# Number of training epochs. The BERT authors recommend between 2 and 4.\n","N_EPOCHS_r = 2\n","\n","LRATE_r = 3e-5\n","FRATE_r = 5e-5\n","EPS_r = 1e-8\n","WU_r = 0.1\n","WDECAY_r = 0.5\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","TOTSTEPS_r = len(train_dataloader) * N_EPOCHS_r\n","WUSTEPS_r = int(TOTSTEPS_r * WU_r)\n","\n","# Apply weight decay to all parameters other than bias and layer normalization terms\n","# Optimize the parameters of the head layer by the learning rate\n","# Optimize the parameters of the pretrain LM by the fine-tuning rate\n","no_decay_r = ['bias', 'LayerNorm.weight']\n","named_parameters_r = real_model_ber.named_parameters()\n","optimizer_grouped_parameters_r = [\n","    {'params': [p for n, p in named_parameters_r if not any(nd in n for nd in no_decay_r)], 'weight_decay': WDECAY_r},\n","    {'params': [p for n, p in named_parameters_r if any(nd in n for nd in no_decay_r)], 'weight_decay': 0.0}\n","]"],"execution_count":270,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"haIhtaTjjOXz","colab":{},"executionInfo":{"status":"ok","timestamp":1595469672983,"user_tz":-60,"elapsed":901,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}}},"source":["# Hyperparameters for the real task LM (2nd v):\n","\n","# Number of training epochs. The BERT authors recommend between 2 and 4.\n","N_EPOCHS_r = 1\n","\n","LRATE_r = 8e-3\n","FRATE_r = 3e-5\n","EPS_r = 1e-8\n","WU_r = 0.3\n","WDECAY_r = 0.05\n","\n","\"\"\"N_EPOCHS_r = 2\n","\n","LRATE_r = 5e-3\n","FRATE_r = 2e-5\n","EPS_r = 1e-8\n","WU_r = 0.1\n","WDECAY_r = 0.1\"\"\"\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","TOTSTEPS_r = len(train_dataloader) * N_EPOCHS_r * 2\n","WUSTEPS_r = int(TOTSTEPS_r * WU_r)\n","\n","# Apply weight decay to all parameters other than bias and layer normalization terms\n","# Optimize the parameters of the head layer by the learning rate\n","# Optimize the parameters of the pretrain LM by the fine-tuning rate\n","no_decay_r = ['bias', 'LayerNorm.weight']\n","named_parameters_r = real_model_ber.named_parameters()\n","optimizer_grouped_parameters_r = [\n","    {'params': [p for n, p in named_parameters_r if not any(nd in n for nd in no_decay_r)], 'weight_decay': WDECAY_r},\n","    {'params': [p for n, p in named_parameters_r if any(nd in n for nd in no_decay_r)], 'weight_decay': 0.0},\n","    {'params': [p for n, p in named_parameters_r if \"albert\" not in n], 'lr': LRATE_r}\n","]\n","#    {'params': [p for n, p in named_parameters if \"albert\" in n], 'lr': FRATE},"],"execution_count":298,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kLmrF1nesQl-"},"source":["## Optimizer & learning rate scheduler"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OdB7Ept2smOu","colab":{},"executionInfo":{"status":"ok","timestamp":1595469674510,"user_tz":-60,"elapsed":704,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}}},"source":["# Create the optimizer, \n","# the epsilon parameter is a very small number to prevent any division by zero\n","optimizer_r = AdamW(optimizer_grouped_parameters_r, lr=FRATE_r, eps = EPS_r)"],"execution_count":299,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YEP7UJ1rsqqs","colab":{},"executionInfo":{"status":"ok","timestamp":1595469674816,"user_tz":-60,"elapsed":498,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}}},"source":["# Create the learning rate scheduler.\n","scheduler_r = get_linear_schedule_with_warmup(optimizer_r, \n","                                            num_warmup_steps = WUSTEPS_r,\n","                                            num_training_steps = TOTSTEPS_r)"],"execution_count":300,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PM-Gpf7iI5jp","colab_type":"text"},"source":["## Real task training"]},{"cell_type":"code","metadata":{"id":"G8UtGb2dEwZ5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1595469873256,"user_tz":-60,"elapsed":197419,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"f878b147-a0f2-4cb4-ff08-3cc5b562981c"},"source":["criterion_c = nn.CrossEntropyLoss()\n","\n","train_loss_r, train_accuracy_r, val_loss_r, val_accuracy_r, model_list_r = train_BERT(real_model_ber,\n","                                                                            train_dataloader,\n","                                                                            valid_dataloader,\n","                                                                            optimizer_r,\n","                                                                            scheduler_r,\n","                                                                            criterion_c,\n","                                                                            N_EPOCHS_r)\n"],"execution_count":301,"outputs":[{"output_type":"stream","text":["Epoch: 01 | Epoch Time: 3m 7s\n","\tTrain Loss: 0.95008 | Train Acc: 49.9981%\n","\t Val. Loss: 0.94389 |  Val. Acc: 54.5608%\n","\n","***Completed***\n","Total time spent: 3m 16s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2m3M1cTNYNSu","colab_type":"text"},"source":["# Testing Real Task Models"]},{"cell_type":"markdown","metadata":{"id":"b-l7wTEcYWIX","colab_type":"text"},"source":["## Start testing"]},{"cell_type":"code","metadata":{"id":"fk0nsayzYNtb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595469899693,"user_tz":-60,"elapsed":11948,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"62a9464e-4711-48aa-d025-425647861132"},"source":["fix_seed()\n","\n","test_loss = 0\n","test_acc = 0\n","test_logits_all = []\n","\n","# Turn on evaluate mode. This de-activates dropout. \n","real_model_ber.eval()\n","\n","# We do not compute gradients within this block, i.e. no training\n","with torch.no_grad():\n","\n","    for input_ids_batch, attention_mask_batch, token_type_ids_batch, labels in test_dataloader:\n","        \n","        # get the output\n","        predictions = real_model_ber(input_ids_batch,\n","                                     attention_mask_batch,\n","                                     token_type_ids_batch)\n","\n","        loss_batch = criterion_c(predictions, labels)\n","        test_logits_all += predictions.tolist()\n","        #test_logits_all = torch.cat((test_logits_all, predictions), 0)\n","        acc_batch = accuracy(predictions, labels)\n","\n","        test_loss += loss_batch.item()\n","        test_acc += acc_batch.item()\n","\n","    average_test_loss = test_loss / len(test_dataloader)\n","    average_test_acc = test_acc / len(test_dataloader)\n","\n","print(f'Test Loss: {average_test_loss:.5f} | Test Acc: {average_test_acc*100:.7f}%')"],"execution_count":302,"outputs":[{"output_type":"stream","text":["Test Loss: 0.95342 | Test Acc: 52.9391892%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ATFoGwAMshav","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595469901857,"user_tz":-60,"elapsed":903,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}}},"source":["#test_logits_all.shape"],"execution_count":303,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dmXyjzmkArMp","colab_type":"text"},"source":["## Write results"]},{"cell_type":"code","metadata":{"id":"sxAE4aJZQcyY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":86},"executionInfo":{"status":"ok","timestamp":1595469903613,"user_tz":-60,"elapsed":621,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"995a0b06-a1e7-47a9-a7c3-21d8cbc5f583"},"source":["def write_predictions(predictions, test_data_frame, out_loc):\n","    test_data_frame['pred'] = predictions\n","    output = test_data_frame[['id','pred']]\n","    output.to_csv(out_loc, index=False)\n","        \n","    print('Output file created:\\n\\t- '+os.path.abspath(out_loc))\n","\n","\n","# write the predictions for the dev data into 'task-2-output.csv'\n","out_loc = 'gdrive/My Drive/subtask-2/task-2-output.csv'\n","\n","test_class_preds = torch.argmax(F.softmax(torch.FloatTensor(test_logits_all), dim = 1), 1)\n","write_predictions(test_class_preds, test, out_loc)\n","\n","\"\"\"test_class_preds = torch.argmax(F.softmax(test_logits_all, dim = 1), 1)\n","write_predictions(test_class_preds.cpu().numpy(), test, out_loc)\"\"\""],"execution_count":304,"outputs":[{"output_type":"stream","text":["Output file created:\n","\t- /content/gdrive/My Drive/subtask-2/task-2-output.csv\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'test_class_preds = torch.argmax(F.softmax(test_logits_all, dim = 1), 1)\\nwrite_predictions(test_class_preds.cpu().numpy(), test, out_loc)'"]},"metadata":{"tags":[]},"execution_count":304}]},{"cell_type":"markdown","metadata":{"id":"WvRyvLeCFaPD","colab_type":"text"},"source":["## Check final results"]},{"cell_type":"code","metadata":{"id":"IzKVlMTJ88tg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595469904788,"user_tz":-60,"elapsed":453,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"66b49473-459d-4de4-ecf4-330f5c3c2242"},"source":["def score(truth_loc, prediction_loc):\n","    truth = pd.read_csv(truth_loc, usecols=['id','label'])\n","    pred = pd.read_csv(prediction_loc, usecols=['id','pred'])\n","         \n","    assert(sorted(truth.id) == sorted(pred.id)),\"ID mismatch between ground truth and prediction!\"\n","    \n","    data = pd.merge(truth,pred)\n","    data = data[data.label != 0]\n","    accuracy = (np.sum(data.label == data.pred)*1.0/len(data))*100\n","    \n","    print(\"Accuracy = %.6f\" % accuracy)\n","\n","    return accuracy   \n","\n","# print Accuracy\n","truth_loc = 'gdrive/My Drive/subtask-2/test.csv'\n","prediction_loc = 'gdrive/My Drive/subtask-2/task-2-output.csv'\n","test_accuracy = score(truth_loc, prediction_loc)"],"execution_count":305,"outputs":[{"output_type":"stream","text":["Accuracy = 50.304414\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Tyk_hoF8safd","colab_type":"text"},"source":["# Logging Real Task Statistics"]},{"cell_type":"code","metadata":{"id":"DwpzJ5MG886_","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595469907328,"user_tz":-60,"elapsed":694,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}}},"source":["B1 = \"bert-base-uncased\"\n","A2 = \"albert-base-v2\"\n","E = \"electra\"\n","B1M = \"bert-base-uncased more_data\"\n","\n","real_task_stats, log_num_r = add_real_task_stats(real_task_stats, \n","                                             log_num_r,\n","                                             B1M,\n","                                             BATCH_SIZE, \n","                                             N_EPOCHS_r,\n","                                             \"{:.0e}\".format(LRATE_r),\n","                                             \"{:.0e}\".format(FRATE_r), \n","                                             \"{:.0e}\".format(EPS_r), \n","                                             WU_r,\n","                                             WDECAY_r, \n","                                             train_loss_r, \n","                                             train_accuracy_r*100, \n","                                             val_loss_r, \n","                                             val_accuracy_r*100,\n","                                             test_accuracy\n","                                             )"],"execution_count":306,"outputs":[]},{"cell_type":"code","metadata":{"id":"PMDDYWnLsZiO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":865},"executionInfo":{"status":"ok","timestamp":1595469981734,"user_tz":-60,"elapsed":945,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"7cfdb02e-1147-4354-bb72-4d21eb76074b"},"source":["# Display floats with five decimal places.\n","pd.set_option('precision', 5)\n","\n","# Create a DataFrame from our training statistics.\n","df_stats = pd.DataFrame(data=real_task_stats)\n","\n","# Use the 'log' as the row index.\n","df_stats = df_stats.set_index('log')\n","\n","# Display the table.\n","df_stats"],"execution_count":307,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Model Name</th>\n","      <th>Batch Size</th>\n","      <th>N_Epochs</th>\n","      <th>lr</th>\n","      <th>fr</th>\n","      <th>eps</th>\n","      <th>wu</th>\n","      <th>wd</th>\n","      <th>Training Loss</th>\n","      <th>Training Accur.</th>\n","      <th>Valid. Loss</th>\n","      <th>Valid. Accur.</th>\n","      <th>Testing Accur.</th>\n","    </tr>\n","    <tr>\n","      <th>log</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>bert-base-uncased</td>\n","      <td>16</td>\n","      <td>3</td>\n","      <td>3e-05</td>\n","      <td>5e-05</td>\n","      <td>1e-08</td>\n","      <td>0.1</td>\n","      <td>0.01</td>\n","      <td>0.23012</td>\n","      <td>92.14225</td>\n","      <td>1.68649</td>\n","      <td>51.29505</td>\n","      <td>46.34703</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>bert-base-uncased</td>\n","      <td>16</td>\n","      <td>1</td>\n","      <td>3e-05</td>\n","      <td>5e-05</td>\n","      <td>1e-08</td>\n","      <td>0.1</td>\n","      <td>0.01</td>\n","      <td>0.30659</td>\n","      <td>88.45400</td>\n","      <td>1.79389</td>\n","      <td>49.22579</td>\n","      <td>44.71081</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>bert-base-uncased</td>\n","      <td>16</td>\n","      <td>1</td>\n","      <td>3e-05</td>\n","      <td>5e-05</td>\n","      <td>1e-08</td>\n","      <td>0.1</td>\n","      <td>0.01</td>\n","      <td>0.10740</td>\n","      <td>96.72913</td>\n","      <td>2.53388</td>\n","      <td>46.77646</td>\n","      <td>41.55251</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>bert-base-uncased</td>\n","      <td>16</td>\n","      <td>1</td>\n","      <td>3e-05</td>\n","      <td>5e-05</td>\n","      <td>1e-08</td>\n","      <td>0.1</td>\n","      <td>0.10</td>\n","      <td>0.04975</td>\n","      <td>98.69889</td>\n","      <td>2.84458</td>\n","      <td>47.62106</td>\n","      <td>41.93303</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>bert-base-uncased</td>\n","      <td>16</td>\n","      <td>1</td>\n","      <td>3e-05</td>\n","      <td>5e-05</td>\n","      <td>1e-08</td>\n","      <td>0.1</td>\n","      <td>0.10</td>\n","      <td>0.96322</td>\n","      <td>47.40630</td>\n","      <td>0.94663</td>\n","      <td>52.36486</td>\n","      <td>49.42922</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>bert-base-uncased</td>\n","      <td>16</td>\n","      <td>3</td>\n","      <td>3e-05</td>\n","      <td>5e-05</td>\n","      <td>1e-08</td>\n","      <td>0.1</td>\n","      <td>0.10</td>\n","      <td>0.29321</td>\n","      <td>89.28876</td>\n","      <td>1.63454</td>\n","      <td>53.40653</td>\n","      <td>46.27093</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>bert-base-uncased</td>\n","      <td>16</td>\n","      <td>2</td>\n","      <td>3e-05</td>\n","      <td>5e-05</td>\n","      <td>1e-08</td>\n","      <td>0.1</td>\n","      <td>0.50</td>\n","      <td>0.82296</td>\n","      <td>63.49446</td>\n","      <td>0.95054</td>\n","      <td>56.58784</td>\n","      <td>50.00000</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>bert-base-uncased</td>\n","      <td>16</td>\n","      <td>2</td>\n","      <td>5e-03</td>\n","      <td>2e-05</td>\n","      <td>1e-08</td>\n","      <td>0.1</td>\n","      <td>0.01</td>\n","      <td>0.86409</td>\n","      <td>60.43654</td>\n","      <td>0.93353</td>\n","      <td>56.84122</td>\n","      <td>50.53272</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>bert-base-uncased</td>\n","      <td>16</td>\n","      <td>2</td>\n","      <td>5e-03</td>\n","      <td>2e-05</td>\n","      <td>1e-08</td>\n","      <td>0.1</td>\n","      <td>0.70</td>\n","      <td>0.86533</td>\n","      <td>60.06388</td>\n","      <td>0.93377</td>\n","      <td>57.01014</td>\n","      <td>50.34247</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>bert-base-uncased</td>\n","      <td>16</td>\n","      <td>2</td>\n","      <td>5e-03</td>\n","      <td>2e-05</td>\n","      <td>1e-08</td>\n","      <td>0.3</td>\n","      <td>0.01</td>\n","      <td>0.89612</td>\n","      <td>57.24446</td>\n","      <td>0.93043</td>\n","      <td>56.02477</td>\n","      <td>49.61948</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>bert-base-uncased</td>\n","      <td>16</td>\n","      <td>2</td>\n","      <td>5e-03</td>\n","      <td>2e-05</td>\n","      <td>1e-08</td>\n","      <td>0.1</td>\n","      <td>0.01</td>\n","      <td>0.95354</td>\n","      <td>49.12266</td>\n","      <td>0.96245</td>\n","      <td>46.73423</td>\n","      <td>49.92390</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>bert-base-uncased</td>\n","      <td>16</td>\n","      <td>3</td>\n","      <td>5e-03</td>\n","      <td>2e-05</td>\n","      <td>1e-08</td>\n","      <td>0.1</td>\n","      <td>0.90</td>\n","      <td>0.70490</td>\n","      <td>70.56005</td>\n","      <td>1.05383</td>\n","      <td>52.02703</td>\n","      <td>50.03805</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>bert-base-uncased more_data</td>\n","      <td>16</td>\n","      <td>3</td>\n","      <td>5e-03</td>\n","      <td>2e-05</td>\n","      <td>1e-08</td>\n","      <td>0.1</td>\n","      <td>0.90</td>\n","      <td>0.27673</td>\n","      <td>90.17296</td>\n","      <td>1.45370</td>\n","      <td>50.78829</td>\n","      <td>49.80974</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>bert-base-uncased more_data</td>\n","      <td>16</td>\n","      <td>1</td>\n","      <td>5e-03</td>\n","      <td>2e-05</td>\n","      <td>1e-08</td>\n","      <td>0.1</td>\n","      <td>0.10</td>\n","      <td>0.94737</td>\n","      <td>49.64061</td>\n","      <td>0.93668</td>\n","      <td>53.23761</td>\n","      <td>50.19026</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>bert-base-uncased more_data</td>\n","      <td>16</td>\n","      <td>2</td>\n","      <td>5e-03</td>\n","      <td>2e-05</td>\n","      <td>1e-08</td>\n","      <td>0.1</td>\n","      <td>0.10</td>\n","      <td>0.69936</td>\n","      <td>71.25356</td>\n","      <td>1.04068</td>\n","      <td>55.22241</td>\n","      <td>50.41857</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>bert-base-uncased more_data</td>\n","      <td>16</td>\n","      <td>1</td>\n","      <td>8e-03</td>\n","      <td>3e-05</td>\n","      <td>1e-08</td>\n","      <td>0.3</td>\n","      <td>0.05</td>\n","      <td>0.95008</td>\n","      <td>49.99813</td>\n","      <td>0.94389</td>\n","      <td>54.56081</td>\n","      <td>50.30441</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                      Model Name  Batch Size  ...  Valid. Accur. Testing Accur.\n","log                                           ...                              \n","1              bert-base-uncased          16  ...       51.29505       46.34703\n","2              bert-base-uncased          16  ...       49.22579       44.71081\n","3              bert-base-uncased          16  ...       46.77646       41.55251\n","4              bert-base-uncased          16  ...       47.62106       41.93303\n","5              bert-base-uncased          16  ...       52.36486       49.42922\n","6              bert-base-uncased          16  ...       53.40653       46.27093\n","7              bert-base-uncased          16  ...       56.58784       50.00000\n","8              bert-base-uncased          16  ...       56.84122       50.53272\n","9              bert-base-uncased          16  ...       57.01014       50.34247\n","10             bert-base-uncased          16  ...       56.02477       49.61948\n","11             bert-base-uncased          16  ...       46.73423       49.92390\n","12             bert-base-uncased          16  ...       52.02703       50.03805\n","13   bert-base-uncased more_data          16  ...       50.78829       49.80974\n","14   bert-base-uncased more_data          16  ...       53.23761       50.19026\n","15   bert-base-uncased more_data          16  ...       55.22241       50.41857\n","16   bert-base-uncased more_data          16  ...       54.56081       50.30441\n","\n","[16 rows x 13 columns]"]},"metadata":{"tags":[]},"execution_count":307}]},{"cell_type":"markdown","metadata":{"id":"qFQZM8plDQyK","colab_type":"text"},"source":["## Write statistics"]},{"cell_type":"code","metadata":{"id":"Fg7-KYSf58QC","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595469982595,"user_tz":-60,"elapsed":486,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}}},"source":["log_loc = 'gdrive/My Drive/subtask-2/log2.csv'\n","df_stats.to_csv(log_loc, index=False)"],"execution_count":308,"outputs":[]},{"cell_type":"code","metadata":{"id":"L1ZBR3Gx7Egy","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595469984565,"user_tz":-60,"elapsed":676,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}}},"source":[""],"execution_count":308,"outputs":[]},{"cell_type":"code","metadata":{"id":"ljGh7wNTs03L","colab_type":"code","colab":{}},"source":["# remove the last row:\n","#training_stats.pop(-1)\n","#log_num -= 1"],"execution_count":null,"outputs":[]}]}