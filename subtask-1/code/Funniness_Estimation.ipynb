{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Funniness_Estimation.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoAuoSxKZogJ",
        "colab_type": "text"
      },
      "source": [
        "# Funniness Estimation System v1.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGWNugYXLid4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "@author: Ziyang Lin\n",
        "         zlin19@sheffield.ac.uk\n",
        "         University of Sheffield, UK\n",
        "\"\"\"\n",
        "\n",
        "'''\n",
        "A two inputs NN regression system for\n",
        "\"Assessing the Funniness of Edited News Headlines (SemEval-2020)\" task 1\n",
        "in which given the original and the edited headline, the system\n",
        "is required to predict the mean funniness of the edited headline.\n",
        "'''\n",
        "\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import re\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "\n",
        "# fix the seeds to get consistent results before every training\n",
        "# loop in what follows\n",
        "def fix_seed(seed=234):\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcLgdOc_yGsF",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DkamEV-LvZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def processed_data_to_lists(train):\n",
        "    headls_words = [(origin_headl, new_word) for (origin_headl, new_word) in zip(train.original.to_list(), train.edit.to_list())]\n",
        "    labels_list = train.meanGrade.to_list()\n",
        "\n",
        "    # list of tuple for original headlines and new edited headlines\n",
        "    o_headls_n_headls = []\n",
        "    \n",
        "    new_word_list = []\n",
        "\n",
        "    for origin_headl, new_word in headls_words:\n",
        "      # pattern\n",
        "      p = re.compile(r'\\<(.*?)\\/\\>')\n",
        "      # get the normal version of the original headline\n",
        "      origin_word = ''.join(re.findall(p, origin_headl))\n",
        "      normal_origin_headl = p.sub(origin_word, origin_headl)\n",
        "      # get the new edited headline\n",
        "      new_headl = p.sub(new_word, origin_headl)\n",
        "      # pair them and put them into the list\n",
        "      o_headls_n_headls.append((normal_origin_headl,new_headl))\n",
        "\n",
        "      new_word_list.append(new_word)\n",
        "\n",
        "    return o_headls_n_headls, labels_list, new_word_list\n",
        "\n",
        "\n",
        "# tokenize both the original headlines and the corresponding new edited headlines\n",
        "def get_tokenized_headls(o_headls_n_headls):\n",
        "    tokenized_headls = [] \n",
        "    for origin_headl, new_headl in o_headls_n_headls:\n",
        "      origin_headl = \" \".join(word_tokenize(origin_headl))\n",
        "      new_headl = \" \".join(word_tokenize(new_headl))    \n",
        "\n",
        "      tokenized_origin = []\n",
        "      tokenized_new = []\n",
        "\n",
        "      for token in origin_headl.split(' '):\n",
        "        token = token.lower()\n",
        "        tokenized_origin.append(token)\n",
        "\n",
        "      for token in new_headl.split(' '):\n",
        "        token = token.lower()\n",
        "        tokenized_new.append(token)\n",
        "\n",
        "      tokenized_headls.append((tokenized_origin, tokenized_new))\n",
        "\n",
        "    return tokenized_headls\n",
        "\n",
        "\n",
        "def get_word2idx(tokenized_headls, new_word_list):\n",
        "    vocabulary = []\n",
        "    for origin_headl, new_headl in tokenized_headls:\n",
        "      for token in origin_headl:\n",
        "          if token not in vocabulary:\n",
        "              vocabulary.append(token)\n",
        "              \n",
        "    for token in new_word_list:\n",
        "      if token not in vocabulary:\n",
        "          vocabulary.append(token)\n",
        "  \n",
        "    word2idx = {w: idx+1 for (idx, w) in enumerate(vocabulary)}\n",
        "    # we reserve the 0 index for the padding token\n",
        "    word2idx['<pad>'] = 0\n",
        "      \n",
        "    return word2idx\n",
        "\n",
        "\n",
        "def get_model_inputs(tokenized_headls, word2idx, labels):\n",
        "    # we index our original headlines and the corresponding new edited headlines\n",
        "    vectorized_headls = [([word2idx[tk] for tk in origin if tk in word2idx],[word2idx[tk] for tk in new if tk in word2idx]) for origin, new in tokenized_headls]\n",
        "\n",
        "    # the original headlines lengths and the new headlines lengths\n",
        "    origin_headl_lengths = [len(origin_headl) for origin_headl, new_headl in vectorized_headls]\n",
        "    new_headl_lengths = [len(new_headl) for origin_headl, new_headl in vectorized_headls]\n",
        "\n",
        "    # Get maximum length\n",
        "    max_len = max(origin_headl_lengths)\n",
        "    \n",
        "    # we create two tensors of the same fixed size filled with zeroes for padding\n",
        "    origin_tensor = torch.zeros((len(vectorized_headls), max_len)).long()\n",
        "    new_tensor = torch.zeros((len(vectorized_headls), max_len)).long()\n",
        "\n",
        "    # we fill them with our vectorized headlines \n",
        "    for idx, ((origin_headl, new_headl), origin_headllen) in enumerate(zip(vectorized_headls, origin_headl_lengths)):\n",
        "      origin_tensor[idx, :origin_headllen] = torch.LongTensor(origin_headl)\n",
        "\n",
        "    for idx, ((origin_headl, new_headl), new_headllen) in enumerate(zip(vectorized_headls, new_headl_lengths)):\n",
        "      new_tensor[idx, :new_headllen] = torch.LongTensor(new_headl)  \n",
        "\n",
        "    # Label tensor\n",
        "    label_tensor = torch.FloatTensor(labels)\n",
        "    \n",
        "    return origin_tensor, new_tensor, label_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwA2cJmXua1b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "c6d1f1f0-6a2f-422b-ec46-adb5f8926c94"
      },
      "source": [
        "train_loc = 'gdrive/My Drive/subtask-1/train.csv'\n",
        "test_loc = 'gdrive/My Drive/subtask-1/dev.csv'\n",
        "train = pd.read_csv(train_loc)    \n",
        "test = pd.read_csv(test_loc)\n",
        "\n",
        "# Prepare the training corpus and labels\n",
        "o_headls_n_headls, labels_list, new_word_list = processed_data_to_lists(train)\n",
        "tokenized_headls = get_tokenized_headls(o_headls_n_headls)\n",
        "word2idx = get_word2idx(tokenized_headls, new_word_list)\n",
        "origin_tensor, new_tensor, label_tensor = get_model_inputs(tokenized_headls, word2idx, labels_list)\n",
        "\n",
        "print('origin_tensor:')\n",
        "print(origin_tensor)\n",
        "print('new_tensor:')\n",
        "print(new_tensor)\n",
        "print('label_tensor:')\n",
        "print(label_tensor)\n",
        "print('vocab_size:')\n",
        "print(len(word2idx))\n",
        "\n",
        "print()\n",
        "print()\n",
        "\n",
        "# Prepare the validation corpus and labels\n",
        "valid_o_headls_n_headls, valid_labels_list, valid_new_word_list = processed_data_to_lists(test)\n",
        "valid_tokenized_headls = get_tokenized_headls(valid_o_headls_n_headls)\n",
        "valid_origin_tensor, valid_new_tensor, valid_label_tensor = get_model_inputs(valid_tokenized_headls, word2idx, valid_labels_list)\n",
        "\n",
        "print('valid_origin_tensor:')\n",
        "print(valid_origin_tensor)\n",
        "print('valid_new_tensor:')\n",
        "print(valid_new_tensor)\n",
        "print('valid_label_tensor:')\n",
        "print(valid_label_tensor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "origin_tensor:\n",
            "tensor([[   1,    2,    3,  ...,    0,    0,    0],\n",
            "        [  16,   17,   18,  ...,    0,    0,    0],\n",
            "        [  32,   33,   34,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [5728, 2737, 5729,  ...,    0,    0,    0],\n",
            "        [7010,   80, 2169,  ...,    0,    0,    0],\n",
            "        [ 105,   93,   27,  ...,    0,    0,    0]])\n",
            "new_tensor:\n",
            "tensor([[   1,    2,    3,  ...,    0,    0,    0],\n",
            "        [  16,   17,   18,  ...,    0,    0,    0],\n",
            "        [  32,   33,   34,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [5728, 2737, 5729,  ...,    0,    0,    0],\n",
            "        [7010,   80, 2169,  ...,    0,    0,    0],\n",
            "        [ 105,   93,   27,  ...,    0,    0,    0]])\n",
            "label_tensor:\n",
            "tensor([0.2000, 1.6000, 1.0000,  ..., 0.6000, 1.4000, 0.4000])\n",
            "vocab_size:\n",
            "11722\n",
            "\n",
            "\n",
            "valid_origin_tensor:\n",
            "tensor([[1674,  323, 1832,  ...,    0,    0,    0],\n",
            "        [ 509, 2944,  855,  ...,    0,    0,    0],\n",
            "        [1598,   80,  749,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [  88,  903,  398,  ...,    0,    0,    0],\n",
            "        [5947, 2501,  234,  ...,    0,    0,    0],\n",
            "        [  14, 3059,  323,  ...,    0,    0,    0]])\n",
            "valid_new_tensor:\n",
            "tensor([[1674,  323, 1832,  ...,    0,    0,    0],\n",
            "        [ 509, 1850,  855,  ...,    0,    0,    0],\n",
            "        [1598,   80,  749,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [  88,  903,  398,  ...,    0,    0,    0],\n",
            "        [5947, 2501,  234,  ...,    0,    0,    0],\n",
            "        [  14, 3059,  323,  ...,    0,    0,    0]])\n",
            "valid_label_tensor:\n",
            "tensor([1.0000, 0.8000, 0.6000,  ..., 1.4000, 1.4000, 0.6000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmxNDiOEMu2N",
        "colab_type": "text"
      },
      "source": [
        "# Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCraGcVm5voH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f59f56f2-cec2-40db-b6d8-8f8fcb58af61"
      },
      "source": [
        "class TwoInputsNN(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim_1, hidden_dim_2, hidden_dim_3, vocab_size):  \n",
        "        super(TwoInputsNN, self).__init__()\n",
        "        \n",
        "        # embedding (lookup layer) layer\n",
        "        # padding_idx argument makes sure that the 0-th token in the vocabulary\n",
        "        # is used for padding purposes i.e. its embedding will be a 0-vector\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        \n",
        "        # hidden layer 1\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim_1)\n",
        "\n",
        "        # hidden layer 2\n",
        "        self.fc2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
        "        \n",
        "        # activation\n",
        "        self.relu1 = nn.ReLU()\n",
        "        \n",
        "        # hidden layer 3\n",
        "        self.fc3 = nn.Linear(hidden_dim_2, hidden_dim_3)\n",
        "\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # tensor x and tensor y have shape (batch_size, max_headl_len)\n",
        "        \n",
        "        # put x into embedding layer\n",
        "        x_embedded = self.embedding(x)\n",
        "        # Now `embedding` has shape (batch size, max_headl_len, embedding dim)\n",
        "        # Compute the average embeddings of shape (batch_size, embedding_dim)\n",
        "        # Implement averaging that ignores padding (average using actual headline lengths).        \n",
        "        x_headl_lens = x.ne(0).sum(1, keepdims=True)\n",
        "        x_averaged = x_embedded.sum(1) / x_headl_lens\n",
        "\n",
        "        # put y into embedding layer\n",
        "        y_embedded = self.embedding(y)       \n",
        "        y_headl_lens = y.ne(0).sum(1, keepdims=True)\n",
        "        y_averaged = y_embedded.sum(1) / y_headl_lens\n",
        "\n",
        "        # hidden layer 1\n",
        "        x_out = self.fc1(x_averaged)\n",
        "        y_out = self.fc1(y_averaged)\n",
        "\n",
        "        x_out = self.relu1(x_out)\n",
        "        y_out = self.relu1(y_out)\n",
        "\n",
        "        # hidden layer 2\n",
        "        x_out = self.fc2(x_out)\n",
        "        y_out = self.fc2(y_out)\n",
        "\n",
        "        x_out = self.relu1(x_out)\n",
        "        y_out = self.relu1(y_out)\n",
        "\n",
        "        # hidden layer 3\n",
        "        x_out = self.fc3(x_out)\n",
        "        y_out = self.fc3(y_out)\n",
        "\n",
        "\n",
        "        # output layer\n",
        "        out = x_out * y_out \n",
        "        out = torch.sum(out, 1, keepdim = True)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xd-1ZG3EM81c",
        "colab_type": "text"
      },
      "source": [
        "# Start Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0S6tmLExueeH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "76a44396-58cd-4e99-fa51-8258d3c7d951"
      },
      "source": [
        "# Reset the seed before every model construction for reproducible results\n",
        "fix_seed()\n",
        "\n",
        "# we will train for N epochs (The model will see the corpus N times)\n",
        "EPOCHS = 100\n",
        "\n",
        "# Learning rate is initially set to 0.145\n",
        "LRATE = 0.145\n",
        "\n",
        "# we define our embedding dimension (dimensionality of the output of the first layer)\n",
        "EMBEDDING_DIM = 300\n",
        "\n",
        "# dimensionality of the output of the second hidden layer\n",
        "HIDDEN_DIM_1 = 100\n",
        "\n",
        "# dimensionality of the output of the third hidden layer\n",
        "HIDDEN_DIM_2 = 50\n",
        "\n",
        "# dimensionality of the output of the fourth hidden layer\n",
        "HIDDEN_DIM_3 = 10\n",
        "\n",
        "# Construct the model\n",
        "model = TwoInputsNN(EMBEDDING_DIM, HIDDEN_DIM_1, HIDDEN_DIM_2, HIDDEN_DIM_3, len(word2idx))\n",
        "\n",
        "# Print the model\n",
        "print(model)\n",
        "\n",
        "# we use the stochastic gradient descent (SGD) optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=LRATE)\n",
        "\n",
        "# schedule learning rate using scheduler\n",
        "steps = 100\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, steps)\n",
        "\n",
        "# Input and label tensors for training\n",
        "x_feature = origin_tensor\n",
        "y_feature = new_tensor\n",
        "target = label_tensor\n",
        "\n",
        "# Input and label tensors for validation\n",
        "valid_x_feature = valid_origin_tensor\n",
        "valid_y_feature = valid_new_tensor\n",
        "valid_target = valid_label_tensor\n",
        "\n",
        "\n",
        "################\n",
        "# Start training\n",
        "################\n",
        "print(f'Will train for {EPOCHS} epochs')\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  # to ensure the dropout (explained later) is \"turned on\" while training\n",
        "  # good practice to include even if do not use here\n",
        "  model.train()\n",
        "  \n",
        "  # we zero the gradients as they are not removed automatically\n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  # squeeze is needed as the predictions will have the shape (batch size, 1)\n",
        "  # and we need to remove the dimension of size 1\n",
        "  predictions = model(x_feature, y_feature).squeeze(1)\n",
        "\n",
        "  # Compute here the RMSE loss\n",
        "  loss = torch.sqrt(((predictions - target)**2).mean())\n",
        "  train_loss = loss.item()\n",
        "\n",
        "  # calculate the gradient of each parameter\n",
        "  loss.backward()\n",
        "\n",
        "  # update the parameters using the gradients and optimizer algorithm \n",
        "  optimizer.step()\n",
        "  \n",
        "  # update the learning rate\n",
        "  scheduler.step()\n",
        "\n",
        "  # this puts the model in \"evaluation mode\" (turns off dropout and batch normalization)\n",
        "  # good practise to include even if we do not use them right now\n",
        "  model.eval()\n",
        "\n",
        "  # we do not compute gradients within this block, i.e. no training\n",
        "  with torch.no_grad():\n",
        "    valid_predictions = model(valid_x_feature, valid_y_feature).squeeze(1)\n",
        "    valid_loss = torch.sqrt(((valid_predictions - valid_target)**2).mean()).item()\n",
        "  \n",
        "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f} |')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TwoInputsNN(\n",
            "  (embedding): Embedding(11722, 300, padding_idx=0)\n",
            "  (fc1): Linear(in_features=300, out_features=100, bias=True)\n",
            "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
            "  (relu1): ReLU()\n",
            "  (fc3): Linear(in_features=50, out_features=10, bias=True)\n",
            ")\n",
            "Will train for 100 epochs\n",
            "| Epoch: 01 | Train Loss: 1.050 | Val. Loss: 1.007 |\n",
            "| Epoch: 02 | Train Loss: 1.012 | Val. Loss: 0.938 |\n",
            "| Epoch: 03 | Train Loss: 0.944 | Val. Loss: 0.819 |\n",
            "| Epoch: 04 | Train Loss: 0.826 | Val. Loss: 0.653 |\n",
            "| Epoch: 05 | Train Loss: 0.660 | Val. Loss: 0.589 |\n",
            "| Epoch: 06 | Train Loss: 0.590 | Val. Loss: 0.586 |\n",
            "| Epoch: 07 | Train Loss: 0.589 | Val. Loss: 0.587 |\n",
            "| Epoch: 08 | Train Loss: 0.588 | Val. Loss: 0.584 |\n",
            "| Epoch: 09 | Train Loss: 0.587 | Val. Loss: 0.586 |\n",
            "| Epoch: 10 | Train Loss: 0.586 | Val. Loss: 0.583 |\n",
            "| Epoch: 11 | Train Loss: 0.586 | Val. Loss: 0.584 |\n",
            "| Epoch: 12 | Train Loss: 0.585 | Val. Loss: 0.583 |\n",
            "| Epoch: 13 | Train Loss: 0.584 | Val. Loss: 0.583 |\n",
            "| Epoch: 14 | Train Loss: 0.584 | Val. Loss: 0.582 |\n",
            "| Epoch: 15 | Train Loss: 0.583 | Val. Loss: 0.583 |\n",
            "| Epoch: 16 | Train Loss: 0.583 | Val. Loss: 0.582 |\n",
            "| Epoch: 17 | Train Loss: 0.583 | Val. Loss: 0.582 |\n",
            "| Epoch: 18 | Train Loss: 0.582 | Val. Loss: 0.582 |\n",
            "| Epoch: 19 | Train Loss: 0.582 | Val. Loss: 0.582 |\n",
            "| Epoch: 20 | Train Loss: 0.582 | Val. Loss: 0.582 |\n",
            "| Epoch: 21 | Train Loss: 0.582 | Val. Loss: 0.582 |\n",
            "| Epoch: 22 | Train Loss: 0.581 | Val. Loss: 0.582 |\n",
            "| Epoch: 23 | Train Loss: 0.581 | Val. Loss: 0.581 |\n",
            "| Epoch: 24 | Train Loss: 0.581 | Val. Loss: 0.581 |\n",
            "| Epoch: 25 | Train Loss: 0.581 | Val. Loss: 0.581 |\n",
            "| Epoch: 26 | Train Loss: 0.580 | Val. Loss: 0.581 |\n",
            "| Epoch: 27 | Train Loss: 0.580 | Val. Loss: 0.581 |\n",
            "| Epoch: 28 | Train Loss: 0.580 | Val. Loss: 0.581 |\n",
            "| Epoch: 29 | Train Loss: 0.580 | Val. Loss: 0.581 |\n",
            "| Epoch: 30 | Train Loss: 0.580 | Val. Loss: 0.581 |\n",
            "| Epoch: 31 | Train Loss: 0.579 | Val. Loss: 0.581 |\n",
            "| Epoch: 32 | Train Loss: 0.579 | Val. Loss: 0.581 |\n",
            "| Epoch: 33 | Train Loss: 0.579 | Val. Loss: 0.581 |\n",
            "| Epoch: 34 | Train Loss: 0.579 | Val. Loss: 0.581 |\n",
            "| Epoch: 35 | Train Loss: 0.579 | Val. Loss: 0.581 |\n",
            "| Epoch: 36 | Train Loss: 0.579 | Val. Loss: 0.581 |\n",
            "| Epoch: 37 | Train Loss: 0.578 | Val. Loss: 0.581 |\n",
            "| Epoch: 38 | Train Loss: 0.578 | Val. Loss: 0.581 |\n",
            "| Epoch: 39 | Train Loss: 0.578 | Val. Loss: 0.581 |\n",
            "| Epoch: 40 | Train Loss: 0.578 | Val. Loss: 0.581 |\n",
            "| Epoch: 41 | Train Loss: 0.578 | Val. Loss: 0.581 |\n",
            "| Epoch: 42 | Train Loss: 0.578 | Val. Loss: 0.581 |\n",
            "| Epoch: 43 | Train Loss: 0.578 | Val. Loss: 0.581 |\n",
            "| Epoch: 44 | Train Loss: 0.577 | Val. Loss: 0.581 |\n",
            "| Epoch: 45 | Train Loss: 0.577 | Val. Loss: 0.581 |\n",
            "| Epoch: 46 | Train Loss: 0.577 | Val. Loss: 0.581 |\n",
            "| Epoch: 47 | Train Loss: 0.577 | Val. Loss: 0.581 |\n",
            "| Epoch: 48 | Train Loss: 0.577 | Val. Loss: 0.581 |\n",
            "| Epoch: 49 | Train Loss: 0.577 | Val. Loss: 0.581 |\n",
            "| Epoch: 50 | Train Loss: 0.577 | Val. Loss: 0.581 |\n",
            "| Epoch: 51 | Train Loss: 0.577 | Val. Loss: 0.581 |\n",
            "| Epoch: 52 | Train Loss: 0.577 | Val. Loss: 0.581 |\n",
            "| Epoch: 53 | Train Loss: 0.577 | Val. Loss: 0.581 |\n",
            "| Epoch: 54 | Train Loss: 0.577 | Val. Loss: 0.581 |\n",
            "| Epoch: 55 | Train Loss: 0.576 | Val. Loss: 0.581 |\n",
            "| Epoch: 56 | Train Loss: 0.576 | Val. Loss: 0.581 |\n",
            "| Epoch: 57 | Train Loss: 0.576 | Val. Loss: 0.581 |\n",
            "| Epoch: 58 | Train Loss: 0.576 | Val. Loss: 0.581 |\n",
            "| Epoch: 59 | Train Loss: 0.576 | Val. Loss: 0.581 |\n",
            "| Epoch: 60 | Train Loss: 0.576 | Val. Loss: 0.581 |\n",
            "| Epoch: 61 | Train Loss: 0.576 | Val. Loss: 0.581 |\n",
            "| Epoch: 62 | Train Loss: 0.576 | Val. Loss: 0.581 |\n",
            "| Epoch: 63 | Train Loss: 0.576 | Val. Loss: 0.581 |\n",
            "| Epoch: 64 | Train Loss: 0.576 | Val. Loss: 0.581 |\n",
            "| Epoch: 65 | Train Loss: 0.576 | Val. Loss: 0.581 |\n",
            "| Epoch: 66 | Train Loss: 0.576 | Val. Loss: 0.581 |\n",
            "| Epoch: 67 | Train Loss: 0.576 | Val. Loss: 0.581 |\n",
            "| Epoch: 68 | Train Loss: 0.576 | Val. Loss: 0.581 |\n",
            "| Epoch: 69 | Train Loss: 0.576 | Val. Loss: 0.581 |\n",
            "| Epoch: 70 | Train Loss: 0.576 | Val. Loss: 0.581 |\n",
            "| Epoch: 71 | Train Loss: 0.576 | Val. Loss: 0.581 |\n",
            "| Epoch: 72 | Train Loss: 0.576 | Val. Loss: 0.581 |\n",
            "| Epoch: 73 | Train Loss: 0.576 | Val. Loss: 0.581 |\n",
            "| Epoch: 74 | Train Loss: 0.576 | Val. Loss: 0.581 |\n",
            "| Epoch: 75 | Train Loss: 0.575 | Val. Loss: 0.581 |\n",
            "| Epoch: 76 | Train Loss: 0.575 | Val. Loss: 0.581 |\n",
            "| Epoch: 77 | Train Loss: 0.575 | Val. Loss: 0.581 |\n",
            "| Epoch: 78 | Train Loss: 0.575 | Val. Loss: 0.581 |\n",
            "| Epoch: 79 | Train Loss: 0.575 | Val. Loss: 0.581 |\n",
            "| Epoch: 80 | Train Loss: 0.575 | Val. Loss: 0.581 |\n",
            "| Epoch: 81 | Train Loss: 0.575 | Val. Loss: 0.581 |\n",
            "| Epoch: 82 | Train Loss: 0.575 | Val. Loss: 0.581 |\n",
            "| Epoch: 83 | Train Loss: 0.575 | Val. Loss: 0.581 |\n",
            "| Epoch: 84 | Train Loss: 0.575 | Val. Loss: 0.581 |\n",
            "| Epoch: 85 | Train Loss: 0.575 | Val. Loss: 0.581 |\n",
            "| Epoch: 86 | Train Loss: 0.575 | Val. Loss: 0.581 |\n",
            "| Epoch: 87 | Train Loss: 0.575 | Val. Loss: 0.581 |\n",
            "| Epoch: 88 | Train Loss: 0.575 | Val. Loss: 0.581 |\n",
            "| Epoch: 89 | Train Loss: 0.575 | Val. Loss: 0.581 |\n",
            "| Epoch: 90 | Train Loss: 0.575 | Val. Loss: 0.581 |\n",
            "| Epoch: 91 | Train Loss: 0.575 | Val. Loss: 0.581 |\n",
            "| Epoch: 92 | Train Loss: 0.575 | Val. Loss: 0.581 |\n",
            "| Epoch: 93 | Train Loss: 0.575 | Val. Loss: 0.581 |\n",
            "| Epoch: 94 | Train Loss: 0.575 | Val. Loss: 0.581 |\n",
            "| Epoch: 95 | Train Loss: 0.575 | Val. Loss: 0.581 |\n",
            "| Epoch: 96 | Train Loss: 0.575 | Val. Loss: 0.581 |\n",
            "| Epoch: 97 | Train Loss: 0.575 | Val. Loss: 0.581 |\n",
            "| Epoch: 98 | Train Loss: 0.575 | Val. Loss: 0.581 |\n",
            "| Epoch: 99 | Train Loss: 0.575 | Val. Loss: 0.581 |\n",
            "| Epoch: 100 | Train Loss: 0.575 | Val. Loss: 0.581 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqIMLrAlNGJQ",
        "colab_type": "text"
      },
      "source": [
        "# Start Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v--HYhF6HMBm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "fa76b3a7-5803-4d88-c20a-716007212f98"
      },
      "source": [
        "test_loc = 'gdrive/My Drive/subtask-1/test.csv'    \n",
        "test = pd.read_csv(test_loc)\n",
        "\n",
        "# Prepare the test corpus and labels\n",
        "test_o_headls_n_headls, test_labels_list, test_new_word_list = processed_data_to_lists(test)\n",
        "test_tokenized_headls = get_tokenized_headls(test_o_headls_n_headls)\n",
        "test_origin_tensor, test_new_tensor, test_label_tensor = get_model_inputs(test_tokenized_headls, word2idx, test_labels_list)\n",
        "\n",
        "print('test_origin_tensor:')\n",
        "print(test_origin_tensor)\n",
        "print('test_new_tensor:')\n",
        "print(test_new_tensor)\n",
        "print('test_label_tensor:')\n",
        "print(test_label_tensor)\n",
        "\n",
        "# run on the test corpus\n",
        "model.eval()\n",
        "\n",
        "test_x_feature = test_origin_tensor\n",
        "test_y_feature = test_new_tensor\n",
        "test_target = test_label_tensor\n",
        "\n",
        "with torch.no_grad():\n",
        "  test_predictions = model(test_x_feature, test_y_feature).squeeze(1)\n",
        "  test_loss = torch.sqrt(((test_predictions - test_target)**2).mean()).item()\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} |')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_origin_tensor:\n",
            "tensor([[  87, 2816,  234,  ...,    0,    0,    0],\n",
            "        [ 392, 1532,  425,  ...,    0,    0,    0],\n",
            "        [ 212,    2, 7535,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 538,  234,  224,  ...,    0,    0,    0],\n",
            "        [4808, 2153, 5571,  ...,    0,    0,    0],\n",
            "        [  58,  429, 1988,  ...,    0,    0,    0]])\n",
            "test_new_tensor:\n",
            "tensor([[  87, 2816,  234,  ...,    0,    0,    0],\n",
            "        [ 392, 1532,  773,  ...,    0,    0,    0],\n",
            "        [ 212,    2, 7535,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 538,  234,  224,  ...,    0,    0,    0],\n",
            "        [4808, 2153, 5571,  ...,    0,    0,    0],\n",
            "        [  58,  429, 1988,  ...,    0,    0,    0]])\n",
            "test_label_tensor:\n",
            "tensor([1.2000, 0.4000, 1.0000,  ..., 0.4000, 0.0000, 0.8000])\n",
            "| Test Loss: 0.576 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dSS9KghNO8a",
        "colab_type": "text"
      },
      "source": [
        "# Write Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ideUIaCPIIj2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fdde6ca3-9a68-42f1-8dc9-aa8efc9969e1"
      },
      "source": [
        "def write_predictions(predictions, test_data_frame, out_loc):\n",
        "    test_data_frame['pred'] = predictions\n",
        "    output = test_data_frame[['id','pred']]\n",
        "    output.to_csv(out_loc, index=False)\n",
        "        \n",
        "    print('Output file created:\\n\\t- '+os.path.abspath(out_loc))\n",
        "\n",
        "\n",
        "# write the predictions for the dev data into 'task-1-output.csv'\n",
        "out_loc = 'gdrive/My Drive/subtask-1/task-1-output.csv'\n",
        "write_predictions(test_predictions, test, out_loc)   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output file created:\n",
            "\t- /content/gdrive/My Drive/subtask-1/task-1-output.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaKAzrvoNVi6",
        "colab_type": "text"
      },
      "source": [
        "# Check Final Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKFbPuoeLd0I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0875292e-1f58-4a4c-b7ac-fa91645d0e68"
      },
      "source": [
        "def score(truth_loc, prediction_loc):\n",
        "    truth = pd.read_csv(truth_loc, usecols=['id','meanGrade'])\n",
        "    pred = pd.read_csv(prediction_loc, usecols=['id','pred'])\n",
        "    \n",
        "    assert(sorted(truth.id) == sorted(pred.id)),\"ID mismatch between ground truth and prediction!\"\n",
        "    \n",
        "    data = pd.merge(truth,pred)\n",
        "    rmse = np.sqrt(np.mean((data['meanGrade'] - data['pred'])**2))\n",
        "    \n",
        "    print(\"RMSE = %.3f\" % rmse)    \n",
        "\n",
        "# print RMSE\n",
        "truth_loc = 'gdrive/My Drive/subtask-1/test.csv'\n",
        "prediction_loc = 'gdrive/My Drive/subtask-1/task-1-output.csv'\n",
        "score(truth_loc, prediction_loc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE = 0.576\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEGy9Bt2WVxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}