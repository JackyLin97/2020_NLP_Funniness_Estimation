{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"“Funniness_Estimation_2nd_version.ipynb”","provenance":[{"file_id":"1gpCGqLeaivXq3IyqBfd193hJFpX2bNDN","timestamp":1593387350434}],"authorship_tag":"ABX9TyPTPW0Zaf19aPgmSUUONRMm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"HCraGcVm5voH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1594289640437,"user_tz":-60,"elapsed":2317,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"243e413d-a918-4100-9d24-e89f8c2b44e7"},"source":["\"\"\"\n","@author: Ziyang Lin\n","         zlin19@sheffield.ac.uk\n","         University of Sheffield, UK\n","\"\"\"\n","\n","'''\n","A two inputs NN regression system for\n","\"Assessing the Funniness of Edited News Headlines (SemEval-2020)\" task 1\n","in which given the original and the edited headline, the system\n","is required to predict the mean funniness of the edited headline.\n","'''\n","\n","import random\n","\n","import pandas as pd\n","import numpy as np\n","\n","import os\n","import re\n","import time\n","import math\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","from torchtext import data\n","# This time we will work with a dataset from the torchtext package consists of data processing utilities and popular datasets for NLP\n","from torchtext import datasets\n","import torch.utils.data as tud\n","\n","from google.colab import drive \n","drive.mount('/content/gdrive')\n","\n","import nltk\n","nltk.download('punkt')\n","from nltk import word_tokenize\n","\n","\n","# fix the seeds to get consistent results before every training\n","# loop in what follows\n","def fix_seed(seed=234):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","\n","\n","# Helper function to print time between epochs\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs\n","\n","\n","def processed_data_to_lists(train):\n","    headls_words = [(origin_headl, new_word) for (origin_headl, new_word) in zip(train.original.to_list(), train.edit.to_list())]\n","    labels_list = train.meanGrade.to_list()\n","\n","    # list of tuple for original headlines and new edited headlines\n","    o_headls_n_headls = []\n","    \n","    new_word_list = []\n","\n","    for origin_headl, new_word in headls_words:\n","      # pattern\n","      p = re.compile(r'\\<(.*?)\\/\\>')\n","      # get the normal version of the original headline\n","      origin_word = ''.join(re.findall(p, origin_headl))\n","      normal_origin_headl = p.sub(origin_word, origin_headl)\n","      # get the new edited headline\n","      new_headl = p.sub(new_word, origin_headl)\n","      # pair them and put them into the list\n","      o_headls_n_headls.append((normal_origin_headl,new_headl))\n","\n","      new_word_list.append(new_word)\n","\n","    return o_headls_n_headls, labels_list, new_word_list\n","\n","\n","# tokenize both the original headlines and the corresponding new edited headlines\n","def get_tokenized_headls(o_headls_n_headls):\n","    tokenized_headls = [] \n","    for origin_headl, new_headl in o_headls_n_headls:\n","      origin_headl = \" \".join(word_tokenize(origin_headl))\n","      new_headl = \" \".join(word_tokenize(new_headl))    \n","\n","      tokenized_origin = []\n","      tokenized_new = []\n","\n","      for token in origin_headl.split(' '):\n","        token = token.lower()\n","        tokenized_origin.append(token)\n","\n","      for token in new_headl.split(' '):\n","        token = token.lower()\n","        tokenized_new.append(token)\n","\n","      tokenized_headls.append((tokenized_origin, tokenized_new))\n","\n","    return tokenized_headls\n","\n","\n","def get_word2idx(tokenized_headls, new_word_list):\n","    vocabulary = []\n","    for origin_headl, new_headl in tokenized_headls:\n","      for token in origin_headl:\n","          if token not in vocabulary:\n","              vocabulary.append(token)\n","              \n","    for token in new_word_list:\n","      if token not in vocabulary:\n","          vocabulary.append(token)\n","  \n","    word2idx = {w: idx+1 for (idx, w) in enumerate(vocabulary)}\n","    # we reserve the 0 index for the padding token\n","    word2idx['<pad>'] = 0\n","      \n","    return word2idx\n","\n","\n","def get_model_inputs(tokenized_headls, word2idx, labels):\n","    # we index our original headlines and the corresponding new edited headlines\n","    vectorized_headls = [([word2idx[tk] for tk in origin if tk in word2idx],[word2idx[tk] for tk in new if tk in word2idx]) for origin, new in tokenized_headls]\n","\n","    # the original headlines lengths and the new headlines lengths\n","    origin_headl_lengths = [len(origin_headl) for origin_headl, new_headl in vectorized_headls]\n","    new_headl_lengths = [len(new_headl) for origin_headl, new_headl in vectorized_headls]\n","\n","    # Get maximum length\n","    max_len = max(origin_headl_lengths)\n","    \n","    # we create two tensors of the same fixed size filled with zeroes for padding\n","    origin_tensor = torch.zeros((len(vectorized_headls), max_len)).long()\n","    new_tensor = torch.zeros((len(vectorized_headls), max_len)).long()\n","\n","    # we fill them with our vectorized headlines \n","    for idx, ((origin_headl, new_headl), origin_headllen) in enumerate(zip(vectorized_headls, origin_headl_lengths)):\n","      origin_tensor[idx, :origin_headllen] = torch.LongTensor(origin_headl)\n","\n","    for idx, ((origin_headl, new_headl), new_headllen) in enumerate(zip(vectorized_headls, new_headl_lengths)):\n","      new_tensor[idx, :new_headllen] = torch.LongTensor(new_headl)  \n","\n","    # Label tensor\n","    label_tensor = torch.FloatTensor(labels)\n","    \n","    return origin_tensor, new_tensor, label_tensor\n","\n","\n","class TwoInputsNN(nn.Module):\n","    def __init__(self, embedding_dim, hidden_dim_1, hidden_dim_2, hidden_dim_3, vocab_size):  \n","        super(TwoInputsNN, self).__init__()\n","        \n","        # embedding (lookup layer) layer\n","        # padding_idx argument makes sure that the 0-th token in the vocabulary\n","        # is used for padding purposes i.e. its embedding will be a 0-vector\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n","        \n","        # hidden layer 1\n","        self.fc1 = nn.Linear(embedding_dim, hidden_dim_1)\n","\n","        # hidden layer 2\n","        self.fc2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n","        \n","        # activation\n","        self.relu1 = nn.ReLU()\n","        \n","        # hidden layer 3\n","        self.fc3 = nn.Linear(hidden_dim_2, hidden_dim_3)\n","\n","\n","    def forward(self, x, y):\n","        # tensor x and tensor y have shape (batch_size, max_headl_len)\n","        \n","        # put x into embedding layer\n","        x_embedded = self.embedding(x)\n","        # Now `embedding` has shape (batch size, max_headl_len, embedding dim)\n","        # Compute the average embeddings of shape (batch_size, embedding_dim)\n","        # Implement averaging that ignores padding (average using actual headline lengths).        \n","        x_headl_lens = x.ne(0).sum(1, keepdims=True)\n","        x_averaged = x_embedded.sum(1) / x_headl_lens\n","\n","        # put y into embedding layer\n","        y_embedded = self.embedding(y)       \n","        y_headl_lens = y.ne(0).sum(1, keepdims=True)\n","        y_averaged = y_embedded.sum(1) / y_headl_lens\n","\n","        # hidden layer 1\n","        x_out = self.fc1(x_averaged)\n","        y_out = self.fc1(y_averaged)\n","\n","        x_out = self.relu1(x_out)\n","        y_out = self.relu1(y_out)\n","\n","        # hidden layer 2\n","        x_out = self.fc2(x_out)\n","        y_out = self.fc2(y_out)\n","\n","        x_out = self.relu1(x_out)\n","        y_out = self.relu1(y_out)\n","\n","        # hidden layer 3\n","        x_out = self.fc3(x_out)\n","        y_out = self.fc3(y_out)\n","\n","\n","        # output layer\n","        out = x_out * y_out \n","        out = torch.sum(out, 1, keepdim = True)\n","\n","        return out\n","\n","class TwoInputsCNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, out_channels, window_size, fc_out_dim, dropout):\n","        super(TwoInputsCNN, self).__init__()\n","        \n","        # Create the embedding layer as usual\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n","        \n","        # in_channels -- 1 text channel\n","        # out_channels -- the number of output channels\n","        # kernel_size is (window size x embedding dim)\n","        self.conv = nn.Conv2d(\n","          in_channels=1, out_channels=out_channels,\n","          kernel_size=(window_size, embedding_dim))\n","        \n","        # the dropout layer\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # the fully connected layer\n","        self.fc = nn.Linear(out_channels, fc_out_dim)\n","          \n","    def forward(self, x, y):\n","        # x -> (batch size, max_sent_length)\n","        \n","        # embedded -> (batch size, max_sent_length, embedding_dim)\n","        # images have 3 RGB channels \n","        # for the text we add 1 channel\n","        # embedded -> (batch size, 1, max_sent_length, embedding_dim)\n","        embedded_x = self.embedding(x).unsqueeze(1)\n","        embedded_y = self.embedding(y).unsqueeze(1)\n","\n","        # Compute the feature maps      \n","        feature_maps_x = self.conv(embedded_x).squeeze(3)\n","        feature_maps_y = self.conv(embedded_y).squeeze(3)\n","       \n","        # Apply ReLU\n","        feature_maps_x = F.relu(feature_maps_x)\n","        feature_maps_y = F.relu(feature_maps_y)\n","        \n","        # Apply the max pooling layer\n","        pooled_x = F.max_pool1d(feature_maps_x, feature_maps_x.shape[2]).squeeze(2)\n","        pooled_y = F.max_pool1d(feature_maps_y, feature_maps_y.shape[2]).squeeze(2)\n","\n","        dropped_x = self.dropout(pooled_x)\n","        dropped_y = self.dropout(pooled_y)\n","\n","        # Pass the fully connected layer\n","        #out_x = self.fc(pooled_x)\n","        #out_y = self.fc(pooled_y)\n","\n","        # Apply ReLU\n","        #out_x = F.relu(out_x)\n","        #out_y = F.relu(out_y)\n","        \n","        # output layer\n","        out = dropped_x * dropped_y \n","        preds = torch.sum(out, 1, keepdim = True)\n","\n","\n","        return preds\n","\n","\n","class TwoInputsRNN(nn.Module):\n","\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, fc_out_dim,\n","                 bidirectional, dropout, embeddings):\n","\n","        super().__init__()\n","\n","        self.bidirectional = bidirectional\n","        self.hidden_dim = hidden_dim\n","        \n","        # Here, we initialize our model with pre-trained embeddings (50D pre-trained GloVe embeddings in our case).\n","        # This layer will fine-tune these embeddings, specific to this model/dataset.\n","        #self.embedding = nn.Embedding.from_pretrained(TEXT.vocab.vectors)\n","\n","        # We can also train the embeddings from scratch:\n","        #self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0) \n","        self.embedding = nn.Embedding.from_pretrained(embeddings, padding_idx=0)\n","        \n","        # An RNN layer. We specify that the batch dimension goes first\n","        # We have a bidirectional flag which indicates whether the model is unidirectional or bidirectional\n","        # RNNs can be stacked - i.e. have multiple layers. Here, we will only look at the 1 layer case.\n","        self.rnn = nn.RNN(embedding_dim,\n","                          hidden_dim,\n","                          batch_first=True,\n","                          bidirectional=bidirectional,\n","                          num_layers=1)\n","\n","          # The linear layer takes the final hidden state and feeds it through a fully connected layer.\n","          # The dimensionality of the output is equal to the output class count.\n","          # For classification in a bidirectional RNN we concatenate:\n","            #  - The last hidden state from the forward RNN (obtained from final word of the sentence)\n","            #  - The last hidden state from the backward RNN (obtained from the first word of the sentence)\n","          # Due to the concatenation, our hidden size is doubled.\n","        \n","        if self.bidirectional:\n","            linear_hidden_in = hidden_dim * 2\n","        else:\n","            linear_hidden_in = hidden_dim\n","\n","        # The linear layer\n","        self.fc = nn.Linear(linear_hidden_in, fc_out_dim)\n","        \n","        # We apply dropout technique that sets a random set of activations of a layer to zero.\n","        # This prevents the network from learning to rely on specific weights and helps to prevent overfitting. \n","        # Note that the dropout layer is only used during training, and not during test time.\n","        self.dropout = nn.Dropout(dropout)\n","    \n","    def forward(self, text1, text2):\n","\n","        # ACRONYMS:\n","          # B = Batch size\n","          # T = Max sentence length\n","          # E = Embedding dimension\n","          # D = Hidden dimension\n","          # O = FC Output dimension\n","\n","        # shape(text1) = [B, T]\n","\n","        embedded1 = self.dropout(self.embedding(text1))\n","        embedded2 = self.dropout(self.embedding(text2))\n","        # shape(embedded1) = [B, T, E]\n","        \n","        # An RNN in PyTorch returns two values:\n","        # (1) All hidden states of the last RNN layer\n","        # (2) Hidden state of the last timestep for every layer\n","        # Note: we are only using 1 layer\n","        all_hidden1, last_hidden1 = self.rnn(embedded1)\n","        all_hidden2, last_hidden2 = self.rnn(embedded2)\n","        # shape(all_hidden1) = [B, T, D*num_directions]\n","        # shape(last_hidden1) = [num_layers*num_directions, B, D].  num_layers = 1\n","        # NOTE. If we were to NOT use the `batch_first` flag, shape of all_hidden would be [T, B, D*num_directions]\n","        \n","        if self.bidirectional:\n","            # Concat the final forward (hidden[0,:,:]) and backward (hidden[1,:,:]) hidden layers\n","            last_hidden1 = torch.cat((last_hidden1[0, :, :], last_hidden1[1, :, :]), dim=-1)\n","            last_hidden2 = torch.cat((last_hidden2[0, :, :], last_hidden2[1, :, :]), dim=-1)\n","            # shape(last_hidden1) = [B, D*2]\n","\n","        else:\n","            last_hidden1 = last_hidden1.squeeze(0)\n","            last_hidden2 = last_hidden2.squeeze(0)\n","            # shape(last_hidden1) = [B, D]\n","\n","        out1 = self.fc(self.dropout(last_hidden1))\n","        out2 = self.fc(self.dropout(last_hidden2))\n","        # shape(out1) = [B, O]\n","\n","        # Our predictions.\n","        out = out1 * out2 \n","        preds = torch.sum(out, 1, keepdim = True)   \n","        # shape(preds) = [B, 1]\n","        \n","        return preds\n","\n","\n","class TwoInputsConcatRNN(nn.Module):\n","\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, fc1_out_dim, fc2_out_dim, out_dim,\n","                 bidirectional, dropout):\n","\n","        super().__init__()\n","\n","        self.bidirectional = bidirectional\n","        self.hidden_dim = hidden_dim\n","        \n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0) \n","        \n","        self.rnn = nn.RNN(embedding_dim,\n","                          hidden_dim,\n","                          batch_first=True,\n","                          bidirectional=bidirectional,\n","                          num_layers=1)\n","\n","        # Due to the concatenation, our hidden size is doubled.        \n","        if self.bidirectional:\n","            linear_hidden_in = hidden_dim * 2\n","        else:\n","            linear_hidden_in = hidden_dim\n","\n","        # The linear layer 1, the input dim is linear_hidden_in * 2 after concatenation \n","        self.fc1 = nn.Linear(linear_hidden_in * 2, fc1_out_dim)\n","        \n","        # The linear layer 2\n","        self.fc2 = nn.Linear(fc1_out_dim, fc2_out_dim)\n","\n","        # The output layer\n","        self.fc3 = nn.Linear(fc2_out_dim, out_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","    \n","    def forward(self, text1, text2):\n","        # ACRONYMS:\n","          # B = Batch size\n","          # T = Max sentence length\n","          # E = Embedding dimension\n","          # D = Hidden dimension\n","          # O = FC Output dimension\n","\n","        # shape(text1) = [B, T]\n","        embedded1 = self.dropout(self.embedding(text1))\n","        embedded2 = self.dropout(self.embedding(text2))\n","        # shape(embedded1) = [B, T, E]\n","        \n","        all_hidden1, last_hidden1 = self.rnn(embedded1)\n","        all_hidden2, last_hidden2 = self.rnn(embedded2)\n","        \n","        if self.bidirectional:\n","            # Concat the final forward (hidden[0,:,:]) and backward (hidden[1,:,:]) hidden layers\n","            last_hidden1 = torch.cat((last_hidden1[0, :, :], last_hidden1[1, :, :]), dim=-1)\n","            last_hidden2 = torch.cat((last_hidden2[0, :, :], last_hidden2[1, :, :]), dim=-1)\n","            # shape(last_hidden1) = [B, D*2]\n","\n","        else:\n","            last_hidden1 = last_hidden1.squeeze(0)\n","            last_hidden2 = last_hidden2.squeeze(0)\n","            # shape(last_hidden1) = [B, D]\n","\n","        # Concat the last_hidden1 and last_hidden2\n","        last_hidden12 = torch.cat((last_hidden1, last_hidden2), dim=-1)\n","\n","        # pass to linear layer 1\n","        out = self.fc1(self.dropout(last_hidden12))\n","\n","        # pass to linear layer 2\n","        out = self.fc2(self.dropout(out))\n","\n","        # Our predictions.\n","        preds = self.fc3(self.dropout(out))   \n","        # shape(preds) = [B, 1]\n","        \n","        return preds\n","\n","\n","class ClassifyRNN(nn.Module):\n","\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim,\n","                 bidirectional, dropout):\n","\n","        super().__init__()\n","\n","        self.bidirectional = bidirectional\n","        self.hidden_dim = hidden_dim       \n","\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0) \n","        \n","        self.rnn = nn.RNN(embedding_dim,\n","                          hidden_dim,\n","                          batch_first=True,\n","                          bidirectional=bidirectional,\n","                          num_layers=1)\n","      \n","        if self.bidirectional:\n","            linear_hidden_in = hidden_dim * 2\n","        else:\n","            linear_hidden_in = hidden_dim\n","\n","        # The classification (linear) layer\n","        self.fc = nn.Linear(linear_hidden_in, output_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","    \n","    def forward(self, text):\n","        # ACRONYMS:\n","          # B = Batch size\n","          # T = Max sentence length\n","          # E = Embedding dimension\n","          # D = Hidden dimension\n","          # O = FC Output dimension\n","\n","        # shape(text) = [B, T]\n","        embedded = self.dropout(self.embedding(text))\n","        \n","        all_hidden, last_hidden = self.rnn(embedded)\n","        \n","        if self.bidirectional:\n","            # Concat the final forward (hidden[0,:,:]) and backward (hidden[1,:,:]) hidden layers\n","            last_hidden = torch.cat((last_hidden[0, :, :], last_hidden[1, :, :]), dim=-1)\n","            # shape(last_hidden) = [B, D*2]\n","\n","        else:\n","            last_hidden = last_hidden.squeeze(0)\n","            # shape(last_hidden1) = [B, D]\n","\n","        # Our predictions.\n","        logits = self.fc(self.dropout(last_hidden))\n","        # shape(logits) = [B, O]\n","          \n","        return logits\n","\n","\n","class ClassifyCNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, out_channels, window_size, output_dim, dropout):\n","        super(ClassifyCNN, self).__init__()\n","        \n","        # Create the embedding layer as usual\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n","        \n","        # in_channels -- 1 text channel\n","        # out_channels -- the number of output channels\n","        # kernel_size is (window size x embedding dim)\n","        self.conv = nn.Conv2d(\n","          in_channels=1, out_channels=out_channels,\n","          kernel_size=(window_size, embedding_dim))\n","        \n","        # the dropout layer\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # the fully connected layer\n","        self.fc = nn.Linear(out_channels, output_dim)\n","          \n","    def forward(self, x):\n","        # x -> (batch size, max_sent_length)\n","        \n","        # embedded -> (batch size, max_sent_length, embedding_dim)\n","        # images have 3 RGB channels \n","        # for the text we add 1 channel\n","        # embedded -> (batch size, 1, max_sent_length, embedding_dim)\n","        embedded = self.embedding(x).unsqueeze(1)\n"," \n","        # Compute the feature maps      \n","        feature_maps = self.conv(embedded).squeeze(3)\n","       \n","        # Apply ReLU\n","        feature_maps = F.relu(feature_maps)\n","        \n","        # Apply the max pooling layer\n","        pooled = F.max_pool1d(feature_maps, feature_maps.shape[2]).squeeze(2)\n","\n","        dropped = self.dropout(pooled)\n"," \n","        # output layer\n","        preds = self.fc(dropped)\n","\n","        return preds        "],"execution_count":10,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FwA2cJmXua1b","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594289647484,"user_tz":-60,"elapsed":7295,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"9069ac2a-f7a3-4eef-ce51-bc4e7f5a7d05"},"source":["train_loc = 'gdrive/My Drive/subtask-1/train.csv'\n","dev_loc = 'gdrive/My Drive/subtask-1/dev.csv'\n","test_loc = 'gdrive/My Drive/subtask-1/test.csv'\n","train = pd.read_csv(train_loc)    \n","valid = pd.read_csv(dev_loc)\n","test = pd.read_csv(test_loc)\n","\n","# Prepare the training corpus and labels\n","o_headls_n_headls, labels_list, new_word_list = processed_data_to_lists(train)\n","tokenized_headls = get_tokenized_headls(o_headls_n_headls)\n","word2idx = get_word2idx(tokenized_headls, new_word_list)\n","origin_tensor, new_tensor, label_tensor = get_model_inputs(tokenized_headls, word2idx, labels_list)\n","\n","print('origin_tensor:')\n","print(origin_tensor)\n","print('new_tensor:')\n","print(new_tensor)\n","print('label_tensor:')\n","print(label_tensor)\n","print('vocab_size:')\n","print(len(word2idx))\n","print(origin_tensor.shape)\n","\n","print()\n","print()\n","\n","# Prepare the validation corpus and labels\n","valid_o_headls_n_headls, valid_labels_list, valid_new_word_list = processed_data_to_lists(valid)\n","valid_tokenized_headls = get_tokenized_headls(valid_o_headls_n_headls)\n","valid_origin_tensor, valid_new_tensor, valid_label_tensor = get_model_inputs(valid_tokenized_headls, word2idx, valid_labels_list)\n","\n","print('valid_origin_tensor:')\n","print(valid_origin_tensor)\n","print('valid_new_tensor:')\n","print(valid_new_tensor)\n","print('valid_label_tensor:')\n","print(valid_label_tensor)\n","print(valid_origin_tensor.shape)\n","\n","print()\n","print()\n","\n","# Prepare the test corpus and labels\n","test_o_headls_n_headls, test_labels_list, test_new_word_list = processed_data_to_lists(test)\n","test_tokenized_headls = get_tokenized_headls(test_o_headls_n_headls)\n","test_origin_tensor, test_new_tensor, test_label_tensor = get_model_inputs(test_tokenized_headls, word2idx, test_labels_list)\n","\n","print('test_origin_tensor:')\n","print(test_origin_tensor)\n","print('test_new_tensor:')\n","print(test_new_tensor)\n","print('test_label_tensor:')\n","print(test_label_tensor)\n","print(test_origin_tensor.shape)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["origin_tensor:\n","tensor([[   1,    2,    3,  ...,    0,    0,    0],\n","        [  16,   17,   18,  ...,    0,    0,    0],\n","        [  32,   33,   34,  ...,    0,    0,    0],\n","        ...,\n","        [5728, 2737, 5729,  ...,    0,    0,    0],\n","        [7010,   80, 2169,  ...,    0,    0,    0],\n","        [ 105,   93,   27,  ...,    0,    0,    0]])\n","new_tensor:\n","tensor([[   1,    2,    3,  ...,    0,    0,    0],\n","        [  16,   17,   18,  ...,    0,    0,    0],\n","        [  32,   33,   34,  ...,    0,    0,    0],\n","        ...,\n","        [5728, 2737, 5729,  ...,    0,    0,    0],\n","        [7010,   80, 2169,  ...,    0,    0,    0],\n","        [ 105,   93,   27,  ...,    0,    0,    0]])\n","label_tensor:\n","tensor([0.2000, 1.6000, 1.0000,  ..., 0.6000, 1.4000, 0.4000])\n","vocab_size:\n","11722\n","torch.Size([9652, 27])\n","\n","\n","valid_origin_tensor:\n","tensor([[1674,  323, 1832,  ...,    0,    0,    0],\n","        [ 509, 2944,  855,  ...,    0,    0,    0],\n","        [1598,   80,  749,  ...,    0,    0,    0],\n","        ...,\n","        [  88,  903,  398,  ...,    0,    0,    0],\n","        [5947, 2501,  234,  ...,    0,    0,    0],\n","        [  14, 3059,  323,  ...,    0,    0,    0]])\n","valid_new_tensor:\n","tensor([[1674,  323, 1832,  ...,    0,    0,    0],\n","        [ 509, 1850,  855,  ...,    0,    0,    0],\n","        [1598,   80,  749,  ...,    0,    0,    0],\n","        ...,\n","        [  88,  903,  398,  ...,    0,    0,    0],\n","        [5947, 2501,  234,  ...,    0,    0,    0],\n","        [  14, 3059,  323,  ...,    0,    0,    0]])\n","valid_label_tensor:\n","tensor([1.0000, 0.8000, 0.6000,  ..., 1.4000, 1.4000, 0.6000])\n","torch.Size([2419, 25])\n","\n","\n","test_origin_tensor:\n","tensor([[  87, 2816,  234,  ...,    0,    0,    0],\n","        [ 392, 1532,  425,  ...,    0,    0,    0],\n","        [ 212,    2, 7535,  ...,    0,    0,    0],\n","        ...,\n","        [ 538,  234,  224,  ...,    0,    0,    0],\n","        [4808, 2153, 5571,  ...,    0,    0,    0],\n","        [  58,  429, 1988,  ...,    0,    0,    0]])\n","test_new_tensor:\n","tensor([[  87, 2816,  234,  ...,    0,    0,    0],\n","        [ 392, 1532,  773,  ...,    0,    0,    0],\n","        [ 212,    2, 7535,  ...,    0,    0,    0],\n","        ...,\n","        [ 538,  234,  224,  ...,    0,    0,    0],\n","        [4808, 2153, 5571,  ...,    0,    0,    0],\n","        [  58,  429, 1988,  ...,    0,    0,    0]])\n","test_label_tensor:\n","tensor([1.2000, 0.4000, 1.0000,  ..., 0.4000, 0.0000, 0.8000])\n","torch.Size([3024, 25])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B_HFsZRaGxBV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1594289647487,"user_tz":-60,"elapsed":5656,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"453eb88c-9107-4f3f-ce22-00ba7d438254"},"source":["# prepare class labels for classification task\n","round_labels = torch.round(label_tensor).long()\n","round_valid_labels = torch.round(valid_label_tensor).long()\n","\n","print(round_labels)\n","print(round_valid_labels)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["tensor([0, 2, 1,  ..., 1, 1, 0])\n","tensor([1, 1, 1,  ..., 1, 1, 1])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yGiuaDqW9Fnn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594289647488,"user_tz":-60,"elapsed":4908,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"aabca9b0-2c30-4266-9714-53fd1cf61d10"},"source":["# do computation on a GPU if possible \n","if torch.cuda.is_available():\n","  torch.backends.cudnn.deterministic = True\n","  DEVICE='cuda:0'\n","else:\n","  DEVICE='cpu'\n","\n","print('Device is', DEVICE)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Device is cuda:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Gm5tiQIUF4hE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1594289647988,"user_tz":-60,"elapsed":4604,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"108f213f-3273-46d8-d247-915d19b352dd"},"source":["class NewDataset(tud.Dataset):\n","    def __init__(self, x1, x2, y1):\n","        self.len = x1.shape[0]\n","\n","        self.x1_data = x1.to(DEVICE)\n","        self.x2_data = x2.to(DEVICE)\n","        self.y1_data = y1.to(DEVICE)\n","\n","\n","    def __getitem__(self, index):\n","        return self.x1_data[index], self.x2_data[index], self.y1_data[index]\n","\n","\n","    def __len__(self):\n","        return self.len\n","\n","# Batching\n","BATCH_SIZE = 36\n","\n","train_dataset = NewDataset(origin_tensor, new_tensor, label_tensor)\n","valid_dataset = NewDataset(valid_origin_tensor, valid_new_tensor, valid_label_tensor)\n","test_dataset = NewDataset(test_origin_tensor, test_new_tensor, test_label_tensor)\n","\n","train_dataloader = tud.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","valid_dataloader = tud.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","test_dataloader = tud.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","\n","\n","##### demo #####\n","print(train_dataloader)\n","\n","for x1, x2, y1 in train_dataloader:\n","    demo_x1 = x1\n","    demo_x2 = x2\n","    demo_y1 = y1\n","    break\n","    \n","print(x1.shape)\n","print(x2.shape)\n","print(y1.shape)\n","print(len(train_dataloader))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["<torch.utils.data.dataloader.DataLoader object at 0x7f3cb36b8c18>\n","torch.Size([36, 27])\n","torch.Size([36, 27])\n","torch.Size([36])\n","269\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1y_Du67mIsdl","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594289647989,"user_tz":-60,"elapsed":3598,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}}},"source":["# prepare dataloader for classification task\n","classify_train_dataset = NewDataset(origin_tensor, new_tensor, round_labels)\n","classify_valid_dataset = NewDataset(valid_origin_tensor, valid_new_tensor, round_valid_labels)\n","\n","classify_train_dataloader = tud.DataLoader(classify_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","classify_valid_dataloader = tud.DataLoader(classify_valid_dataset, batch_size=BATCH_SIZE, shuffle=True)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"IWLklU_UP7ht","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594289647990,"user_tz":-60,"elapsed":2996,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}}},"source":["# define rmse\n","def rmse(predictions, labels):\n","    loss = torch.sqrt(((predictions - labels)**2).mean())\n","\n","    return loss\n","    "],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"mogU4r4vHVJS","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594289647990,"user_tz":-60,"elapsed":1779,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}}},"source":["# define train and evaluate\n","def train(model, train_dataloader, valid_dataloader, optimizer, scheduler, criterion, N_EPOCHS, are_two_input):\n","    optimizer = optimizer\n","    model = model.to(DEVICE)\n","\n","    for epoch in range(N_EPOCHS):\n","    \n","        start_time = time.time()\n","\n","        # To ensure the dropout is \"turned on\" while training\n","        # (good practice to include in your projects even if it is not used)\n","        model.train()\n","        \n","        epoch_loss = 0\n","    \n","        for origin_batch, new_batch, labels in train_dataloader:\n","                        \n","            # Zero the gradients\n","            optimizer.zero_grad()\n","\n","            # shape(origin_batch) = [B, T]\n","            # shape(new_batch) = [B, T]\n","            # shape(label) = [B]\n","\n","            if are_two_input:\n","               predictions = model(origin_batch, new_batch)\n","            else:\n","               predictions = model(new_batch)\n","            \n","            # compute the loss\n","            loss = criterion(predictions, labels)\n","            #print(loss)\n","                      \n","            # calculate the gradient of each parameter\n","            loss.backward()\n","        \n","            # update the parameters using the gradients and optimizer algorithm \n","            optimizer.step()\n","\n","            # update the learning rate\n","            scheduler.step()\n","            \n","            epoch_loss += loss.item()\n","            \n","        average_epoch_loss = epoch_loss / len(train_dataloader)\n","        \n","        end_time = time.time()\n","               \n","        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","        average_epoch_valid_loss = evaluate(model, criterion, are_two_input, valid_dataloader)\n","\n","        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","        print(f'\\tTrain Loss: {average_epoch_loss:.3f} | Val. Loss: {average_epoch_valid_loss:.3f} ')\n","\n","\n","def evaluate(model, criterion, are_two_input, dataloader):\n","\n","    epoch_loss = 0\n","\n","    # Turn on evaluate mode. This de-activates dropout. \n","    model.eval()\n","\n","    # We do not compute gradients within this block, i.e. no training\n","    with torch.no_grad():\n","\n","        for origin_batch, new_batch, labels in dataloader:\n","            \n","            if are_two_input:\n","               predictions = model(origin_batch, new_batch)\n","            else:\n","               predictions = model(new_batch)\n","\n","            loss = criterion(predictions, labels)\n","\n","            epoch_loss += loss.item()\n","\n","    return epoch_loss / len(dataloader)\n"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"rWUiz8MOYaOG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":459},"executionInfo":{"status":"ok","timestamp":1594289864727,"user_tz":-60,"elapsed":15977,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"aa9ff976-7fb0-411f-8d14-9b934e415a86"},"source":["# classification task\n","fix_seed()\n","\n","INPUT_DIM = len(word2idx)\n","EMBEDDING_DIM = 50\n","HIDDEN_DIM = 128\n","OUTPUT_DIM = 4\n","BIDIRECTIONAL = True\n","DROPOUT = 0.4\n","\n","LRATE = 1e-4\n","N_EPOCHS = 10\n","\n","classify_RNN_model = ClassifyRNN(INPUT_DIM, \n","                                 EMBEDDING_DIM, \n","                                 HIDDEN_DIM, \n","                                 OUTPUT_DIM,\n","                                 BIDIRECTIONAL, \n","                                 DROPOUT)\n","\n","N_OUT_CHANNELS = 100\n","WINDOW_SIZE = 3\n","\n","classify_CNN_model = ClassifyCNN(INPUT_DIM, \n","                                 EMBEDDING_DIM, \n","                                 N_OUT_CHANNELS,\n","                                 WINDOW_SIZE,\n","                                 OUTPUT_DIM,\n","                                 DROPOUT)\n","\n","print(classify_RNN_model)\n","\n","optimizer = optim.AdamW(classify_RNN_model.parameters(), lr=LRATE)\n","# schedule learning rate using scheduler\n","steps = 36\n","scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, steps)\n","# we use the Cross Entropy Loss for classification\n","criterion = nn.CrossEntropyLoss()\n","# note that by default losses are averaged over the minibatch\n","train(classify_RNN_model, classify_train_dataloader, classify_valid_dataloader, optimizer, scheduler, criterion, N_EPOCHS, are_two_input=False)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["ClassifyRNN(\n","  (embedding): Embedding(11722, 50, padding_idx=0)\n","  (rnn): RNN(50, 128, batch_first=True, bidirectional=True)\n","  (fc): Linear(in_features=256, out_features=4, bias=True)\n","  (dropout): Dropout(p=0.4, inplace=False)\n",")\n","Epoch: 01 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.133 | Val. Loss: 1.014 \n","Epoch: 02 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.031 | Val. Loss: 0.998 \n","Epoch: 03 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.028 | Val. Loss: 1.000 \n","Epoch: 04 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.020 | Val. Loss: 1.004 \n","Epoch: 05 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.024 | Val. Loss: 1.007 \n","Epoch: 06 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.018 | Val. Loss: 0.997 \n","Epoch: 07 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.018 | Val. Loss: 0.998 \n","Epoch: 08 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.015 | Val. Loss: 1.000 \n","Epoch: 09 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.014 | Val. Loss: 1.002 \n","Epoch: 10 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.008 | Val. Loss: 1.001 \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SJUzArcvntjv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"executionInfo":{"status":"ok","timestamp":1594120362018,"user_tz":-60,"elapsed":691,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"b98808d2-7bd0-465e-f7dc-2cb8a3a39158"},"source":["ebd = model.embedding.weight.data\n","print(ebd)"],"execution_count":160,"outputs":[{"output_type":"stream","text":["tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.8249, -0.4135,  0.9061,  ...,  1.2568, -1.6915, -0.2913],\n","        [-0.3912,  0.3121,  0.0772,  ...,  0.8107,  1.0791,  1.0931],\n","        ...,\n","        [ 0.5560,  1.6289, -0.0073,  ..., -0.1796,  0.5554,  0.4185],\n","        [ 1.6986,  0.6687, -0.5823,  ..., -2.1529,  0.5683, -0.5236],\n","        [ 1.2224,  0.8689, -1.2513,  ..., -0.4243,  0.7523, -0.1753]],\n","       device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"78UABNXsNxcD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594123242522,"user_tz":-60,"elapsed":72774,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"6298c206-2d7d-4abe-aed8-7e1cfb49bbbd"},"source":["# regression task \n","fix_seed()\n","\n","INPUT_DIM = len(word2idx)\n","EMBEDDING_DIM = 50\n","HIDDEN_DIM = 128\n","FC_OUTPUT_DIM = 32\n","BIDIRECTIONAL = True\n","DROPOUT = 0.3\n","\n","LRATE = 1e-4\n","N_EPOCHS = 30\n","\n","model = TwoInputsRNN(INPUT_DIM, \n","                     EMBEDDING_DIM, \n","                     HIDDEN_DIM, \n","                     FC_OUTPUT_DIM,\n","                     BIDIRECTIONAL, \n","                     DROPOUT,\n","                     ebd)\n","\n","print(model)\n","\n","optimizer = optim.AdamW(model.parameters(), lr=LRATE)\n","# schedule learning rate using scheduler\n","steps = 36\n","scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, steps)\n","# we use the RMSE loss\n","criterion = rmse\n","# note that by default losses are averaged over the minibatch\n","train(model, train_dataloader, valid_dataloader, optimizer, scheduler, criterion, N_EPOCHS, are_two_input=True)"],"execution_count":175,"outputs":[{"output_type":"stream","text":["TwoInputsRNN(\n","  (embedding): Embedding(11722, 50, padding_idx=0)\n","  (rnn): RNN(50, 128, batch_first=True, bidirectional=True)\n","  (fc): Linear(in_features=256, out_features=32, bias=True)\n","  (dropout): Dropout(p=0.3, inplace=False)\n",")\n","Epoch: 01 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.672 | Val. Loss: 0.607 \n","Epoch: 02 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.637 | Val. Loss: 0.595 \n","Epoch: 03 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.623 | Val. Loss: 0.589 \n","Epoch: 04 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.615 | Val. Loss: 0.597 \n","Epoch: 05 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.608 | Val. Loss: 0.585 \n","Epoch: 06 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.604 | Val. Loss: 0.580 \n","Epoch: 07 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.599 | Val. Loss: 0.581 \n","Epoch: 08 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.599 | Val. Loss: 0.582 \n","Epoch: 09 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.596 | Val. Loss: 0.579 \n","Epoch: 10 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.593 | Val. Loss: 0.577 \n","Epoch: 11 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.594 | Val. Loss: 0.581 \n","Epoch: 12 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.592 | Val. Loss: 0.579 \n","Epoch: 13 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.591 | Val. Loss: 0.574 \n","Epoch: 14 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.591 | Val. Loss: 0.581 \n","Epoch: 15 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.591 | Val. Loss: 0.575 \n","Epoch: 16 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.590 | Val. Loss: 0.575 \n","Epoch: 17 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.589 | Val. Loss: 0.576 \n","Epoch: 18 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.589 | Val. Loss: 0.575 \n","Epoch: 19 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.587 | Val. Loss: 0.575 \n","Epoch: 20 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.589 | Val. Loss: 0.575 \n","Epoch: 21 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.588 | Val. Loss: 0.576 \n","Epoch: 22 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.587 | Val. Loss: 0.578 \n","Epoch: 23 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.587 | Val. Loss: 0.576 \n","Epoch: 24 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.587 | Val. Loss: 0.575 \n","Epoch: 25 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.587 | Val. Loss: 0.576 \n","Epoch: 26 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.586 | Val. Loss: 0.575 \n","Epoch: 27 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.585 | Val. Loss: 0.579 \n","Epoch: 28 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.588 | Val. Loss: 0.576 \n","Epoch: 29 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.589 | Val. Loss: 0.573 \n","Epoch: 30 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.586 | Val. Loss: 0.576 \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SYQUqvKIpZQe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1594123273756,"user_tz":-60,"elapsed":1087,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"dfe3d949-6c36-4765-b940-1c9aa42fccde"},"source":["# run on the test corpus\n","\n","\n","test_loss = 0\n","test_predictions = []\n","\n","# Turn on evaluate mode. This de-activates dropout. \n","model.eval()\n","\n","# We do not compute gradients within this block, i.e. no training\n","with torch.no_grad():\n","\n","    for origin_batch, new_batch, labels in test_dataloader:\n","        \n","        predictions__batch = model(origin_batch, new_batch).squeeze(1)\n","        test_predictions += predictions__batch.tolist()\n","\n","        loss = torch.sqrt(((predictions__batch - labels)**2).mean())\n","\n","        test_loss += loss.item()\n","\n","    average_test_loss = test_loss / len(test_dataloader)\n","\n","print(f'| Test Loss: {average_test_loss:.6f} |')\n","print(test_predictions)"],"execution_count":178,"outputs":[{"output_type":"stream","text":["| Test Loss: 0.570986 |\n","[0.911490797996521, 0.9284030795097351, 0.9483744502067566, 0.9401677846908569, 0.9436121582984924, 0.9446495771408081, 0.9446260929107666, 0.9508267045021057, 0.9472134113311768, 0.9080698490142822, 0.950901985168457, 0.9367788434028625, 0.9169864654541016, 0.96546471118927, 0.9398969411849976, 0.9585702419281006, 0.962973415851593, 0.9767415523529053, 0.9482578635215759, 0.9442061185836792, 0.9615516066551208, 0.9299001097679138, 0.9455171823501587, 0.9355592131614685, 0.9311229586601257, 0.9594352841377258, 0.9309986233711243, 0.9331187009811401, 0.9600181579589844, 0.9398894309997559, 0.9448714852333069, 0.935460090637207, 0.9531043171882629, 0.9369505047798157, 0.9360882639884949, 0.9581944346427917, 0.9523606896400452, 0.9517067074775696, 0.9230743646621704, 0.967362642288208, 0.9616958498954773, 0.9221483469009399, 0.9278527498245239, 0.9509905576705933, 0.9426507949829102, 0.9549456834793091, 0.9229129552841187, 0.9282147884368896, 0.962388277053833, 0.9333106279373169, 0.9197467565536499, 0.9286119341850281, 0.9572122097015381, 0.9497330784797668, 0.9321790933609009, 0.9550349712371826, 0.9954555034637451, 0.9479117393493652, 0.9659227132797241, 0.9466407895088196, 0.920063853263855, 0.9505947828292847, 0.9507478475570679, 0.9582663774490356, 0.9347516298294067, 0.9485684633255005, 0.8837507963180542, 0.9704983234405518, 0.9578860402107239, 0.9551074504852295, 0.9323318004608154, 0.9735221862792969, 0.9284039735794067, 0.9512943625450134, 0.9374035596847534, 0.9340171217918396, 0.9330559372901917, 0.9436208009719849, 0.9457930326461792, 0.9426764249801636, 0.9500277042388916, 0.9610419869422913, 0.9271312952041626, 0.960672914981842, 0.962006688117981, 0.9636504650115967, 0.9264307022094727, 0.9437524676322937, 0.9180600047111511, 0.9272269606590271, 0.9317178130149841, 0.9262181520462036, 0.9510087966918945, 0.9621765613555908, 0.9461140632629395, 0.9478221535682678, 0.9217647314071655, 0.9233077168464661, 0.9172959923744202, 0.9410673379898071, 0.9887068271636963, 0.9374134540557861, 0.9350969791412354, 0.9595319032669067, 0.9587056636810303, 0.9561942219734192, 0.9057060480117798, 0.9324060678482056, 0.942198634147644, 0.9728389978408813, 0.9371610283851624, 0.9351967573165894, 0.9656089544296265, 0.94336998462677, 0.9465365409851074, 0.967545747756958, 0.970504641532898, 0.9308255910873413, 0.9220741987228394, 0.9357783794403076, 0.9149503707885742, 0.9587048888206482, 0.9376115798950195, 0.9107965230941772, 0.947784960269928, 0.9434646368026733, 0.9371519088745117, 0.94455885887146, 0.9337142705917358, 0.9485586285591125, 0.9265576601028442, 0.9498987197875977, 0.9506411552429199, 0.945432186126709, 0.9571043252944946, 0.9464316964149475, 0.9370722770690918, 0.9620891809463501, 0.9836156368255615, 0.9520865082740784, 0.9461203217506409, 0.9017131328582764, 0.9667116403579712, 0.9426125884056091, 0.9392133951187134, 0.9448429942131042, 0.90944904088974, 0.9503871202468872, 0.9463919401168823, 0.9483113288879395, 0.9337518811225891, 0.9483736157417297, 0.9475932121276855, 0.9448778033256531, 0.9564165472984314, 0.9510143995285034, 0.9529525637626648, 0.9547101259231567, 0.9462577104568481, 0.9575912952423096, 0.9465958476066589, 0.9559344053268433, 0.9495546817779541, 0.9536036849021912, 0.9780503511428833, 0.9383644461631775, 0.9524261355400085, 0.9453177452087402, 0.9425287246704102, 0.910117506980896, 0.9449292421340942, 0.9447504281997681, 0.9573330879211426, 0.9389127492904663, 0.9505016207695007, 0.9101376533508301, 0.9561450481414795, 0.9463441371917725, 0.9412472248077393, 0.9867880344390869, 0.9615388512611389, 0.9365531206130981, 0.9307481050491333, 0.9431027770042419, 0.9457848072052002, 0.9854546785354614, 0.9613925218582153, 0.953222930431366, 0.9623032808303833, 0.9154353737831116, 0.9422043561935425, 0.9581896066665649, 0.9511445164680481, 0.9679776430130005, 0.945293128490448, 0.9198474884033203, 0.9403144717216492, 0.9480772614479065, 0.9754487872123718, 0.9661573171615601, 0.9422810077667236, 0.8903936743736267, 0.9393106698989868, 0.9447210431098938, 0.9564272165298462, 0.9306701421737671, 0.9329088926315308, 0.9775246977806091, 0.940619707107544, 0.962761402130127, 0.9603936076164246, 0.9601948261260986, 0.9479984045028687, 0.956169843673706, 0.9513117671012878, 0.9553811550140381, 0.9278222322463989, 0.9310506582260132, 0.9382432699203491, 0.9493930339813232, 0.9550232887268066, 0.923574686050415, 0.9579766392707825, 0.9435186386108398, 0.935431182384491, 0.9405351281166077, 0.9480265378952026, 0.9420000910758972, 0.9543948173522949, 0.9468979239463806, 0.9203115701675415, 0.9680854082107544, 0.9568265676498413, 0.9620139002799988, 0.9387602210044861, 0.9374699592590332, 0.9034538865089417, 0.9483416676521301, 0.9438238739967346, 0.9387283325195312, 0.9381340742111206, 0.9364053010940552, 0.8807072639465332, 0.9203411340713501, 0.932893693447113, 0.9575220346450806, 0.9373788833618164, 0.9512825012207031, 0.9563274383544922, 0.9646629095077515, 0.9448133707046509, 0.9596303701400757, 0.9601774215698242, 0.9409012794494629, 0.9415159821510315, 0.9459706544876099, 0.9440051317214966, 0.9258859753608704, 0.9358376264572144, 0.9447338581085205, 0.9372133612632751, 0.9387543797492981, 0.9444714784622192, 0.9571766257286072, 0.923474907875061, 0.9207143187522888, 0.9390429258346558, 0.9419153928756714, 0.9479767084121704, 0.9515784978866577, 0.9398227334022522, 0.9538174867630005, 0.9519846439361572, 0.9632927179336548, 0.9386234283447266, 0.9388805031776428, 0.9434394836425781, 0.9371628761291504, 0.9968500137329102, 0.9211363196372986, 0.9830976128578186, 0.937408983707428, 0.9494599103927612, 0.8972375392913818, 0.9216856956481934, 0.911225438117981, 0.9524994492530823, 0.9695093035697937, 1.4604665040969849, 0.967215895652771, 0.9407871961593628, 0.9623555541038513, 0.923825204372406, 0.952495813369751, 0.9619864225387573, 0.9521817564964294, 0.9489269256591797, 0.9613955616950989, 0.956148087978363, 0.9577046632766724, 0.9234451651573181, 0.9151857495307922, 0.9294889569282532, 0.9536372423171997, 0.940535306930542, 0.9444749355316162, 0.9104486703872681, 0.9312713146209717, 0.9561437964439392, 0.9636527299880981, 0.9328657388687134, 0.9308948516845703, 0.9238119721412659, 0.9629054665565491, 0.9134963750839233, 0.9409699440002441, 0.9217195510864258, 0.9299291372299194, 0.9100532531738281, 0.9470821022987366, 0.9418026208877563, 0.9095057249069214, 0.9481967687606812, 0.9449113607406616, 0.9382936954498291, 0.9622060060501099, 0.9457917809486389, 0.9426457285881042, 0.9635862708091736, 0.9066664576530457, 0.9368059635162354, 0.945275068283081, 0.9429947137832642, 0.9256448149681091, 0.9432947039604187, 0.9616029262542725, 0.9281612634658813, 0.9584982395172119, 0.9601680040359497, 0.9606638550758362, 0.9712104797363281, 0.9376727342605591, 0.9482578635215759, 0.9404400587081909, 0.9373513460159302, 0.9477574825286865, 0.9313880801200867, 0.9598006010055542, 0.9695221781730652, 0.9582173824310303, 0.9333595633506775, 0.9621109366416931, 0.9518352746963501, 0.9547823071479797, 0.9399765729904175, 0.9192147850990295, 0.9533363580703735, 0.9315263032913208, 0.908031702041626, 0.9648891091346741, 0.9278475046157837, 0.9777822494506836, 0.9400475025177002, 0.9144392013549805, 0.9447236061096191, 0.925168514251709, 0.9140892028808594, 0.9265037775039673, 0.9073147773742676, 0.8954280614852905, 0.9273701906204224, 0.9674660563468933, 0.9437217712402344, 0.9539761543273926, 0.9561951160430908, 0.9439758062362671, 0.9313066601753235, 0.9589051008224487, 0.9667452573776245, 0.9458757638931274, 0.9480177760124207, 0.9553027749061584, 0.9463728070259094, 0.9099233150482178, 0.9766882658004761, 0.9580205678939819, 0.9647626280784607, 0.9629669189453125, 0.9352266192436218, 0.9461027383804321, 0.9476112127304077, 0.9588268399238586, 0.9508236646652222, 0.9622586369514465, 0.9514340162277222, 0.9351944923400879, 0.8712543249130249, 0.927519679069519, 0.9571856260299683, 0.9288449883460999, 0.9864583015441895, 0.9450430870056152, 0.9589071273803711, 0.908474326133728, 0.9126876592636108, 0.8856191039085388, 0.9364145994186401, 0.9014317989349365, 0.9327127933502197, 0.9393550157546997, 0.9217212200164795, 0.9503871202468872, 0.9311342239379883, 0.9474338889122009, 0.9550080299377441, 0.9535457491874695, 0.9301470518112183, 0.9538137912750244, 0.9278372526168823, 0.9438306093215942, 0.9283808469772339, 0.945446252822876, 0.9133992791175842, 0.96291583776474, 0.9406785368919373, 0.937340259552002, 0.9429607391357422, 0.9367217421531677, 0.9535131454467773, 0.9406852722167969, 0.9624146223068237, 0.9211745858192444, 0.925301194190979, 0.9478023648262024, 0.9339979290962219, 0.9360417127609253, 0.9529837965965271, 0.9889805316925049, 0.9522711038589478, 0.9465597867965698, 0.946028470993042, 0.9442533850669861, 0.9420616626739502, 0.9362232685089111, 0.9628815650939941, 0.9131633043289185, 0.9354126453399658, 0.9855343103408813, 0.9585021734237671, 0.9605716466903687, 0.950525164604187, 0.9380657076835632, 1.0193192958831787, 0.9208873510360718, 0.9334864616394043, 0.9375262260437012, 0.9358031153678894, 0.9507482051849365, 0.9392451047897339, 0.9478209614753723, 0.94184410572052, 0.9507166743278503, 0.9307110905647278, 0.9530119299888611, 0.9653695225715637, 0.9162509441375732, 0.9564511179924011, 0.952282190322876, 0.9025505781173706, 0.986207127571106, 0.9359780550003052, 0.9486269950866699, 0.7863818407058716, 0.9354474544525146, 0.9520946741104126, 0.9451248645782471, 0.9449270367622375, 0.8858640193939209, 0.9155575633049011, 0.9500303268432617, 0.9462668895721436, 0.9524821043014526, 0.9484480023384094, 0.94256991147995, 0.9507439136505127, 0.8976184129714966, 0.9514175653457642, 0.9367336630821228, 0.9600547552108765, 0.9249322414398193, 0.9255224466323853, 0.9467759728431702, 0.944473385810852, 0.9146011471748352, 0.9480277299880981, 0.9449739456176758, 0.9548389315605164, 0.9325405359268188, 0.9311178922653198, 0.9440912008285522, 0.953873872756958, 0.9386227130889893, 0.9335637092590332, 0.9424903392791748, 0.9595468044281006, 0.9510319828987122, 0.9377316236495972, 0.9640496969223022, 0.9212306141853333, 0.9557656049728394, 0.9860451221466064, 0.9585469365119934, 0.9425657987594604, 0.9109442830085754, 0.933851420879364, 0.9286573529243469, 0.9508110284805298, 0.9884536266326904, 0.9563668966293335, 0.9628888964653015, 0.9556326866149902, 0.9428423643112183, 0.9233198761940002, 0.9454475045204163, 0.9621462225914001, 0.9480048418045044, 0.9311121106147766, 0.9413026571273804, 0.9032307267189026, 0.9274662137031555, 0.9434332847595215, 0.9590188264846802, 0.915752112865448, 0.9561886191368103, 0.9166756868362427, 0.9445752501487732, 0.9489889144897461, 0.9477574229240417, 0.9154781699180603, 0.9352690577507019, 0.8995501399040222, 0.932748556137085, 0.9623703956604004, 0.9507855772972107, 0.9369509220123291, 0.9355412721633911, 0.8983354568481445, 0.9317288994789124, 0.9335505962371826, 0.9319967031478882, 0.9483605623245239, 0.9332844614982605, 0.9535660147666931, 0.9373824596405029, 0.9470782279968262, 0.9591504335403442, 0.9686967134475708, 0.9532307386398315, 0.9666538238525391, 0.9268110990524292, 0.9252128005027771, 0.9625861644744873, 0.9343326091766357, 0.9255362153053284, 0.9635845422744751, 0.930698037147522, 0.9165393710136414, 0.9347048997879028, 0.9450027346611023, 0.9354541301727295, 0.9816610217094421, 0.9457634687423706, 0.9603936076164246, 0.9592934846878052, 0.9640778303146362, 0.9397743940353394, 0.9496909379959106, 0.9466522932052612, 0.9271789789199829, 0.9528805017471313, 0.9348023533821106, 0.9443880915641785, 0.938320517539978, 0.9651539325714111, 0.9384365081787109, 0.9542539119720459, 0.9273508191108704, 0.9251771569252014, 0.9480605721473694, 0.947551965713501, 0.9493019580841064, 0.9405725002288818, 0.9493119716644287, 0.938208818435669, 0.9479552507400513, 0.9525995850563049, 0.959636926651001, 0.9327268600463867, 0.9060616493225098, 0.9102662801742554, 0.9199068546295166, 0.9382230639457703, 0.905227541923523, 0.9535928964614868, 0.9322072863578796, 0.9213959574699402, 0.9766043424606323, 0.9426262974739075, 0.9174462556838989, 0.9357345700263977, 0.8992730379104614, 0.8745297193527222, 0.939131498336792, 0.9380906820297241, 0.9291743040084839, 0.9503748416900635, 0.9359831213951111, 0.9118701219558716, 0.9636911153793335, 0.9508814811706543, 0.9477145075798035, 0.9532084465026855, 0.9521278738975525, 0.9317103624343872, 0.9485408067703247, 0.9543030858039856, 0.9551421403884888, 0.9495748281478882, 0.9233434200286865, 0.9430053234100342, 0.938227653503418, 0.9342348575592041, 0.920613706111908, 0.9379516243934631, 0.9316001534461975, 0.9620188474655151, 0.9524310827255249, 0.9545760154724121, 0.9572217464447021, 0.947458028793335, 0.937605619430542, 0.9350884556770325, 0.9474750757217407, 0.9481584429740906, 0.9408296346664429, 0.9362576007843018, 0.8897674083709717, 0.9564938545227051, 0.9105575084686279, 0.9457802176475525, 0.9348023533821106, 0.9384199380874634, 0.9386717081069946, 0.9381786584854126, 0.949638307094574, 0.9388746023178101, 0.9472065567970276, 0.9614488482475281, 0.9391432404518127, 0.9333082437515259, 0.9194727540016174, 0.9100541472434998, 0.9522472620010376, 0.9238795042037964, 0.9586634635925293, 0.9453245401382446, 0.9182859659194946, 0.9577511548995972, 0.9287168979644775, 0.9601203799247742, 0.9422341585159302, 0.952888011932373, 0.9564915299415588, 0.9287657141685486, 0.9833614826202393, 0.9507708549499512, 0.9434789419174194, 0.9340935349464417, 0.9410927295684814, 0.9522528648376465, 0.9395463466644287, 0.9617481827735901, 0.9240232706069946, 0.9416914582252502, 0.9248127937316895, 0.9189019203186035, 0.952703595161438, 0.9566496014595032, 0.9430243372917175, 0.9724589586257935, 0.9345159530639648, 0.9505809545516968, 0.9404651522636414, 0.9596368670463562, 0.9480581283569336, 0.928321123123169, 0.9574751257896423, 0.9246565103530884, 0.9454208612442017, 0.8930301666259766, 0.9290761947631836, 0.9595539569854736, 0.951127290725708, 0.990803599357605, 0.9211387634277344, 0.9570825099945068, 0.9307695031166077, 0.9468240737915039, 0.9530887603759766, 0.9238674640655518, 0.9619314074516296, 0.9432677030563354, 0.9533208012580872, 0.9585512280464172, 0.9483184218406677, 0.9174618721008301, 0.963555097579956, 0.9408226609230042, 0.9513654112815857, 0.9410574436187744, 0.954174816608429, 0.932001531124115, 0.957017183303833, 0.9511463046073914, 0.9433792233467102, 0.9389490485191345, 0.9498081207275391, 0.9375165700912476, 0.9395617246627808, 0.920074999332428, 0.9307776689529419, 0.9101594686508179, 0.9576326608657837, 0.9396227598190308, 0.9185555577278137, 0.9471503496170044, 0.9470864534378052, 0.9412612318992615, 0.940829336643219, 0.9677849411964417, 0.9201593399047852, 0.9553540945053101, 0.9628687500953674, 0.9272408485412598, 0.9445163011550903, 0.9178498387336731, 0.9765285849571228, 0.9440592527389526, 0.9193000197410583, 0.9604519605636597, 0.9883912801742554, 0.9378248453140259, 0.977635383605957, 0.9306905269622803, 0.9572806358337402, 0.9454038143157959, 0.9164448976516724, 0.9364567995071411, 0.9443538188934326, 1.0010432004928589, 0.9821981191635132, 0.9479438066482544, 0.9570324420928955, 0.9441046714782715, 0.9726314544677734, 0.9385589361190796, 0.9223572611808777, 0.9047274589538574, 0.9360640645027161, 0.9229583740234375, 0.9435725212097168, 0.9581380486488342, 0.949862003326416, 0.8859426379203796, 0.9112948775291443, 0.9642143249511719, 0.961386501789093, 0.9385665655136108, 0.9520293474197388, 0.9506934881210327, 0.9478163719177246, 0.9168758392333984, 0.9419512748718262, 0.9546806812286377, 0.9547652006149292, 0.900623083114624, 0.9540565609931946, 0.9585598111152649, 0.9420920610427856, 0.9259943962097168, 0.9405219554901123, 0.958824098110199, 0.92530357837677, 0.9527014493942261, 0.9460973739624023, 0.9375947713851929, 0.939598560333252, 0.9373871088027954, 0.9273350834846497, 0.9697625041007996, 0.9264598488807678, 0.9480401277542114, 0.9444500207901001, 0.9278674125671387, 0.9482037425041199, 0.9268511533737183, 0.962303876876831, 0.9236717224121094, 0.9278154969215393, 0.9521016478538513, 0.9522819519042969, 0.9424091577529907, 0.9310711622238159, 0.9120919704437256, 0.9445044994354248, 0.95215904712677, 0.9558244347572327, 0.9314424395561218, 0.9469858407974243, 0.9300864934921265, 0.9003064632415771, 0.9342130422592163, 0.9567872285842896, 0.9565695524215698, 0.924353837966919, 0.9506840705871582, 0.9573339819908142, 0.9354271292686462, 0.9206101894378662, 1.0773097276687622, 0.9508676528930664, 0.9635287523269653, 0.9473243355751038, 0.9271920323371887, 0.9333251714706421, 0.9213860034942627, 0.951180100440979, 0.9451569318771362, 0.9334313273429871, 0.9426170587539673, 0.9204808473587036, 0.9252289533615112, 0.969765841960907, 0.9745076298713684, 0.9141323566436768, 0.944412350654602, 0.9574469923973083, 0.9369842410087585, 0.9036849737167358, 0.9770742654800415, 0.9510658383369446, 0.9362227916717529, 0.9594560265541077, 0.9081395864486694, 0.931719183921814, 0.9142524003982544, 0.9652037024497986, 0.9617159962654114, 0.9479517340660095, 0.9532074928283691, 0.9490036368370056, 0.9326826930046082, 0.9248051643371582, 0.9402921199798584, 0.9296257495880127, 0.9385520815849304, 0.9746723771095276, 0.9868373274803162, 0.9635685682296753, 0.9134513139724731, 0.9526646137237549, 0.9572345614433289, 0.9346626996994019, 0.9383310079574585, 0.9287705421447754, 0.9319462776184082, 0.9766099452972412, 0.9782665967941284, 0.9436200857162476, 0.8717080354690552, 0.9104053974151611, 0.9373093843460083, 0.9361635446548462, 0.9265446662902832, 0.944983184337616, 0.9579242467880249, 0.9457689523696899, 0.9271957874298096, 0.9234127998352051, 0.9582216143608093, 0.869604229927063, 0.9525943994522095, 0.9306299686431885, 0.9576727747917175, 0.9593380689620972, 0.959419846534729, 0.9322441816329956, 0.9459261894226074, 0.9777514338493347, 0.9483023881912231, 0.9412459135055542, 0.9621130228042603, 0.9645047187805176, 0.9433426260948181, 0.9498205780982971, 0.9340065717697144, 0.94096440076828, 0.9727959632873535, 0.9419946074485779, 0.9426101446151733, 0.9594272375106812, 0.9446371793746948, 0.9613966345787048, 0.9374735355377197, 0.9177978038787842, 0.898408055305481, 0.9405125379562378, 0.9417487382888794, 0.9467638731002808, 0.9665766954421997, 0.9384993314743042, 0.9452444314956665, 0.9536945819854736, 0.8950690031051636, 0.9501434564590454, 0.9430022239685059, 0.920051097869873, 0.9577839374542236, 0.9522589445114136, 0.9388893246650696, 0.9241483211517334, 0.9530411958694458, 0.9554398059844971, 0.9555798768997192, 0.9501689672470093, 0.892010509967804, 0.9336436986923218, 0.9431426525115967, 0.9210628271102905, 0.9578502178192139, 0.9476560354232788, 0.944254994392395, 0.9439588785171509, 0.9549980163574219, 0.9352930784225464, 0.9111212491989136, 0.9165581464767456, 0.9117710590362549, 0.9414611458778381, 0.9845468997955322, 0.9519157409667969, 0.9518303871154785, 0.9256457090377808, 0.9329252243041992, 0.9411540627479553, 0.952346920967102, 0.9570815563201904, 0.9597437977790833, 0.9521991014480591, 0.9158183932304382, 0.9098396301269531, 0.958034098148346, 0.9403417110443115, 0.9236190915107727, 0.9576640129089355, 0.9414691925048828, 0.938583254814148, 1.0030452013015747, 0.9415727257728577, 0.986559271812439, 0.9538418054580688, 0.9601458311080933, 0.9548653364181519, 0.9082159996032715, 0.959160566329956, 0.9107812643051147, 0.9752601385116577, 0.9458135962486267, 0.9409818649291992, 0.9745703935623169, 0.9299949407577515, 0.9371695518493652, 0.9217658638954163, 0.9585776925086975, 0.9420365691184998, 0.9514219164848328, 0.925894021987915, 0.9291238784790039, 1.0046319961547852, 0.9331970810890198, 0.9902670383453369, 0.923814058303833, 0.9144304394721985, 0.9628557562828064, 0.9422740936279297, 0.9665663242340088, 0.9343945980072021, 0.9508026838302612, 0.9319930076599121, 0.940535306930542, 0.9585952758789062, 0.9322236180305481, 0.938257098197937, 0.9413551092147827, 0.9553388357162476, 0.9023829698562622, 0.9229583740234375, 0.9429641962051392, 0.9343386888504028, 0.9163832664489746, 0.9334694743156433, 0.9453594088554382, 0.9555957317352295, 0.9501070976257324, 0.9414074420928955, 0.9575995802879333, 0.93692946434021, 0.941415548324585, 0.9390429258346558, 0.9192980527877808, 0.9127583503723145, 0.9512421488761902, 0.9506029486656189, 0.9579412341117859, 0.9343760013580322, 0.940991997718811, 0.9827224016189575, 0.9387130737304688, 0.949294924736023, 0.9575943350791931, 0.9160482883453369, 0.9325517416000366, 0.9603219032287598, 0.9034215211868286, 0.9232434034347534, 0.9512879252433777, 0.9448481798171997, 0.9521277546882629, 0.9328571557998657, 0.9085025191307068, 0.9451857805252075, 0.9421115517616272, 0.9338308572769165, 0.9218047857284546, 0.960731565952301, 0.9477689266204834, 0.9529547095298767, 0.9257774949073792, 0.9652284383773804, 0.9534811973571777, 0.9400143027305603, 0.9033419489860535, 0.9774360060691833, 0.9493080377578735, 0.9590016603469849, 0.9462025165557861, 0.9510890245437622, 0.9355154037475586, 0.9582663774490356, 0.9621797204017639, 0.967675507068634, 0.9494093656539917, 0.9197275638580322, 0.9375670552253723, 0.926720380783081, 0.9247491955757141, 0.9263746738433838, 0.9121434092521667, 0.9490635395050049, 0.9468653202056885, 0.9404309988021851, 0.9622042179107666, 0.9114348888397217, 0.9305026531219482, 0.9288736581802368, 0.9440610408782959, 0.938597559928894, 0.9455186128616333, 0.9403288960456848, 0.9350026845932007, 0.933247447013855, 0.9556994438171387, 0.8930985927581787, 0.9578160047531128, 0.9272851943969727, 0.9456108212471008, 0.9527089595794678, 0.928399920463562, 0.9791584014892578, 0.9635862708091736, 0.9381834268569946, 0.9399917125701904, 0.9175797700881958, 0.9481946229934692, 0.9667866230010986, 0.9626873135566711, 0.9384021759033203, 0.9530051946640015, 0.930563747882843, 0.9147099852561951, 0.9602483510971069, 0.918220579624176, 0.8819890022277832, 0.9277068376541138, 0.9398806691169739, 0.9479782581329346, 0.9331145286560059, 0.9323921203613281, 0.9287889003753662, 0.9680252075195312, 0.9705913066864014, 0.9317775368690491, 0.923920214176178, 0.9606592655181885, 0.9460904598236084, 0.9381592273712158, 0.9392340779304504, 0.9283562898635864, 0.9213298559188843, 0.937096118927002, 0.9360300302505493, 0.9493569731712341, 0.9406970143318176, 0.9437057375907898, 0.9145617485046387, 0.939848780632019, 0.9621910452842712, 0.9705527424812317, 0.9144965410232544, 0.9571460485458374, 0.9511704444885254, 0.9432244896888733, 0.9467161297798157, 0.9292542338371277, 0.9164860844612122, 0.9364659786224365, 0.9429103136062622, 0.9403348565101624, 0.928321123123169, 0.9686666131019592, 0.9466278553009033, 0.9252064228057861, 0.9148004055023193, 0.9433442950248718, 0.933231770992279, 0.9539055228233337, 0.9138291478157043, 0.9036049842834473, 0.9512250423431396, 0.9353216886520386, 0.9484511017799377, 0.9328571557998657, 0.9405220746994019, 0.9268931150436401, 0.9909435510635376, 0.9180947542190552, 0.9181309342384338, 0.9614969491958618, 0.9566134214401245, 0.970770537853241, 0.9400830268859863, 0.9325540661811829, 0.9590033888816833, 0.9219952821731567, 0.938052237033844, 0.9464513063430786, 0.9105092883110046, 0.9521452784538269, 0.9594974517822266, 0.9506886005401611, 0.953551173210144, 0.9624069929122925, 0.9463108777999878, 0.9390847086906433, 0.9479445219039917, 0.9532088041305542, 0.929516077041626, 0.9585474729537964, 0.9558504819869995, 0.9986560344696045, 0.9436964988708496, 0.9517369270324707, 0.963577926158905, 0.9298818111419678, 0.879599392414093, 0.950337827205658, 0.8460310697555542, 0.9646853804588318, 0.9340105056762695, 0.939922571182251, 0.9342801570892334, 0.9523216485977173, 0.9410818815231323, 0.8935549259185791, 0.9334307909011841, 0.9600680470466614, 0.975585401058197, 0.9330636262893677, 0.9630255699157715, 0.9181275367736816, 0.9516350626945496, 0.9830902814865112, 0.9412468671798706, 0.9268475770950317, 0.9554451704025269, 0.958231508731842, 0.9418603181838989, 0.9326321482658386, 0.904897928237915, 0.9340487718582153, 0.9502567052841187, 0.9659514427185059, 0.9525518417358398, 0.9410760402679443, 0.9475946426391602, 0.9513231515884399, 0.9334828853607178, 0.9374651908874512, 0.9530411958694458, 0.900812029838562, 0.9516487121582031, 0.9301470518112183, 0.9470606446266174, 0.9528369307518005, 0.9278323650360107, 0.950434684753418, 0.952065110206604, 0.9562159180641174, 0.9331187009811401, 0.9320555925369263, 0.9589444398880005, 0.9434899091720581, 0.9391801357269287, 0.943974494934082, 0.9790210723876953, 0.9616672992706299, 0.9580366015434265, 0.9384961724281311, 0.9472134113311768, 0.9607746601104736, 0.95899897813797, 0.9264759421348572, 0.9322148561477661, 0.9614028334617615, 0.9006457924842834, 0.9639257788658142, 0.9553915858268738, 0.9146583080291748, 0.955552339553833, 0.9356277585029602, 0.9305465221405029, 0.9573823809623718, 0.9370447397232056, 0.9343852996826172, 0.9378658533096313, 0.9447946548461914, 0.9510297179222107, 0.9364457130432129, 0.9760185480117798, 0.9614585638046265, 0.9344872236251831, 0.9670529365539551, 0.9471667408943176, 0.9336471557617188, 0.9104486703872681, 0.9543424844741821, 0.9508181810379028, 0.9435186386108398, 0.9651561379432678, 0.9547408223152161, 0.9048388004302979, 0.9508997201919556, 0.9463909864425659, 0.9635553956031799, 0.9080171585083008, 0.9224603176116943, 0.9203305244445801, 0.9515438079833984, 0.9432051181793213, 0.9523324966430664, 0.928931713104248, 0.9353934526443481, 0.935696005821228, 0.943934440612793, 0.9724269509315491, 0.9278934001922607, 0.9499174952507019, 0.9444160461425781, 0.9572829604148865, 0.9095821976661682, 0.9566694498062134, 0.9467273950576782, 0.9378607273101807, 0.9832700490951538, 0.9439734220504761, 0.9460554718971252, 0.9449096918106079, 0.945927083492279, 0.9382038712501526, 0.9287415742874146, 0.8581178188323975, 0.9473532438278198, 0.9560956954956055, 0.9339793920516968, 0.9309096336364746, 0.954174816608429, 0.9590970277786255, 0.9495329260826111, 0.948316216468811, 0.9562270045280457, 0.9397983551025391, 0.9164251685142517, 0.9292634129524231, 0.9614549875259399, 0.9308902621269226, 0.9171354174613953, 0.9419655799865723, 0.8921653032302856, 0.956089973449707, 0.9312765002250671, 0.9480145573616028, 0.9366209506988525, 0.9426891207695007, 0.9734451770782471, 0.9125117659568787, 0.9276071786880493, 0.9436421394348145, 0.9226903915405273, 0.9164860844612122, 0.9444820880889893, 0.9501953721046448, 0.9398250579833984, 0.9389903545379639, 0.9643895626068115, 0.952648401260376, 0.91884446144104, 0.9546393156051636, 0.9537515640258789, 0.9179375171661377, 0.9641077518463135, 0.9362978935241699, 0.9373194575309753, 0.921073317527771, 0.895428478717804, 0.9363319277763367, 0.9511662125587463, 0.9475986957550049, 0.9532926082611084, 0.9308663606643677, 0.9212965965270996, 0.9277690649032593, 0.9435650706291199, 0.9504928588867188, 0.9362800717353821, 0.9127572774887085, 0.9448221921920776, 0.9352800250053406, 0.9121002554893494, 0.9354300498962402, 0.9244338870048523, 0.9467334151268005, 0.9437408447265625, 0.9582576751708984, 0.9345637559890747, 0.961318850517273, 0.932338535785675, 0.9466211795806885, 0.9302098751068115, 0.9449656009674072, 0.9312369227409363, 0.9510931968688965, 0.9494388103485107, 0.9350504279136658, 0.8897674083709717, 0.941877007484436, 0.8907676339149475, 0.9445002675056458, 0.937305212020874, 0.9450960159301758, 0.9487773180007935, 0.9308626055717468, 0.9437078833580017, 0.9272067546844482, 0.9453656673431396, 0.9445728063583374, 0.9462646245956421, 0.8975545167922974, 0.9252851009368896, 0.9627808928489685, 0.966896653175354, 0.8819863796234131, 0.9280242323875427, 0.9630592465400696, 0.9320089221000671, 0.9619452953338623, 0.9115267992019653, 0.9420920610427856, 0.9512592554092407, 0.9200958013534546, 0.9262779951095581, 0.9433166980743408, 0.9830735921859741, 0.9318886995315552, 0.9424448013305664, 0.9474263787269592, 0.9477676749229431, 0.9511812925338745, 0.980461835861206, 0.930926501750946, 0.9139156937599182, 0.8986107110977173, 0.9480594396591187, 0.9149582386016846, 0.952738881111145, 0.9265329837799072, 0.9489282369613647, 0.9506373405456543, 0.9437015056610107, 0.931334376335144, 0.9352527856826782, 0.9717407822608948, 0.9388042688369751, 0.941981315612793, 0.9088699817657471, 0.9583483934402466, 0.9530844688415527, 0.9204851388931274, 0.9434404969215393, 0.9438260793685913, 0.9219006299972534, 0.9459959864616394, 0.9407293200492859, 0.9432553052902222, 0.953738808631897, 0.9183964729309082, 0.9405879974365234, 0.9790554642677307, 0.9498443007469177, 0.9283199906349182, 0.9474974870681763, 0.9153736233711243, 0.9198648929595947, 0.9594447612762451, 0.934274435043335, 0.9495816230773926, 0.9350686073303223, 0.9604789018630981, 0.9431556463241577, 0.9244967699050903, 0.944535493850708, 0.9717736840248108, 0.9510705471038818, 0.9366037845611572, 0.9478243589401245, 0.9208590388298035, 0.9488276839256287, 0.9084553718566895, 0.9499011039733887, 0.9420462250709534, 0.9420132637023926, 0.9587473273277283, 0.965995728969574, 0.9328546524047852, 0.9362800717353821, 0.9658092856407166, 0.9623598456382751, 0.9436241388320923, 0.9600056409835815, 0.9338865280151367, 0.9417803287506104, 0.9394357204437256, 0.9348224997520447, 0.9420034885406494, 0.900113582611084, 0.9287168979644775, 0.9521501064300537, 0.9394999742507935, 0.9472134113311768, 0.9083536863327026, 0.9257227182388306, 0.9340876936912537, 0.9330384731292725, 0.9610956907272339, 0.9296908378601074, 0.9497416615486145, 0.9374934434890747, 0.915532112121582, 0.9322816133499146, 0.9768669009208679, 0.9786690473556519, 0.8740139007568359, 0.9404553174972534, 0.9527145624160767, 0.9647547602653503, 0.9509949684143066, 0.9751893281936646, 0.950928807258606, 0.9646987915039062, 0.9476096034049988, 0.9593585729598999, 0.8777499198913574, 0.9169256687164307, 0.9546312093734741, 0.9437041282653809, 0.9800032377243042, 0.9514421224594116, 0.9429231882095337, 0.9553548693656921, 0.9435064196586609, 0.9414492845535278, 0.9438768625259399, 0.9316368103027344, 0.9382354021072388, 0.9338400959968567, 0.91098552942276, 0.9471758604049683, 0.951980471611023, 0.9731005430221558, 0.9626356363296509, 0.9155739545822144, 0.9437302947044373, 0.9354645609855652, 0.945246696472168, 0.9572217464447021, 0.9238813519477844, 0.9213238954544067, 0.9828300476074219, 0.9333338737487793, 0.9465458989143372, 0.9844017028808594, 0.9079273343086243, 0.9328023791313171, 0.9298019409179688, 0.9312498569488525, 0.9523637294769287, 0.9533796310424805, 0.9651589393615723, 0.9102518558502197, 0.9518818855285645, 0.951228141784668, 0.9463714361190796, 0.9509649276733398, 0.9454431533813477, 0.9586247801780701, 0.9406740069389343, 0.9335267543792725, 0.9503593444824219, 0.9490955471992493, 0.9274176359176636, 0.9544305801391602, 0.9222431778907776, 0.906895637512207, 0.9380027055740356, 0.9148408770561218, 0.9481505751609802, 0.9584953784942627, 0.9206674098968506, 0.9634407162666321, 0.84645676612854, 0.9635812044143677, 0.9373995065689087, 0.9437740445137024, 0.9246077537536621, 0.9453827142715454, 0.9098882675170898, 0.9596068263053894, 0.9594743847846985, 0.9777780771255493, 0.9361003637313843, 0.9207927584648132, 0.9364528656005859, 0.9429619908332825, 0.9412106275558472, 0.9249928593635559, 0.9392926692962646, 0.9401131272315979, 0.9381592273712158, 0.9884001016616821, 0.9308582544326782, 0.9313606023788452, 0.944137692451477, 0.9374277591705322, 0.9504468441009521, 0.9561152458190918, 0.9547823071479797, 0.929431676864624, 0.967776358127594, 0.9317827224731445, 0.9194797277450562, 1.0024065971374512, 0.9484118223190308, 0.9589473605155945, 0.95600825548172, 0.9769647121429443, 0.9487950801849365, 0.9496557116508484, 0.9391841292381287, 0.9277576208114624, 0.9279376864433289, 0.9705754518508911, 0.9272397756576538, 0.9600681066513062, 0.9290156960487366, 0.9313687086105347, 0.9211745858192444, 0.9261809587478638, 0.9547570943832397, 1.469871997833252, 0.949141800403595, 0.9445730447769165, 0.9423986673355103, 0.9249783754348755, 0.9440047144889832, 0.9617997407913208, 0.909856915473938, 0.9624406695365906, 0.9188963174819946, 0.9298067092895508, 0.9374128580093384, 0.9271787405014038, 0.9534823894500732, 0.9393492341041565, 0.8470633625984192, 0.9330027103424072, 0.9407182931900024, 0.9503952860832214, 0.9484262466430664, 0.9224464893341064, 0.9258298873901367, 0.9085025191307068, 0.9529408812522888, 0.9319183230400085, 0.9259936809539795, 0.9530633687973022, 0.9522145986557007, 0.9475597739219666, 0.9436172246932983, 0.8951197266578674, 0.9401278495788574, 1.0046586990356445, 0.9311617016792297, 0.9567046165466309, 0.9369642734527588, 0.9436964988708496, 0.9405699968338013, 0.9385859966278076, 0.9229583740234375, 0.9618198275566101, 0.9435086250305176, 0.9482388496398926, 0.9299321174621582, 0.9800860285758972, 0.9482734203338623, 0.9549058675765991, 0.953694760799408, 0.9343878030776978, 0.9869567155838013, 0.9436430931091309, 0.9522279500961304, 0.9404438734054565, 0.9196826815605164, 0.9619894623756409, 0.9465322494506836, 0.9367185831069946, 0.9665834307670593, 0.9580931663513184, 0.9658492803573608, 0.9547122716903687, 0.9689126014709473, 0.9461857080459595, 0.9045144319534302, 0.9197718501091003, 0.9557330012321472, 0.9155559539794922, 0.9089635610580444, 0.9382988810539246, 0.9333963394165039, 0.9072245955467224, 0.9532034397125244, 0.9711814522743225, 0.9410225749015808, 0.965505838394165, 0.9135557413101196, 0.935293436050415, 0.9566470384597778, 0.954098105430603, 0.8993973731994629, 0.9707685708999634, 0.9141846895217896, 0.9474155306816101, 0.909902036190033, 0.9297720193862915, 0.9591493606567383, 0.9408687353134155, 0.9493277072906494, 0.9418463110923767, 0.9393905997276306, 0.9492576122283936, 0.908260703086853, 0.918521523475647, 0.954247772693634, 0.9426946640014648, 0.9470566511154175, 0.9345074892044067, 0.9429208636283875, 0.9351541996002197, 0.9167982339859009, 0.9591537714004517, 0.9312504529953003, 0.9239077568054199, 0.9509177207946777, 0.9760879278182983, 0.9118987321853638, 0.9643324613571167, 0.9560508728027344, 1.0179297924041748, 0.9394549131393433, 0.9363719820976257, 0.934412956237793, 0.9273413419723511, 0.9517682194709778, 0.9463356733322144, 0.9017199277877808, 0.9489094018936157, 0.9501008987426758, 0.9162042140960693, 0.9306837916374207, 0.9242976307868958, 0.947100043296814, 0.9581212997436523, 0.9219679832458496, 0.9335407018661499, 0.9457738399505615, 0.923954427242279, 0.938870906829834, 0.9395760297775269, 0.9601267576217651, 0.9611786603927612, 0.9398708939552307, 0.9511287212371826, 0.9371132850646973, 0.9190880060195923, 0.9505871534347534, 0.9424389004707336, 0.9331187009811401, 0.9494112133979797, 0.9414628148078918, 0.944983959197998, 0.9386876225471497, 0.9537270069122314, 0.9454821944236755, 0.9496403336524963, 0.9315931797027588, 0.9617171287536621, 0.9430513381958008, 0.9508382081985474, 0.9452977180480957, 0.938920795917511, 0.937137246131897, 0.960702657699585, 0.9299087524414062, 0.9629594683647156, 0.9454212188720703, 0.9787694215774536, 0.9425690770149231, 0.9251136779785156, 0.9431458115577698, 0.967772901058197, 0.9611291289329529, 0.9488319158554077, 0.9584993720054626, 0.9532915353775024, 1.1380772590637207, 0.9220748543739319, 0.9423385858535767, 0.9464885592460632, 0.9017425775527954, 0.9644014239311218, 0.9417036771774292, 0.9569888114929199, 0.9585656523704529, 0.9520748853683472, 0.9161595106124878, 0.9424373507499695, 0.9576460123062134, 0.9321843385696411, 0.9349430799484253, 0.9905441999435425, 0.9645159840583801, 0.970639169216156, 0.9521138668060303, 0.9348545074462891, 0.9509533643722534, 0.8921520113945007, 0.9388375878334045, 0.9409811496734619, 0.9508873224258423, 0.9542262554168701, 0.9269986748695374, 0.924554705619812, 0.9465981721878052, 0.904956579208374, 0.9463292956352234, 0.8957318663597107, 0.9206873178482056, 0.9386531710624695, 0.9470953941345215, 0.9638112187385559, 0.9193911552429199, 0.9139578342437744, 0.9134144186973572, 0.9556905031204224, 0.9775066375732422, 0.9906899929046631, 0.9491344094276428, 0.9565317034721375, 0.9510396718978882, 0.9522921442985535, 0.9378973245620728, 0.961510419845581, 0.9518566727638245, 0.9555659294128418, 0.9459905624389648, 0.93723464012146, 0.9514740705490112, 0.9525766372680664, 0.9387617707252502, 0.9402694702148438, 0.9372261762619019, 0.9629570245742798, 0.9503488540649414, 0.9607829451560974, 0.960842490196228, 0.9101278781890869, 0.9288636445999146, 0.9531043171882629, 0.9627478122711182, 0.9529837965965271, 0.9353582859039307, 0.965201735496521, 0.9453047513961792, 0.8945329189300537, 0.9349454045295715, 0.929317831993103, 0.9523410797119141, 0.9411956071853638, 0.958231508731842, 0.946924090385437, 0.9145725965499878, 0.9384036064147949, 0.9132306575775146, 0.9436744451522827, 0.9713020324707031, 0.9453837275505066, 0.968732476234436, 1.043787956237793, 0.9575459361076355, 0.9561479091644287, 0.9393746852874756, 0.931707501411438, 0.9282275438308716, 0.9399581551551819, 0.938346266746521, 0.9310787916183472, 0.9218138456344604, 0.9483227729797363, 0.9617394804954529, 0.9509340524673462, 0.9367560148239136, 0.9358766078948975, 0.9373598098754883, 0.8965581655502319, 0.9320071935653687, 0.9489092826843262, 0.9390429258346558, 0.9440224170684814, 0.9402049779891968, 0.9426687955856323, 0.921392560005188, 0.9475741386413574, 0.9594237804412842, 0.9507584571838379, 0.9509080648422241, 0.9515831470489502, 0.9524418115615845, 0.9425147175788879, 0.9306708574295044, 0.9605515599250793, 0.9474206566810608, 0.958419919013977, 0.9478550553321838, 0.9579472541809082, 0.9306434392929077, 0.9378840923309326, 0.9721338748931885, 0.9529212713241577, 0.9396597146987915, 0.92303067445755, 0.9197705984115601, 0.9324676990509033, 0.9435622096061707, 0.9628483653068542, 0.9350981712341309, 0.9351358413696289, 0.9726943969726562, 0.941329836845398, 0.9360858798027039, 0.9213959574699402, 0.9533427357673645, 0.922069251537323, 0.8845530152320862, 0.9538367986679077, 1.0027363300323486, 0.9288535714149475, 0.961386501789093, 0.9492863416671753, 0.8974200487136841, 0.9288094639778137, 0.9498788118362427, 0.9681025743484497, 0.9381804466247559, 0.9591902494430542, 0.9602404832839966, 0.9220642447471619, 0.9318399429321289, 0.9568401575088501, 0.9441321492195129, 0.9245390892028809, 0.976826012134552, 0.9402183294296265, 0.9653742909431458, 0.9674769043922424, 0.9444873332977295, 0.9204610586166382, 0.9344505071640015, 0.9439029693603516, 0.9576708674430847, 0.929908037185669, 0.9126390218734741, 0.9570099115371704, 0.9520810842514038, 0.929479718208313, 0.9509724974632263, 0.9167622327804565, 0.9627163410186768, 0.9343376755714417, 0.9219787120819092, 0.9144333600997925, 0.9549052715301514, 0.9540672302246094, 0.9499849677085876, 0.957337498664856, 0.964034378528595, 0.9252098798751831, 0.9430513381958008, 0.9567326307296753, 0.9593460559844971, 0.9201724529266357, 0.9675209522247314, 0.9241890907287598, 0.9399713277816772, 0.9550585150718689, 0.9426084756851196, 0.9623689651489258, 0.948316216468811, 0.9307146668434143, 0.9800052046775818, 0.9599823355674744, 0.948305606842041, 0.9191679358482361, 0.9334909915924072, 0.9667373299598694, 0.921100914478302, 0.9355894923210144, 0.9510664343833923, 0.9517189860343933, 0.9175130128860474, 0.9329837560653687, 0.9348940849304199, 0.9211763739585876, 0.9631729125976562, 0.9475406408309937, 0.9113560318946838, 0.9324444532394409, 0.9386247396469116, 0.9314733147621155, 0.9130779504776001, 0.9301363229751587, 0.9346975088119507, 0.9573606252670288, 0.9374973773956299, 0.9580591917037964, 0.9427236318588257, 0.9341176748275757, 0.932826817035675, 0.9606637358665466, 0.933552622795105, 0.9412452578544617, 0.9567273855209351, 0.9144200086593628, 0.955894947052002, 0.9335694909095764, 0.961698055267334, 0.8753291964530945, 0.9375641345977783, 0.9344872236251831, 0.9156948328018188, 0.9352266788482666, 0.9472600221633911, 0.9163640141487122, 0.9389796257019043, 0.9695369005203247, 0.9380892515182495, 0.9491209983825684, 0.9434124231338501, 0.9107822179794312, 0.9473574161529541, 0.949840784072876, 0.9307063817977905, 0.9380888938903809, 0.9247563481330872, 0.944553554058075, 0.9107619524002075, 1.003004550933838, 0.9275074005126953, 0.9611327648162842, 0.9633097052574158, 0.9405816197395325, 0.9346250891685486, 0.9328042268753052, 0.9338167309761047, 0.937911868095398, 0.928816020488739, 0.9407848119735718, 0.9596735835075378, 0.9484332203865051, 0.9114141464233398, 0.9197790026664734, 0.9479888081550598, 0.9414254426956177, 0.9569540023803711, 0.9204742908477783, 0.9045144319534302, 0.905176043510437, 0.9368613958358765, 0.9577667713165283, 0.9996454119682312, 0.9562299847602844, 0.9279013872146606, 0.9316577911376953, 0.9398552179336548, 0.9430966377258301, 0.9476611614227295, 0.9286550879478455, 0.9440727233886719, 0.9479707479476929, 0.9553042650222778, 0.9485363960266113, 0.9604482054710388, 0.9596378803253174, 0.9507849216461182, 0.9403972625732422, 0.9455100297927856, 0.9554387331008911, 0.9406991004943848, 0.9810560941696167, 0.9542323350906372, 0.9185147285461426, 0.9385766983032227, 0.919546902179718, 0.9528191685676575, 0.965477705001831, 0.9601948261260986, 0.9245491027832031, 0.957955002784729, 0.9340329170227051, 0.9436147809028625, 0.9218174815177917, 0.9453288316726685, 0.9500383138656616, 0.9128074645996094, 0.9693247079849243, 0.9340007901191711, 0.9393417835235596, 0.929176390171051, 0.9780669808387756, 0.9479517340660095, 0.9506504535675049, 0.933512270450592, 0.9093518257141113, 0.9600231647491455, 0.9486632347106934, 0.9232577085494995, 0.9614719152450562, 0.9342275857925415, 0.945311963558197, 0.9421993494033813, 0.9502174258232117, 0.95090651512146, 0.950851559638977, 0.9579529762268066, 0.9521784782409668, 0.9180582761764526, 0.9384360313415527, 0.9766746759414673, 0.9454101324081421, 0.9263182878494263, 0.9056656360626221, 0.9363514184951782, 0.9337776899337769, 0.9455363154411316, 0.9751105308532715, 0.9320156574249268, 0.9555118083953857, 0.963382363319397, 0.9506840705871582, 0.9670277237892151, 0.9299383759498596, 0.9520276188850403, 0.9317429065704346, 0.962563157081604, 0.9478968381881714, 0.944412350654602, 0.9807218909263611, 0.9400970935821533, 0.9239307641983032, 0.9407980442047119, 0.941480815410614, 0.8997398018836975, 0.9502264261245728, 0.9315820932388306, 0.9466069936752319, 0.9350498914718628, 0.9677788019180298, 0.9442373514175415, 0.9770742654800415, 0.9167087078094482, 0.9867880344390869, 0.9752374291419983, 0.943557620048523, 0.9701148867607117, 0.9750160574913025, 0.9533838033676147, 0.9526847004890442, 0.9520256519317627, 0.9375717639923096, 0.9332667589187622, 0.9346487522125244, 0.9417970180511475, 0.945977509021759, 0.9506942629814148, 0.9287090301513672, 0.9437582492828369, 0.9361107349395752, 0.9624441862106323, 0.9571100473403931, 0.9416278004646301, 0.9162459969520569, 0.9501857161521912, 0.9382466077804565, 0.9557381272315979, 0.9567184448242188, 0.9517903327941895, 0.9316931962966919, 0.9460504055023193, 0.9579645991325378, 0.9362491965293884, 0.9570168256759644, 1.109114646911621, 0.9106967449188232, 0.9312417507171631, 0.9456435441970825, 0.9491832256317139, 0.9428371787071228, 0.946424663066864, 0.9700579643249512, 0.9414445161819458, 0.9553223848342896, 0.9317953586578369, 0.9252601265907288, 0.935772716999054, 0.9114869832992554, 0.9369380474090576, 0.9480254054069519, 0.9113566279411316, 0.9854346513748169, 0.936867892742157, 0.9489414095878601, 0.9321027994155884, 0.954552948474884, 0.9240100383758545, 0.9440085291862488, 0.9337167739868164, 0.945446252822876, 0.9511969089508057, 0.9560688138008118, 0.9236190915107727, 0.9460443258285522, 0.9514399170875549, 0.9076092839241028, 0.9591786861419678, 0.930545449256897, 0.9864717721939087, 0.9224133491516113, 0.9257234930992126, 0.9844752550125122, 0.9465001821517944, 0.9355562925338745, 0.9405725002288818, 0.9328361749649048, 0.9296970963478088, 0.9485160708427429, 0.9224368929862976, 0.9583443999290466, 0.9264415502548218, 1.0568430423736572, 0.9366236925125122, 0.9589137434959412, 0.9040405750274658, 0.8811326622962952, 0.9468845129013062, 0.9379918575286865, 0.9509028792381287, 0.9429112672805786, 0.9547564387321472, 0.9542175531387329, 0.9670277237892151, 0.9171786904335022, 0.9074127674102783, 0.9678829908370972, 0.9377328157424927, 0.9465069770812988, 0.9520549774169922, 0.9384338855743408, 0.9374279975891113, 0.9445858001708984, 0.9302792549133301, 0.9620077610015869, 0.9500777125358582, 0.9445501565933228, 0.9654053449630737, 0.9702712297439575, 0.9271067380905151, 0.9622842073440552, 0.8763035535812378, 0.9159314036369324, 0.9397397637367249, 0.9670836925506592, 0.926699161529541, 0.9367908835411072, 0.9292843341827393, 0.9571757316589355, 0.9519404768943787, 0.9581780433654785, 0.9600152969360352, 0.9596735239028931, 0.9434394836425781, 0.9502074122428894, 0.9628954529762268, 0.9627993106842041, 0.9763740301132202, 0.9423537254333496, 0.9221517443656921, 0.9322373270988464, 0.9450961351394653, 0.9438612461090088, 1.0288714170455933, 0.959821343421936, 0.956474244594574, 0.9422221779823303, 0.9586400985717773, 0.9226770401000977, 0.9485881924629211, 0.9475752115249634, 0.9424630403518677, 0.9468590617179871, 0.9600734114646912, 0.9426901936531067, 0.9335943460464478, 0.9414867162704468, 0.9572576880455017, 0.9082897305488586, 0.9261711239814758, 0.9402649402618408, 0.9244855642318726, 0.9502250552177429, 0.9349247217178345, 0.9471206068992615, 0.9164873957633972, 0.9737815260887146, 0.9514594674110413, 0.9502884149551392, 0.9470292329788208, 0.9666122198104858, 0.944328784942627, 0.9424534440040588, 0.8949570655822754, 0.9646830558776855, 0.9155157804489136, 0.9708545207977295, 0.9563732147216797, 0.9327030777931213, 0.94669508934021, 0.9216290712356567, 0.9359617233276367, 0.9346400499343872, 0.9532262682914734, 0.9320865273475647, 0.885881781578064, 0.9676385521888733, 0.9179494380950928, 0.9375214576721191, 0.9586509466171265, 0.9127962589263916, 0.9010072946548462, 0.9300153255462646, 0.9504680633544922, 0.9509166479110718, 0.9581986665725708, 0.9484736919403076, 0.9671578407287598, 0.9378763437271118, 0.9376260042190552, 0.9114264249801636, 0.9181950092315674, 0.9811657667160034, 0.9358515739440918, 0.9535956978797913, 0.9270902276039124, 0.9135222434997559, 0.9086681604385376, 0.9039323925971985, 0.9439446330070496, 0.937792181968689, 0.9605265259742737, 0.9163492918014526, 0.9649398922920227, 0.9452214241027832, 0.946858286857605, 0.9797726273536682, 0.9578810930252075, 0.8984649777412415, 0.9572797417640686, 0.9221112132072449, 0.9402183294296265, 0.9435698986053467, 0.9343945980072021, 0.9393633604049683, 0.9478283524513245, 0.9003762006759644, 0.9194797277450562, 0.9517432451248169, 0.932544469833374, 1.036194086074829, 0.9162804484367371, 0.9473210573196411, 0.953830897808075, 0.9678829908370972, 0.9376835823059082, 0.9515036344528198, 0.9680901765823364, 0.9135234355926514, 0.9335669279098511, 0.9573823809623718, 0.9036849737167358, 0.9039195775985718, 0.9433634281158447, 0.9708707332611084, 0.9578288197517395, 0.9221615791320801, 0.9303319454193115, 0.9521951675415039, 0.9375183582305908, 0.9498081207275391, 0.9337947368621826, 0.9500894546508789, 0.9414364099502563, 0.9462162852287292, 0.950275182723999, 0.9569252729415894, 0.9448890686035156, 0.972372829914093, 0.9430102109909058, 0.951501727104187, 0.9246426224708557, 0.9677410125732422, 0.9496996402740479, 0.9337141513824463, 0.9091670513153076, 0.9460220336914062, 0.9429678320884705, 0.913051962852478, 0.9252647757530212, 0.9436417818069458, 0.9442523121833801, 0.9493908286094666, 0.9082489013671875, 0.933770477771759, 0.9464322328567505, 0.9424006938934326, 0.9543445110321045, 0.9484083652496338, 0.9449797868728638, 0.9054833650588989, 0.9267542362213135, 0.9020137786865234, 0.9309539794921875, 0.9536569714546204, 0.9434975385665894, 0.9335265755653381, 0.9410827159881592, 0.9412106275558472, 0.9659267067909241, 0.9437302947044373, 0.9582837820053101, 0.9596861600875854, 0.9599951505661011, 0.9633321762084961, 0.9479620456695557, 0.9222795367240906, 0.9695167541503906, 0.9321777820587158, 0.9041704535484314, 0.9343640804290771, 0.9658885598182678, 0.9488887190818787, 0.9708234667778015, 0.9611063003540039, 0.9499911069869995, 0.9325924515724182, 0.9479007720947266, 0.9827899932861328, 0.9365801811218262, 0.9375408887863159, 0.9574464559555054, 0.947165846824646, 0.947767972946167, 0.9226217269897461, 0.9435481429100037, 0.9765670299530029, 0.9395731687545776, 0.913856029510498, 0.967871904373169, 0.9337601661682129, 0.965099036693573, 0.9474817514419556, 0.9673488736152649, 0.9421230554580688, 0.9317693710327148, 0.9485684633255005, 0.9174367189407349, 0.917775571346283, 0.9201870560646057, 0.924648642539978, 0.9353127479553223, 0.9255857467651367, 0.943871021270752, 0.9434525966644287, 0.9340226054191589, 0.937420666217804, 0.9223864078521729, 0.937187910079956, 0.896211564540863, 0.9531936645507812, 0.9451284408569336, 0.93964684009552, 0.928267240524292, 0.972279965877533, 0.9312055706977844, 0.961368203163147, 0.9477070569992065, 0.9175825715065002, 0.9356042146682739, 0.9213860034942627, 0.968755304813385, 0.9158836603164673, 0.9126203656196594, 0.9721122980117798, 0.9868065118789673, 0.9138433337211609, 0.9355967044830322, 0.9530489444732666, 0.906126856803894, 0.9140569567680359, 0.9525479078292847, 0.9668389558792114, 0.9295271635055542, 0.9503673315048218, 0.953679084777832, 0.9364798069000244, 0.9507107734680176, 0.9406970143318176, 0.9574435353279114, 0.9500662088394165, 0.9516962766647339, 0.9272189140319824, 0.9306005835533142, 0.9427568912506104, 0.9532307386398315, 0.9204484820365906, 0.9422824382781982, 0.9562075138092041, 0.957714319229126, 0.9567545652389526, 0.9468702673912048, 0.928875207901001, 0.9574780464172363, 0.9524339437484741, 0.925106406211853, 0.9546018838882446, 0.952700138092041, 0.9504711627960205, 0.9589195251464844, 0.9481775164604187, 0.9655919075012207, 0.9374134540557861, 0.9336254596710205, 0.9607062935829163, 0.931919515132904, 0.9526568651199341, 0.9631277322769165, 0.9263377785682678, 0.9277515411376953, 0.9587205052375793, 0.9359617233276367, 0.8859164714813232, 0.9280296564102173, 0.910794198513031, 0.9540810585021973, 0.9679679870605469, 0.923474907875061, 0.9360578060150146, 0.9805395603179932, 0.9364789128303528, 0.9329838752746582, 0.9590152502059937, 0.9444714784622192, 0.9271979331970215, 0.9215712547302246, 0.9325743913650513, 0.9478581547737122, 0.9691162705421448, 0.938806414604187, 0.9221658706665039, 0.9391926527023315, 0.943834125995636, 0.9284437894821167, 0.944381833076477, 0.9529449939727783, 0.8719898462295532, 0.9355742931365967, 0.9173989295959473, 0.9197079539299011, 0.9350749254226685, 0.9563764929771423, 0.9602704048156738, 0.9392763376235962, 0.9526852369308472, 0.9333577752113342, 0.9383456110954285, 0.9335652589797974, 0.9536011219024658, 0.9141864776611328, 0.95181804895401, 0.9469105005264282, 0.9459336996078491, 0.8683885335922241, 0.9501243233680725, 0.9376009702682495, 0.9550480842590332, 0.9365389347076416, 0.906491756439209, 0.9522732496261597, 0.949367880821228, 0.9403972625732422, 0.9530660510063171, 1.0742390155792236, 0.9211761951446533, 0.9063264727592468, 0.941860020160675, 0.9188519716262817, 0.9066593647003174, 0.9057966470718384, 0.9383621215820312, 0.9230254888534546, 0.9679490327835083, 0.9617335200309753, 0.9628698229789734, 1.0755454301834106, 0.945320188999176, 0.9208588600158691, 0.932426929473877, 0.9451485276222229, 0.8842382431030273, 0.9326045513153076, 0.9630326628684998, 0.9317312836647034, 0.966896653175354, 0.9479493498802185, 0.9437253475189209, 0.9354708194732666, 0.9514645934104919, 0.9162259101867676, 0.9373738169670105, 0.9439281225204468, 0.9635229110717773, 0.9272004961967468, 0.9328478574752808, 0.9420357942581177, 0.920417308807373, 0.9155579805374146, 0.962401270866394, 0.9353358745574951, 0.920991063117981, 0.9470957517623901, 0.9527126550674438, 0.9509525299072266, 0.9783000946044922, 0.947020411491394, 0.921451210975647, 0.9458837509155273, 0.9102495908737183, 0.9301155805587769, 0.9331580400466919, 0.9661903381347656, 0.8930985927581787, 0.965333104133606, 0.971877932548523, 0.934747040271759, 0.9624190926551819, 0.8920464515686035, 0.9413443803787231, 0.9357588291168213, 0.9441542625427246, 0.9282933473587036, 0.9448788166046143, 0.9406416416168213, 0.954064667224884, 0.9367719888687134, 0.9566624760627747, 0.940729022026062, 0.9735286235809326, 0.9274497032165527, 0.920307457447052, 0.9554729461669922, 1.0027775764465332, 0.9619182348251343, 0.9544546604156494, 0.9277529716491699, 0.9570224285125732, 0.925136148929596, 0.937628448009491, 0.9399239420890808, 0.9496972560882568, 0.9529428482055664, 0.9414535164833069, 0.9349448680877686, 0.9354621171951294, 0.9402089715003967, 0.9694405198097229, 0.9501857161521912, 0.9281027913093567, 0.9550856351852417, 0.9389517903327942, 0.9397735595703125, 0.9393664598464966, 0.926900327205658, 0.9456353783607483, 0.9621315598487854, 0.9290549159049988, 0.9488481283187866, 0.9554895162582397, 0.9555531144142151, 0.9515160322189331, 0.933053731918335, 0.9629843831062317, 0.9467542767524719, 0.9354621171951294, 0.9432156085968018, 0.9417146444320679, 0.9472978115081787, 0.9364198446273804, 0.9288039803504944, 0.9402639865875244, 0.9211745858192444, 0.8666889071464539, 0.9568403959274292, 0.9384052753448486, 0.9194788932800293, 0.9594143033027649, 0.957428514957428, 0.9357872009277344, 0.9208376407623291, 0.9359162449836731, 0.9525975584983826, 0.9353530406951904, 0.9570043683052063, 0.9529983997344971, 0.9304080605506897, 0.9202067852020264, 0.9156090617179871, 0.9093761444091797, 0.9479579925537109, 0.9507708549499512, 0.9475580453872681, 1.0918543338775635, 0.9467810392379761, 0.9574339389801025, 0.9200160503387451, 0.935046374797821, 0.9191189408302307, 0.9304398894309998, 0.9393737316131592, 0.962234616279602, 0.9496474266052246, 0.9487869143486023, 0.957343339920044, 0.9729754328727722, 0.9426718354225159, 0.9579115509986877, 0.9606336951255798, 0.9615696668624878, 0.8984329104423523, 0.9586257338523865, 0.9550480842590332, 0.9463471174240112, 0.9444795846939087, 0.9525068998336792, 0.9665744304656982, 0.967772901058197, 0.9454192519187927, 0.9444031715393066, 0.9638856649398804, 0.9285885691642761, 0.9436264038085938, 0.923683226108551, 0.9634369611740112, 0.9571100473403931, 0.9530445337295532, 0.9736135005950928, 0.9200305938720703, 0.9430102109909058, 0.9651057124137878, 0.9268577098846436, 0.9289384484291077, 0.940819501876831, 0.9280564188957214, 0.9571658372879028, 0.9689891338348389, 0.9065269827842712, 0.9331911206245422, 0.9467962980270386, 0.9498534798622131, 0.9120162129402161, 0.9479437470436096, 0.9478040933609009, 0.9356578588485718, 0.9530080556869507, 0.9555385112762451, 0.9082938432693481, 0.47440552711486816, 0.9407442212104797, 0.9436241388320923, 0.9525861740112305, 0.9461551904678345, 0.9623266458511353, 0.9556667804718018, 0.9145311117172241, 0.9611239433288574, 0.942070722579956, 0.9406601190567017, 0.9080837965011597, 0.950947642326355, 0.9538980722427368, 0.9568543434143066, 0.9407091736793518, 0.947846531867981, 0.943440318107605, 0.9635180830955505, 0.9553495645523071, 0.9193105697631836, 0.9731571674346924, 0.9558994174003601, 0.9355742931365967, 0.9247918128967285, 0.9478371143341064, 0.9382508993148804, 0.8989154100418091, 0.9425870180130005, 0.9357221126556396, 0.9336668252944946, 0.9513688683509827, 0.9139416217803955, 0.9488643407821655, 0.9545091986656189, 0.9709917306900024, 0.9633316993713379, 0.9217458367347717, 0.905752420425415, 0.9375678300857544, 0.9108123183250427, 0.9485695362091064, 0.934016227722168, 0.9354057908058167, 0.9167681932449341, 0.9079427719116211, 0.9516419172286987, 0.9744935035705566, 0.9436172246932983, 0.9279336333274841, 0.9722980260848999, 0.9514418840408325, 0.9178896546363831, 0.9297939538955688, 1.0008076429367065, 0.9387139081954956, 0.9222646951675415, 0.9794539213180542, 0.952468752861023, 0.9710418581962585, 0.9520147442817688, 0.9158713817596436, 0.9706352353096008, 0.9122297763824463, 0.9435413479804993, 0.9510787725448608, 0.9060586094856262, 0.9577839374542236, 0.9543318152427673, 0.9566413760185242, 0.9570128917694092, 0.9487948417663574, 0.9486749172210693, 0.9370573163032532, 0.9385378956794739, 0.9115654826164246, 0.9569250345230103, 0.9361491203308105, 0.9334242343902588, 0.9343977570533752, 0.9446263313293457, 0.9663776159286499, 0.9512887001037598, 0.9298843145370483, 0.9470913410186768, 0.957017183303833, 0.9400854110717773, 0.9410123825073242, 0.9530260562896729, 0.9230716228485107, 0.9369640946388245, 0.8876177072525024, 0.9348350763320923, 0.9401469230651855, 0.9314133524894714, 0.9353069067001343, 0.935434103012085, 0.9457671642303467, 0.9403609037399292, 0.9505311250686646, 0.9200474619865417, 0.9428650736808777, 0.9304140210151672, 0.9481770992279053, 0.9437364935874939, 0.9149156808853149, 1.0040233135223389, 0.9613732695579529, 0.9577121734619141, 0.9450430870056152, 0.9786731600761414, 0.9543684720993042, 0.9687516093254089, 0.9362896680831909, 1.1091346740722656, 0.9332125186920166, 0.9353785514831543, 0.9533427357673645, 0.9948475956916809, 0.9313477277755737, 0.9234632253646851, 0.9354708194732666, 0.9111765623092651, 0.9437106251716614, 0.9336490631103516, 0.9143140912055969, 0.9611524343490601, 0.943169355392456, 0.9391494393348694, 0.9499278664588928, 0.9564395546913147, 0.948498010635376, 0.9159889221191406, 0.9635101556777954, 0.9232432246208191, 0.93330979347229, 0.9125463962554932, 0.9646499156951904, 0.9288434982299805, 0.9366925954818726, 0.9671425223350525, 0.9348165988922119, 0.9320701360702515, 0.9379810094833374, 0.9096900224685669, 0.9517240524291992, 0.9278810024261475, 0.9218635559082031, 0.940434455871582, 0.9668845534324646, 0.961988091468811, 0.9272199869155884, 0.9500293731689453, 0.9468590617179871, 0.9409766793251038, 0.9354285001754761, 0.9347349405288696, 0.952738881111145, 0.9587004780769348, 0.9407575130462646, 0.9553651809692383, 0.9221483469009399, 0.9301445484161377, 0.9479556083679199, 0.9314500093460083, 0.9079216718673706, 0.9560801982879639, 0.9311280250549316, 0.9386328458786011, 0.9320104122161865, 0.9519721269607544, 0.9305885434150696, 0.9414569735527039, 0.918572723865509, 0.947472095489502, 0.934914767742157, 0.9639708995819092, 0.9206830859184265, 0.9433164596557617, 0.9279858469963074, 0.9385430812835693, 0.9123798608779907, 0.9309152960777283, 0.9313545823097229, 0.9362515211105347, 0.9412830471992493, 0.8787806630134583, 0.9750111103057861, 0.9466414451599121, 0.9409769177436829, 0.9537414312362671, 0.9469391107559204, 0.9453303813934326, 0.9447997808456421, 0.954931914806366, 0.9485460519790649, 0.9451635479927063, 0.9297223687171936, 0.9480714201927185, 0.9523306488990784, 0.9676498174667358, 0.9520761966705322, 0.9843200445175171, 0.9203425645828247, 0.9337015151977539, 0.8852653503417969, 0.8808525800704956, 0.9396281838417053, 0.8965820074081421, 0.9392257928848267, 0.9665663242340088, 0.93120938539505, 0.9474160075187683, 0.9470676183700562, 0.9320505857467651, 0.9200770854949951, 0.9640318751335144, 0.9379409551620483, 0.902338445186615, 0.9523733854293823, 0.9395952820777893, 0.9199503064155579, 0.8835179805755615, 0.9655934572219849, 0.9766870737075806, 0.9304150938987732, 0.9394888281822205, 0.9454992413520813, 0.9372783899307251, 0.9432239532470703, 0.9624762535095215, 0.9721732139587402, 0.9200395941734314, 1.0424787998199463, 0.9418026804924011, 0.9350889921188354, 0.9662482738494873, 0.9580286741256714, 0.9542323350906372, 0.9576709270477295, 0.8975610136985779, 0.9319790005683899, 0.946330189704895, 0.9889805316925049, 0.9483319520950317, 0.9651342630386353, 1.1090409755706787, 0.9169425964355469, 0.9416782855987549, 0.9566352963447571, 0.9415749907493591, 0.9291583299636841, 0.9171778559684753, 0.9207494258880615, 0.9275074005126953, 0.9330139756202698, 0.9290469884872437, 0.946791410446167, 0.9291340708732605, 0.9694885015487671, 0.9358505010604858, 0.8790282607078552, 0.9509707093238831, 0.9415120482444763]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0S6tmLExueeH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594124428377,"user_tz":-60,"elapsed":44502,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"4cd19c3c-c80c-4fff-9b13-7d125478ffc3"},"source":["# CNN for regression task\n","fix_seed()\n","\n","EPOCHS = 500\n","LRATE = 5e-3\n","\n","EMBEDDING_DIM = 50\n","FC_OUT_DIM = 25\n","\n","# the hyperparameters specific to CNN\n","# we define the number of filters\n","N_OUT_CHANNELS = 100\n","\n","# we define the window size\n","WINDOW_SIZE = 3\n","\n","# we apply the dropout with the probability 0.2\n","DROPOUT = 0.7\n","\n","# Construct the model\n","model = TwoInputsCNN(len(word2idx), EMBEDDING_DIM, N_OUT_CHANNELS, WINDOW_SIZE, FC_OUT_DIM, DROPOUT)\n","\n","# Print the model\n","print(model)\n","\n","model = model.to(DEVICE)\n","\n","# we use the stochastic gradient descent (SGD) optimizer\n","#optimizer = optim.SGD(model.parameters(), lr=LRATE)\n","\n","optimizer = optim.Adam(model.parameters(), lr=LRATE)\n","\n","# schedule learning rate using scheduler\n","steps = 150\n","scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, steps)\n","\n","# Input and label tensors for training\n","x_feature = origin_tensor.to(DEVICE)\n","y_feature = new_tensor.to(DEVICE)\n","target = label_tensor.to(DEVICE)\n","\n","# Input and label tensors for validation\n","valid_x_feature = valid_origin_tensor.to(DEVICE)\n","valid_y_feature = valid_new_tensor.to(DEVICE)\n","valid_target = valid_label_tensor.to(DEVICE)\n","\n","\n","################\n","# Start training\n","################\n","print(f'Will train for {EPOCHS} epochs')\n","for epoch in range(1, EPOCHS + 1):\n","  model.train()\n","  \n","  optimizer.zero_grad()\n","  \n","  # squeeze is needed as the predictions will have the shape (batch size, 1)\n","  # and we need to remove the dimension of size 1\n","  predictions = model(x_feature, y_feature).squeeze(1)\n","\n","  # Compute here the RMSE loss\n","  loss = torch.sqrt(((predictions - target)**2).mean())\n","  train_loss = loss.item()\n","\n","  # calculate the gradient of each parameter\n","  loss.backward()\n","\n","  # update the parameters using the gradients and optimizer algorithm \n","  optimizer.step()\n","  \n","  # update the learning rate\n","  scheduler.step()\n","\n","  # \"evaluation mode\" (turns off dropout and batch normalization)\n","  model.eval()\n","\n","  # we do not compute gradients within this block, i.e. no training\n","  with torch.no_grad():\n","    valid_predictions = model(valid_x_feature, valid_y_feature).squeeze(1)\n","    valid_loss = torch.sqrt(((valid_predictions - valid_target)**2).mean()).item()\n","  \n","  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.6f} | Val. Loss: {valid_loss:.6f} |')"],"execution_count":185,"outputs":[{"output_type":"stream","text":["TwoInputsCNN(\n","  (embedding): Embedding(11722, 50, padding_idx=0)\n","  (conv): Conv2d(1, 100, kernel_size=(3, 50), stride=(1, 1))\n","  (dropout): Dropout(p=0.7, inplace=False)\n","  (fc): Linear(in_features=100, out_features=25, bias=True)\n",")\n","Will train for 500 epochs\n","| Epoch: 01 | Train Loss: 98.611549 | Val. Loss: 59.526054 |\n","| Epoch: 02 | Train Loss: 66.814018 | Val. Loss: 39.622589 |\n","| Epoch: 03 | Train Loss: 45.480709 | Val. Loss: 25.952444 |\n","| Epoch: 04 | Train Loss: 30.376968 | Val. Loss: 16.712650 |\n","| Epoch: 05 | Train Loss: 20.173019 | Val. Loss: 10.630790 |\n","| Epoch: 06 | Train Loss: 12.964316 | Val. Loss: 6.785033 |\n","| Epoch: 07 | Train Loss: 8.638209 | Val. Loss: 4.454254 |\n","| Epoch: 08 | Train Loss: 5.851375 | Val. Loss: 3.113842 |\n","| Epoch: 09 | Train Loss: 4.222412 | Val. Loss: 2.395776 |\n","| Epoch: 10 | Train Loss: 3.270630 | Val. Loss: 2.041988 |\n","| Epoch: 11 | Train Loss: 2.837114 | Val. Loss: 1.889567 |\n","| Epoch: 12 | Train Loss: 2.606101 | Val. Loss: 1.839073 |\n","| Epoch: 13 | Train Loss: 2.554130 | Val. Loss: 1.821296 |\n","| Epoch: 14 | Train Loss: 2.515826 | Val. Loss: 1.797636 |\n","| Epoch: 15 | Train Loss: 2.469103 | Val. Loss: 1.744477 |\n","| Epoch: 16 | Train Loss: 2.444552 | Val. Loss: 1.654671 |\n","| Epoch: 17 | Train Loss: 2.340813 | Val. Loss: 1.537191 |\n","| Epoch: 18 | Train Loss: 2.159425 | Val. Loss: 1.404118 |\n","| Epoch: 19 | Train Loss: 1.939237 | Val. Loss: 1.271624 |\n","| Epoch: 20 | Train Loss: 1.786121 | Val. Loss: 1.151403 |\n","| Epoch: 21 | Train Loss: 1.565569 | Val. Loss: 1.053146 |\n","| Epoch: 22 | Train Loss: 1.433932 | Val. Loss: 0.980237 |\n","| Epoch: 23 | Train Loss: 1.303120 | Val. Loss: 0.932634 |\n","| Epoch: 24 | Train Loss: 1.194159 | Val. Loss: 0.905630 |\n","| Epoch: 25 | Train Loss: 1.079831 | Val. Loss: 0.893375 |\n","| Epoch: 26 | Train Loss: 1.026017 | Val. Loss: 0.889961 |\n","| Epoch: 27 | Train Loss: 0.993379 | Val. Loss: 0.891097 |\n","| Epoch: 28 | Train Loss: 0.959888 | Val. Loss: 0.894020 |\n","| Epoch: 29 | Train Loss: 0.955451 | Val. Loss: 0.897051 |\n","| Epoch: 30 | Train Loss: 0.940385 | Val. Loss: 0.899143 |\n","| Epoch: 31 | Train Loss: 0.937721 | Val. Loss: 0.900006 |\n","| Epoch: 32 | Train Loss: 0.940951 | Val. Loss: 0.899448 |\n","| Epoch: 33 | Train Loss: 0.937143 | Val. Loss: 0.897367 |\n","| Epoch: 34 | Train Loss: 0.923435 | Val. Loss: 0.893664 |\n","| Epoch: 35 | Train Loss: 0.924020 | Val. Loss: 0.888576 |\n","| Epoch: 36 | Train Loss: 0.920014 | Val. Loss: 0.882136 |\n","| Epoch: 37 | Train Loss: 0.907610 | Val. Loss: 0.874201 |\n","| Epoch: 38 | Train Loss: 0.901772 | Val. Loss: 0.864797 |\n","| Epoch: 39 | Train Loss: 0.903818 | Val. Loss: 0.854698 |\n","| Epoch: 40 | Train Loss: 0.893986 | Val. Loss: 0.844166 |\n","| Epoch: 41 | Train Loss: 0.894478 | Val. Loss: 0.833961 |\n","| Epoch: 42 | Train Loss: 0.885564 | Val. Loss: 0.824604 |\n","| Epoch: 43 | Train Loss: 0.887096 | Val. Loss: 0.816233 |\n","| Epoch: 44 | Train Loss: 0.888197 | Val. Loss: 0.810019 |\n","| Epoch: 45 | Train Loss: 0.874542 | Val. Loss: 0.805542 |\n","| Epoch: 46 | Train Loss: 0.882801 | Val. Loss: 0.803061 |\n","| Epoch: 47 | Train Loss: 0.867120 | Val. Loss: 0.801648 |\n","| Epoch: 48 | Train Loss: 0.862277 | Val. Loss: 0.801155 |\n","| Epoch: 49 | Train Loss: 0.866415 | Val. Loss: 0.800951 |\n","| Epoch: 50 | Train Loss: 0.852263 | Val. Loss: 0.800776 |\n","| Epoch: 51 | Train Loss: 0.851411 | Val. Loss: 0.800458 |\n","| Epoch: 52 | Train Loss: 0.846931 | Val. Loss: 0.799303 |\n","| Epoch: 53 | Train Loss: 0.838832 | Val. Loss: 0.797773 |\n","| Epoch: 54 | Train Loss: 0.838385 | Val. Loss: 0.794988 |\n","| Epoch: 55 | Train Loss: 0.840841 | Val. Loss: 0.791530 |\n","| Epoch: 56 | Train Loss: 0.839222 | Val. Loss: 0.787771 |\n","| Epoch: 57 | Train Loss: 0.827601 | Val. Loss: 0.783671 |\n","| Epoch: 58 | Train Loss: 0.831097 | Val. Loss: 0.779444 |\n","| Epoch: 59 | Train Loss: 0.824869 | Val. Loss: 0.775450 |\n","| Epoch: 60 | Train Loss: 0.819584 | Val. Loss: 0.771297 |\n","| Epoch: 61 | Train Loss: 0.808727 | Val. Loss: 0.766684 |\n","| Epoch: 62 | Train Loss: 0.816633 | Val. Loss: 0.762625 |\n","| Epoch: 63 | Train Loss: 0.813437 | Val. Loss: 0.759622 |\n","| Epoch: 64 | Train Loss: 0.812286 | Val. Loss: 0.757879 |\n","| Epoch: 65 | Train Loss: 0.811293 | Val. Loss: 0.757303 |\n","| Epoch: 66 | Train Loss: 0.806857 | Val. Loss: 0.757325 |\n","| Epoch: 67 | Train Loss: 0.806488 | Val. Loss: 0.757454 |\n","| Epoch: 68 | Train Loss: 0.806998 | Val. Loss: 0.757648 |\n","| Epoch: 69 | Train Loss: 0.795131 | Val. Loss: 0.757708 |\n","| Epoch: 70 | Train Loss: 0.795385 | Val. Loss: 0.757357 |\n","| Epoch: 71 | Train Loss: 0.788191 | Val. Loss: 0.756422 |\n","| Epoch: 72 | Train Loss: 0.799825 | Val. Loss: 0.755239 |\n","| Epoch: 73 | Train Loss: 0.786304 | Val. Loss: 0.753303 |\n","| Epoch: 74 | Train Loss: 0.794827 | Val. Loss: 0.751141 |\n","| Epoch: 75 | Train Loss: 0.787455 | Val. Loss: 0.748595 |\n","| Epoch: 76 | Train Loss: 0.789559 | Val. Loss: 0.746399 |\n","| Epoch: 77 | Train Loss: 0.777005 | Val. Loss: 0.744029 |\n","| Epoch: 78 | Train Loss: 0.774360 | Val. Loss: 0.741352 |\n","| Epoch: 79 | Train Loss: 0.793166 | Val. Loss: 0.739248 |\n","| Epoch: 80 | Train Loss: 0.777459 | Val. Loss: 0.737770 |\n","| Epoch: 81 | Train Loss: 0.784164 | Val. Loss: 0.737191 |\n","| Epoch: 82 | Train Loss: 0.783560 | Val. Loss: 0.736838 |\n","| Epoch: 83 | Train Loss: 0.776890 | Val. Loss: 0.736278 |\n","| Epoch: 84 | Train Loss: 0.771508 | Val. Loss: 0.735535 |\n","| Epoch: 85 | Train Loss: 0.773812 | Val. Loss: 0.734957 |\n","| Epoch: 86 | Train Loss: 0.773324 | Val. Loss: 0.734511 |\n","| Epoch: 87 | Train Loss: 0.767545 | Val. Loss: 0.733799 |\n","| Epoch: 88 | Train Loss: 0.771360 | Val. Loss: 0.733089 |\n","| Epoch: 89 | Train Loss: 0.763813 | Val. Loss: 0.732421 |\n","| Epoch: 90 | Train Loss: 0.764763 | Val. Loss: 0.731747 |\n","| Epoch: 91 | Train Loss: 0.766191 | Val. Loss: 0.730962 |\n","| Epoch: 92 | Train Loss: 0.764784 | Val. Loss: 0.729712 |\n","| Epoch: 93 | Train Loss: 0.765212 | Val. Loss: 0.729010 |\n","| Epoch: 94 | Train Loss: 0.762395 | Val. Loss: 0.728494 |\n","| Epoch: 95 | Train Loss: 0.759606 | Val. Loss: 0.728378 |\n","| Epoch: 96 | Train Loss: 0.764177 | Val. Loss: 0.728305 |\n","| Epoch: 97 | Train Loss: 0.764589 | Val. Loss: 0.728282 |\n","| Epoch: 98 | Train Loss: 0.757629 | Val. Loss: 0.728122 |\n","| Epoch: 99 | Train Loss: 0.763496 | Val. Loss: 0.727809 |\n","| Epoch: 100 | Train Loss: 0.759025 | Val. Loss: 0.727155 |\n","| Epoch: 101 | Train Loss: 0.756910 | Val. Loss: 0.726046 |\n","| Epoch: 102 | Train Loss: 0.761693 | Val. Loss: 0.725067 |\n","| Epoch: 103 | Train Loss: 0.748961 | Val. Loss: 0.723957 |\n","| Epoch: 104 | Train Loss: 0.757293 | Val. Loss: 0.722941 |\n","| Epoch: 105 | Train Loss: 0.756000 | Val. Loss: 0.722118 |\n","| Epoch: 106 | Train Loss: 0.752014 | Val. Loss: 0.721412 |\n","| Epoch: 107 | Train Loss: 0.756927 | Val. Loss: 0.720980 |\n","| Epoch: 108 | Train Loss: 0.750111 | Val. Loss: 0.720774 |\n","| Epoch: 109 | Train Loss: 0.744412 | Val. Loss: 0.720782 |\n","| Epoch: 110 | Train Loss: 0.753054 | Val. Loss: 0.720802 |\n","| Epoch: 111 | Train Loss: 0.759540 | Val. Loss: 0.720918 |\n","| Epoch: 112 | Train Loss: 0.753142 | Val. Loss: 0.721145 |\n","| Epoch: 113 | Train Loss: 0.747034 | Val. Loss: 0.721357 |\n","| Epoch: 114 | Train Loss: 0.755186 | Val. Loss: 0.721440 |\n","| Epoch: 115 | Train Loss: 0.753680 | Val. Loss: 0.721449 |\n","| Epoch: 116 | Train Loss: 0.749878 | Val. Loss: 0.721313 |\n","| Epoch: 117 | Train Loss: 0.746944 | Val. Loss: 0.721179 |\n","| Epoch: 118 | Train Loss: 0.751934 | Val. Loss: 0.720962 |\n","| Epoch: 119 | Train Loss: 0.747934 | Val. Loss: 0.720612 |\n","| Epoch: 120 | Train Loss: 0.755162 | Val. Loss: 0.720116 |\n","| Epoch: 121 | Train Loss: 0.739344 | Val. Loss: 0.719613 |\n","| Epoch: 122 | Train Loss: 0.754779 | Val. Loss: 0.719133 |\n","| Epoch: 123 | Train Loss: 0.750058 | Val. Loss: 0.718739 |\n","| Epoch: 124 | Train Loss: 0.748838 | Val. Loss: 0.718434 |\n","| Epoch: 125 | Train Loss: 0.741697 | Val. Loss: 0.718187 |\n","| Epoch: 126 | Train Loss: 0.749747 | Val. Loss: 0.717938 |\n","| Epoch: 127 | Train Loss: 0.746567 | Val. Loss: 0.717816 |\n","| Epoch: 128 | Train Loss: 0.750059 | Val. Loss: 0.717745 |\n","| Epoch: 129 | Train Loss: 0.745125 | Val. Loss: 0.717685 |\n","| Epoch: 130 | Train Loss: 0.745663 | Val. Loss: 0.717636 |\n","| Epoch: 131 | Train Loss: 0.751200 | Val. Loss: 0.717596 |\n","| Epoch: 132 | Train Loss: 0.751460 | Val. Loss: 0.717574 |\n","| Epoch: 133 | Train Loss: 0.744090 | Val. Loss: 0.717598 |\n","| Epoch: 134 | Train Loss: 0.747857 | Val. Loss: 0.717628 |\n","| Epoch: 135 | Train Loss: 0.747836 | Val. Loss: 0.717659 |\n","| Epoch: 136 | Train Loss: 0.744202 | Val. Loss: 0.717692 |\n","| Epoch: 137 | Train Loss: 0.741611 | Val. Loss: 0.717718 |\n","| Epoch: 138 | Train Loss: 0.743290 | Val. Loss: 0.717741 |\n","| Epoch: 139 | Train Loss: 0.740087 | Val. Loss: 0.717765 |\n","| Epoch: 140 | Train Loss: 0.739974 | Val. Loss: 0.717791 |\n","| Epoch: 141 | Train Loss: 0.749651 | Val. Loss: 0.717813 |\n","| Epoch: 142 | Train Loss: 0.748048 | Val. Loss: 0.717833 |\n","| Epoch: 143 | Train Loss: 0.752161 | Val. Loss: 0.717843 |\n","| Epoch: 144 | Train Loss: 0.744669 | Val. Loss: 0.717858 |\n","| Epoch: 145 | Train Loss: 0.740932 | Val. Loss: 0.717865 |\n","| Epoch: 146 | Train Loss: 0.740782 | Val. Loss: 0.717870 |\n","| Epoch: 147 | Train Loss: 0.738816 | Val. Loss: 0.717875 |\n","| Epoch: 148 | Train Loss: 0.745820 | Val. Loss: 0.717875 |\n","| Epoch: 149 | Train Loss: 0.744205 | Val. Loss: 0.717876 |\n","| Epoch: 150 | Train Loss: 0.738542 | Val. Loss: 0.717876 |\n","| Epoch: 151 | Train Loss: 0.742509 | Val. Loss: 0.717876 |\n","| Epoch: 152 | Train Loss: 0.744704 | Val. Loss: 0.717877 |\n","| Epoch: 153 | Train Loss: 0.742098 | Val. Loss: 0.717877 |\n","| Epoch: 154 | Train Loss: 0.741431 | Val. Loss: 0.717880 |\n","| Epoch: 155 | Train Loss: 0.739906 | Val. Loss: 0.717883 |\n","| Epoch: 156 | Train Loss: 0.751339 | Val. Loss: 0.717887 |\n","| Epoch: 157 | Train Loss: 0.746708 | Val. Loss: 0.717892 |\n","| Epoch: 158 | Train Loss: 0.747861 | Val. Loss: 0.717904 |\n","| Epoch: 159 | Train Loss: 0.749744 | Val. Loss: 0.717923 |\n","| Epoch: 160 | Train Loss: 0.746632 | Val. Loss: 0.717945 |\n","| Epoch: 161 | Train Loss: 0.742411 | Val. Loss: 0.717958 |\n","| Epoch: 162 | Train Loss: 0.739140 | Val. Loss: 0.717949 |\n","| Epoch: 163 | Train Loss: 0.740735 | Val. Loss: 0.717932 |\n","| Epoch: 164 | Train Loss: 0.752092 | Val. Loss: 0.717934 |\n","| Epoch: 165 | Train Loss: 0.751961 | Val. Loss: 0.717908 |\n","| Epoch: 166 | Train Loss: 0.745765 | Val. Loss: 0.717919 |\n","| Epoch: 167 | Train Loss: 0.737893 | Val. Loss: 0.717923 |\n","| Epoch: 168 | Train Loss: 0.743156 | Val. Loss: 0.717883 |\n","| Epoch: 169 | Train Loss: 0.745762 | Val. Loss: 0.717774 |\n","| Epoch: 170 | Train Loss: 0.742322 | Val. Loss: 0.717659 |\n","| Epoch: 171 | Train Loss: 0.748496 | Val. Loss: 0.717558 |\n","| Epoch: 172 | Train Loss: 0.744696 | Val. Loss: 0.717448 |\n","| Epoch: 173 | Train Loss: 0.745024 | Val. Loss: 0.717406 |\n","| Epoch: 174 | Train Loss: 0.748557 | Val. Loss: 0.717391 |\n","| Epoch: 175 | Train Loss: 0.747176 | Val. Loss: 0.717414 |\n","| Epoch: 176 | Train Loss: 0.746070 | Val. Loss: 0.717481 |\n","| Epoch: 177 | Train Loss: 0.741926 | Val. Loss: 0.717447 |\n","| Epoch: 178 | Train Loss: 0.739551 | Val. Loss: 0.717333 |\n","| Epoch: 179 | Train Loss: 0.745725 | Val. Loss: 0.717255 |\n","| Epoch: 180 | Train Loss: 0.750915 | Val. Loss: 0.717213 |\n","| Epoch: 181 | Train Loss: 0.741280 | Val. Loss: 0.717028 |\n","| Epoch: 182 | Train Loss: 0.746537 | Val. Loss: 0.716795 |\n","| Epoch: 183 | Train Loss: 0.736755 | Val. Loss: 0.716552 |\n","| Epoch: 184 | Train Loss: 0.741791 | Val. Loss: 0.716076 |\n","| Epoch: 185 | Train Loss: 0.744928 | Val. Loss: 0.715780 |\n","| Epoch: 186 | Train Loss: 0.737975 | Val. Loss: 0.715264 |\n","| Epoch: 187 | Train Loss: 0.738711 | Val. Loss: 0.714620 |\n","| Epoch: 188 | Train Loss: 0.728472 | Val. Loss: 0.713925 |\n","| Epoch: 189 | Train Loss: 0.741741 | Val. Loss: 0.713737 |\n","| Epoch: 190 | Train Loss: 0.745250 | Val. Loss: 0.713889 |\n","| Epoch: 191 | Train Loss: 0.735660 | Val. Loss: 0.714283 |\n","| Epoch: 192 | Train Loss: 0.736479 | Val. Loss: 0.714706 |\n","| Epoch: 193 | Train Loss: 0.735849 | Val. Loss: 0.714839 |\n","| Epoch: 194 | Train Loss: 0.738086 | Val. Loss: 0.714781 |\n","| Epoch: 195 | Train Loss: 0.743532 | Val. Loss: 0.714534 |\n","| Epoch: 196 | Train Loss: 0.728753 | Val. Loss: 0.713940 |\n","| Epoch: 197 | Train Loss: 0.736198 | Val. Loss: 0.713479 |\n","| Epoch: 198 | Train Loss: 0.734168 | Val. Loss: 0.712747 |\n","| Epoch: 199 | Train Loss: 0.733041 | Val. Loss: 0.711850 |\n","| Epoch: 200 | Train Loss: 0.727387 | Val. Loss: 0.710569 |\n","| Epoch: 201 | Train Loss: 0.732305 | Val. Loss: 0.709158 |\n","| Epoch: 202 | Train Loss: 0.737187 | Val. Loss: 0.708522 |\n","| Epoch: 203 | Train Loss: 0.728083 | Val. Loss: 0.708136 |\n","| Epoch: 204 | Train Loss: 0.732583 | Val. Loss: 0.707944 |\n","| Epoch: 205 | Train Loss: 0.731823 | Val. Loss: 0.707776 |\n","| Epoch: 206 | Train Loss: 0.728505 | Val. Loss: 0.707898 |\n","| Epoch: 207 | Train Loss: 0.730061 | Val. Loss: 0.708080 |\n","| Epoch: 208 | Train Loss: 0.732768 | Val. Loss: 0.708436 |\n","| Epoch: 209 | Train Loss: 0.731503 | Val. Loss: 0.708666 |\n","| Epoch: 210 | Train Loss: 0.724446 | Val. Loss: 0.707576 |\n","| Epoch: 211 | Train Loss: 0.728508 | Val. Loss: 0.705947 |\n","| Epoch: 212 | Train Loss: 0.730984 | Val. Loss: 0.704521 |\n","| Epoch: 213 | Train Loss: 0.726102 | Val. Loss: 0.703539 |\n","| Epoch: 214 | Train Loss: 0.726825 | Val. Loss: 0.703329 |\n","| Epoch: 215 | Train Loss: 0.731097 | Val. Loss: 0.703872 |\n","| Epoch: 216 | Train Loss: 0.727121 | Val. Loss: 0.703988 |\n","| Epoch: 217 | Train Loss: 0.716336 | Val. Loss: 0.703337 |\n","| Epoch: 218 | Train Loss: 0.729425 | Val. Loss: 0.703332 |\n","| Epoch: 219 | Train Loss: 0.722854 | Val. Loss: 0.703166 |\n","| Epoch: 220 | Train Loss: 0.714993 | Val. Loss: 0.702305 |\n","| Epoch: 221 | Train Loss: 0.719847 | Val. Loss: 0.700548 |\n","| Epoch: 222 | Train Loss: 0.720500 | Val. Loss: 0.698929 |\n","| Epoch: 223 | Train Loss: 0.720085 | Val. Loss: 0.698287 |\n","| Epoch: 224 | Train Loss: 0.721340 | Val. Loss: 0.697342 |\n","| Epoch: 225 | Train Loss: 0.717898 | Val. Loss: 0.695946 |\n","| Epoch: 226 | Train Loss: 0.721512 | Val. Loss: 0.696309 |\n","| Epoch: 227 | Train Loss: 0.723491 | Val. Loss: 0.698516 |\n","| Epoch: 228 | Train Loss: 0.713265 | Val. Loss: 0.700403 |\n","| Epoch: 229 | Train Loss: 0.717125 | Val. Loss: 0.700735 |\n","| Epoch: 230 | Train Loss: 0.714812 | Val. Loss: 0.699711 |\n","| Epoch: 231 | Train Loss: 0.721947 | Val. Loss: 0.698685 |\n","| Epoch: 232 | Train Loss: 0.715286 | Val. Loss: 0.697026 |\n","| Epoch: 233 | Train Loss: 0.716532 | Val. Loss: 0.695409 |\n","| Epoch: 234 | Train Loss: 0.716876 | Val. Loss: 0.694162 |\n","| Epoch: 235 | Train Loss: 0.713958 | Val. Loss: 0.694676 |\n","| Epoch: 236 | Train Loss: 0.715050 | Val. Loss: 0.694313 |\n","| Epoch: 237 | Train Loss: 0.703182 | Val. Loss: 0.693683 |\n","| Epoch: 238 | Train Loss: 0.716972 | Val. Loss: 0.693065 |\n","| Epoch: 239 | Train Loss: 0.716835 | Val. Loss: 0.693498 |\n","| Epoch: 240 | Train Loss: 0.704653 | Val. Loss: 0.694085 |\n","| Epoch: 241 | Train Loss: 0.714619 | Val. Loss: 0.694661 |\n","| Epoch: 242 | Train Loss: 0.715043 | Val. Loss: 0.693905 |\n","| Epoch: 243 | Train Loss: 0.706288 | Val. Loss: 0.691822 |\n","| Epoch: 244 | Train Loss: 0.715699 | Val. Loss: 0.689405 |\n","| Epoch: 245 | Train Loss: 0.702502 | Val. Loss: 0.685973 |\n","| Epoch: 246 | Train Loss: 0.708186 | Val. Loss: 0.685516 |\n","| Epoch: 247 | Train Loss: 0.712606 | Val. Loss: 0.687542 |\n","| Epoch: 248 | Train Loss: 0.710604 | Val. Loss: 0.689949 |\n","| Epoch: 249 | Train Loss: 0.702919 | Val. Loss: 0.692563 |\n","| Epoch: 250 | Train Loss: 0.711155 | Val. Loss: 0.692775 |\n","| Epoch: 251 | Train Loss: 0.699710 | Val. Loss: 0.689560 |\n","| Epoch: 252 | Train Loss: 0.707402 | Val. Loss: 0.685289 |\n","| Epoch: 253 | Train Loss: 0.700956 | Val. Loss: 0.682876 |\n","| Epoch: 254 | Train Loss: 0.699071 | Val. Loss: 0.680997 |\n","| Epoch: 255 | Train Loss: 0.711525 | Val. Loss: 0.684866 |\n","| Epoch: 256 | Train Loss: 0.705151 | Val. Loss: 0.687697 |\n","| Epoch: 257 | Train Loss: 0.702164 | Val. Loss: 0.687638 |\n","| Epoch: 258 | Train Loss: 0.698103 | Val. Loss: 0.684799 |\n","| Epoch: 259 | Train Loss: 0.704482 | Val. Loss: 0.683044 |\n","| Epoch: 260 | Train Loss: 0.704272 | Val. Loss: 0.681123 |\n","| Epoch: 261 | Train Loss: 0.698944 | Val. Loss: 0.682274 |\n","| Epoch: 262 | Train Loss: 0.696870 | Val. Loss: 0.685593 |\n","| Epoch: 263 | Train Loss: 0.700244 | Val. Loss: 0.687818 |\n","| Epoch: 264 | Train Loss: 0.693933 | Val. Loss: 0.686420 |\n","| Epoch: 265 | Train Loss: 0.701493 | Val. Loss: 0.681166 |\n","| Epoch: 266 | Train Loss: 0.701256 | Val. Loss: 0.678429 |\n","| Epoch: 267 | Train Loss: 0.701356 | Val. Loss: 0.678633 |\n","| Epoch: 268 | Train Loss: 0.697590 | Val. Loss: 0.682003 |\n","| Epoch: 269 | Train Loss: 0.697987 | Val. Loss: 0.683586 |\n","| Epoch: 270 | Train Loss: 0.699683 | Val. Loss: 0.680643 |\n","| Epoch: 271 | Train Loss: 0.691837 | Val. Loss: 0.674072 |\n","| Epoch: 272 | Train Loss: 0.697903 | Val. Loss: 0.671340 |\n","| Epoch: 273 | Train Loss: 0.692743 | Val. Loss: 0.672825 |\n","| Epoch: 274 | Train Loss: 0.688978 | Val. Loss: 0.674930 |\n","| Epoch: 275 | Train Loss: 0.708036 | Val. Loss: 0.681066 |\n","| Epoch: 276 | Train Loss: 0.701254 | Val. Loss: 0.684604 |\n","| Epoch: 277 | Train Loss: 0.693149 | Val. Loss: 0.682162 |\n","| Epoch: 278 | Train Loss: 0.694721 | Val. Loss: 0.676167 |\n","| Epoch: 279 | Train Loss: 0.685711 | Val. Loss: 0.671999 |\n","| Epoch: 280 | Train Loss: 0.701233 | Val. Loss: 0.675420 |\n","| Epoch: 281 | Train Loss: 0.690068 | Val. Loss: 0.678702 |\n","| Epoch: 282 | Train Loss: 0.695580 | Val. Loss: 0.679503 |\n","| Epoch: 283 | Train Loss: 0.694148 | Val. Loss: 0.678795 |\n","| Epoch: 284 | Train Loss: 0.700267 | Val. Loss: 0.678258 |\n","| Epoch: 285 | Train Loss: 0.690969 | Val. Loss: 0.677383 |\n","| Epoch: 286 | Train Loss: 0.685703 | Val. Loss: 0.678427 |\n","| Epoch: 287 | Train Loss: 0.696056 | Val. Loss: 0.679469 |\n","| Epoch: 288 | Train Loss: 0.691299 | Val. Loss: 0.676850 |\n","| Epoch: 289 | Train Loss: 0.685928 | Val. Loss: 0.674380 |\n","| Epoch: 290 | Train Loss: 0.696979 | Val. Loss: 0.677063 |\n","| Epoch: 291 | Train Loss: 0.686526 | Val. Loss: 0.677505 |\n","| Epoch: 292 | Train Loss: 0.694844 | Val. Loss: 0.677525 |\n","| Epoch: 293 | Train Loss: 0.698212 | Val. Loss: 0.676719 |\n","| Epoch: 294 | Train Loss: 0.690788 | Val. Loss: 0.676143 |\n","| Epoch: 295 | Train Loss: 0.685944 | Val. Loss: 0.677641 |\n","| Epoch: 296 | Train Loss: 0.681744 | Val. Loss: 0.674913 |\n","| Epoch: 297 | Train Loss: 0.689189 | Val. Loss: 0.674074 |\n","| Epoch: 298 | Train Loss: 0.686592 | Val. Loss: 0.675552 |\n","| Epoch: 299 | Train Loss: 0.696095 | Val. Loss: 0.680190 |\n","| Epoch: 300 | Train Loss: 0.694765 | Val. Loss: 0.681530 |\n","| Epoch: 301 | Train Loss: 0.684495 | Val. Loss: 0.674415 |\n","| Epoch: 302 | Train Loss: 0.690397 | Val. Loss: 0.668484 |\n","| Epoch: 303 | Train Loss: 0.687863 | Val. Loss: 0.672381 |\n","| Epoch: 304 | Train Loss: 0.696735 | Val. Loss: 0.678471 |\n","| Epoch: 305 | Train Loss: 0.684668 | Val. Loss: 0.680611 |\n","| Epoch: 306 | Train Loss: 0.680726 | Val. Loss: 0.680022 |\n","| Epoch: 307 | Train Loss: 0.686872 | Val. Loss: 0.678218 |\n","| Epoch: 308 | Train Loss: 0.696830 | Val. Loss: 0.676947 |\n","| Epoch: 309 | Train Loss: 0.688038 | Val. Loss: 0.675700 |\n","| Epoch: 310 | Train Loss: 0.687033 | Val. Loss: 0.676476 |\n","| Epoch: 311 | Train Loss: 0.680058 | Val. Loss: 0.676067 |\n","| Epoch: 312 | Train Loss: 0.687026 | Val. Loss: 0.676806 |\n","| Epoch: 313 | Train Loss: 0.692432 | Val. Loss: 0.679760 |\n","| Epoch: 314 | Train Loss: 0.688522 | Val. Loss: 0.678794 |\n","| Epoch: 315 | Train Loss: 0.678075 | Val. Loss: 0.676805 |\n","| Epoch: 316 | Train Loss: 0.682316 | Val. Loss: 0.671979 |\n","| Epoch: 317 | Train Loss: 0.685889 | Val. Loss: 0.671022 |\n","| Epoch: 318 | Train Loss: 0.676740 | Val. Loss: 0.675331 |\n","| Epoch: 319 | Train Loss: 0.678356 | Val. Loss: 0.677291 |\n","| Epoch: 320 | Train Loss: 0.677311 | Val. Loss: 0.677953 |\n","| Epoch: 321 | Train Loss: 0.682608 | Val. Loss: 0.679288 |\n","| Epoch: 322 | Train Loss: 0.681427 | Val. Loss: 0.677897 |\n","| Epoch: 323 | Train Loss: 0.675394 | Val. Loss: 0.677521 |\n","| Epoch: 324 | Train Loss: 0.683495 | Val. Loss: 0.679671 |\n","| Epoch: 325 | Train Loss: 0.672342 | Val. Loss: 0.682378 |\n","| Epoch: 326 | Train Loss: 0.675785 | Val. Loss: 0.682830 |\n","| Epoch: 327 | Train Loss: 0.684471 | Val. Loss: 0.682261 |\n","| Epoch: 328 | Train Loss: 0.676410 | Val. Loss: 0.678400 |\n","| Epoch: 329 | Train Loss: 0.679931 | Val. Loss: 0.676421 |\n","| Epoch: 330 | Train Loss: 0.674897 | Val. Loss: 0.676296 |\n","| Epoch: 331 | Train Loss: 0.671402 | Val. Loss: 0.679282 |\n","| Epoch: 332 | Train Loss: 0.669390 | Val. Loss: 0.682068 |\n","| Epoch: 333 | Train Loss: 0.680073 | Val. Loss: 0.682882 |\n","| Epoch: 334 | Train Loss: 0.672287 | Val. Loss: 0.680158 |\n","| Epoch: 335 | Train Loss: 0.668286 | Val. Loss: 0.676061 |\n","| Epoch: 336 | Train Loss: 0.675387 | Val. Loss: 0.676769 |\n","| Epoch: 337 | Train Loss: 0.663951 | Val. Loss: 0.677812 |\n","| Epoch: 338 | Train Loss: 0.672319 | Val. Loss: 0.677715 |\n","| Epoch: 339 | Train Loss: 0.672784 | Val. Loss: 0.677633 |\n","| Epoch: 340 | Train Loss: 0.667894 | Val. Loss: 0.674745 |\n","| Epoch: 341 | Train Loss: 0.675686 | Val. Loss: 0.671539 |\n","| Epoch: 342 | Train Loss: 0.666974 | Val. Loss: 0.670735 |\n","| Epoch: 343 | Train Loss: 0.668251 | Val. Loss: 0.672896 |\n","| Epoch: 344 | Train Loss: 0.669390 | Val. Loss: 0.677450 |\n","| Epoch: 345 | Train Loss: 0.672563 | Val. Loss: 0.679421 |\n","| Epoch: 346 | Train Loss: 0.668591 | Val. Loss: 0.676160 |\n","| Epoch: 347 | Train Loss: 0.666172 | Val. Loss: 0.671595 |\n","| Epoch: 348 | Train Loss: 0.669040 | Val. Loss: 0.670901 |\n","| Epoch: 349 | Train Loss: 0.658307 | Val. Loss: 0.671648 |\n","| Epoch: 350 | Train Loss: 0.669540 | Val. Loss: 0.675297 |\n","| Epoch: 351 | Train Loss: 0.663816 | Val. Loss: 0.676314 |\n","| Epoch: 352 | Train Loss: 0.669887 | Val. Loss: 0.675371 |\n","| Epoch: 353 | Train Loss: 0.661707 | Val. Loss: 0.671664 |\n","| Epoch: 354 | Train Loss: 0.668874 | Val. Loss: 0.667213 |\n","| Epoch: 355 | Train Loss: 0.666853 | Val. Loss: 0.666614 |\n","| Epoch: 356 | Train Loss: 0.657230 | Val. Loss: 0.669855 |\n","| Epoch: 357 | Train Loss: 0.659666 | Val. Loss: 0.672994 |\n","| Epoch: 358 | Train Loss: 0.661268 | Val. Loss: 0.673110 |\n","| Epoch: 359 | Train Loss: 0.664622 | Val. Loss: 0.673315 |\n","| Epoch: 360 | Train Loss: 0.659616 | Val. Loss: 0.672195 |\n","| Epoch: 361 | Train Loss: 0.665260 | Val. Loss: 0.669711 |\n","| Epoch: 362 | Train Loss: 0.665286 | Val. Loss: 0.668974 |\n","| Epoch: 363 | Train Loss: 0.659851 | Val. Loss: 0.669805 |\n","| Epoch: 364 | Train Loss: 0.656259 | Val. Loss: 0.672681 |\n","| Epoch: 365 | Train Loss: 0.661229 | Val. Loss: 0.673779 |\n","| Epoch: 366 | Train Loss: 0.661502 | Val. Loss: 0.672999 |\n","| Epoch: 367 | Train Loss: 0.654179 | Val. Loss: 0.672322 |\n","| Epoch: 368 | Train Loss: 0.656643 | Val. Loss: 0.672906 |\n","| Epoch: 369 | Train Loss: 0.656400 | Val. Loss: 0.673376 |\n","| Epoch: 370 | Train Loss: 0.661686 | Val. Loss: 0.673066 |\n","| Epoch: 371 | Train Loss: 0.661653 | Val. Loss: 0.672004 |\n","| Epoch: 372 | Train Loss: 0.654274 | Val. Loss: 0.669703 |\n","| Epoch: 373 | Train Loss: 0.648032 | Val. Loss: 0.667141 |\n","| Epoch: 374 | Train Loss: 0.660896 | Val. Loss: 0.666658 |\n","| Epoch: 375 | Train Loss: 0.645582 | Val. Loss: 0.667533 |\n","| Epoch: 376 | Train Loss: 0.649618 | Val. Loss: 0.668411 |\n","| Epoch: 377 | Train Loss: 0.652509 | Val. Loss: 0.669246 |\n","| Epoch: 378 | Train Loss: 0.642687 | Val. Loss: 0.669330 |\n","| Epoch: 379 | Train Loss: 0.642769 | Val. Loss: 0.668618 |\n","| Epoch: 380 | Train Loss: 0.648509 | Val. Loss: 0.668317 |\n","| Epoch: 381 | Train Loss: 0.652427 | Val. Loss: 0.667888 |\n","| Epoch: 382 | Train Loss: 0.648599 | Val. Loss: 0.668036 |\n","| Epoch: 383 | Train Loss: 0.644849 | Val. Loss: 0.668504 |\n","| Epoch: 384 | Train Loss: 0.644559 | Val. Loss: 0.668525 |\n","| Epoch: 385 | Train Loss: 0.647355 | Val. Loss: 0.668364 |\n","| Epoch: 386 | Train Loss: 0.647977 | Val. Loss: 0.668867 |\n","| Epoch: 387 | Train Loss: 0.644421 | Val. Loss: 0.668841 |\n","| Epoch: 388 | Train Loss: 0.641896 | Val. Loss: 0.668186 |\n","| Epoch: 389 | Train Loss: 0.644228 | Val. Loss: 0.667187 |\n","| Epoch: 390 | Train Loss: 0.642733 | Val. Loss: 0.666725 |\n","| Epoch: 391 | Train Loss: 0.645491 | Val. Loss: 0.666697 |\n","| Epoch: 392 | Train Loss: 0.642219 | Val. Loss: 0.667820 |\n","| Epoch: 393 | Train Loss: 0.638084 | Val. Loss: 0.667507 |\n","| Epoch: 394 | Train Loss: 0.639834 | Val. Loss: 0.667069 |\n","| Epoch: 395 | Train Loss: 0.641245 | Val. Loss: 0.666461 |\n","| Epoch: 396 | Train Loss: 0.639048 | Val. Loss: 0.665804 |\n","| Epoch: 397 | Train Loss: 0.637132 | Val. Loss: 0.665237 |\n","| Epoch: 398 | Train Loss: 0.636228 | Val. Loss: 0.664785 |\n","| Epoch: 399 | Train Loss: 0.640520 | Val. Loss: 0.664340 |\n","| Epoch: 400 | Train Loss: 0.642861 | Val. Loss: 0.664410 |\n","| Epoch: 401 | Train Loss: 0.639601 | Val. Loss: 0.664698 |\n","| Epoch: 402 | Train Loss: 0.634210 | Val. Loss: 0.665035 |\n","| Epoch: 403 | Train Loss: 0.636169 | Val. Loss: 0.665231 |\n","| Epoch: 404 | Train Loss: 0.631864 | Val. Loss: 0.665254 |\n","| Epoch: 405 | Train Loss: 0.638178 | Val. Loss: 0.665120 |\n","| Epoch: 406 | Train Loss: 0.638270 | Val. Loss: 0.664596 |\n","| Epoch: 407 | Train Loss: 0.640653 | Val. Loss: 0.664053 |\n","| Epoch: 408 | Train Loss: 0.637645 | Val. Loss: 0.663619 |\n","| Epoch: 409 | Train Loss: 0.627793 | Val. Loss: 0.663469 |\n","| Epoch: 410 | Train Loss: 0.634336 | Val. Loss: 0.663870 |\n","| Epoch: 411 | Train Loss: 0.626952 | Val. Loss: 0.663989 |\n","| Epoch: 412 | Train Loss: 0.630488 | Val. Loss: 0.663781 |\n","| Epoch: 413 | Train Loss: 0.637181 | Val. Loss: 0.663781 |\n","| Epoch: 414 | Train Loss: 0.629636 | Val. Loss: 0.663849 |\n","| Epoch: 415 | Train Loss: 0.629810 | Val. Loss: 0.663729 |\n","| Epoch: 416 | Train Loss: 0.617546 | Val. Loss: 0.663410 |\n","| Epoch: 417 | Train Loss: 0.632188 | Val. Loss: 0.663089 |\n","| Epoch: 418 | Train Loss: 0.629647 | Val. Loss: 0.662715 |\n","| Epoch: 419 | Train Loss: 0.632899 | Val. Loss: 0.662447 |\n","| Epoch: 420 | Train Loss: 0.628007 | Val. Loss: 0.662114 |\n","| Epoch: 421 | Train Loss: 0.636079 | Val. Loss: 0.662067 |\n","| Epoch: 422 | Train Loss: 0.625499 | Val. Loss: 0.662231 |\n","| Epoch: 423 | Train Loss: 0.624146 | Val. Loss: 0.662279 |\n","| Epoch: 424 | Train Loss: 0.637124 | Val. Loss: 0.662292 |\n","| Epoch: 425 | Train Loss: 0.626968 | Val. Loss: 0.662284 |\n","| Epoch: 426 | Train Loss: 0.619231 | Val. Loss: 0.662200 |\n","| Epoch: 427 | Train Loss: 0.624271 | Val. Loss: 0.662216 |\n","| Epoch: 428 | Train Loss: 0.626476 | Val. Loss: 0.662282 |\n","| Epoch: 429 | Train Loss: 0.618762 | Val. Loss: 0.662263 |\n","| Epoch: 430 | Train Loss: 0.629437 | Val. Loss: 0.662221 |\n","| Epoch: 431 | Train Loss: 0.623973 | Val. Loss: 0.662172 |\n","| Epoch: 432 | Train Loss: 0.626887 | Val. Loss: 0.662100 |\n","| Epoch: 433 | Train Loss: 0.624362 | Val. Loss: 0.662043 |\n","| Epoch: 434 | Train Loss: 0.622866 | Val. Loss: 0.662070 |\n","| Epoch: 435 | Train Loss: 0.618480 | Val. Loss: 0.662081 |\n","| Epoch: 436 | Train Loss: 0.629786 | Val. Loss: 0.662089 |\n","| Epoch: 437 | Train Loss: 0.622485 | Val. Loss: 0.662106 |\n","| Epoch: 438 | Train Loss: 0.624842 | Val. Loss: 0.662129 |\n","| Epoch: 439 | Train Loss: 0.625728 | Val. Loss: 0.662129 |\n","| Epoch: 440 | Train Loss: 0.623774 | Val. Loss: 0.662120 |\n","| Epoch: 441 | Train Loss: 0.629826 | Val. Loss: 0.662131 |\n","| Epoch: 442 | Train Loss: 0.622607 | Val. Loss: 0.662138 |\n","| Epoch: 443 | Train Loss: 0.628198 | Val. Loss: 0.662155 |\n","| Epoch: 444 | Train Loss: 0.628358 | Val. Loss: 0.662164 |\n","| Epoch: 445 | Train Loss: 0.626841 | Val. Loss: 0.662171 |\n","| Epoch: 446 | Train Loss: 0.624813 | Val. Loss: 0.662179 |\n","| Epoch: 447 | Train Loss: 0.631604 | Val. Loss: 0.662184 |\n","| Epoch: 448 | Train Loss: 0.628247 | Val. Loss: 0.662187 |\n","| Epoch: 449 | Train Loss: 0.624804 | Val. Loss: 0.662188 |\n","| Epoch: 450 | Train Loss: 0.628703 | Val. Loss: 0.662188 |\n","| Epoch: 451 | Train Loss: 0.627252 | Val. Loss: 0.662188 |\n","| Epoch: 452 | Train Loss: 0.630292 | Val. Loss: 0.662188 |\n","| Epoch: 453 | Train Loss: 0.623692 | Val. Loss: 0.662189 |\n","| Epoch: 454 | Train Loss: 0.625705 | Val. Loss: 0.662190 |\n","| Epoch: 455 | Train Loss: 0.627888 | Val. Loss: 0.662189 |\n","| Epoch: 456 | Train Loss: 0.625107 | Val. Loss: 0.662189 |\n","| Epoch: 457 | Train Loss: 0.617446 | Val. Loss: 0.662181 |\n","| Epoch: 458 | Train Loss: 0.627362 | Val. Loss: 0.662165 |\n","| Epoch: 459 | Train Loss: 0.633258 | Val. Loss: 0.662141 |\n","| Epoch: 460 | Train Loss: 0.629418 | Val. Loss: 0.662108 |\n","| Epoch: 461 | Train Loss: 0.624606 | Val. Loss: 0.662050 |\n","| Epoch: 462 | Train Loss: 0.619363 | Val. Loss: 0.661959 |\n","| Epoch: 463 | Train Loss: 0.618565 | Val. Loss: 0.661868 |\n","| Epoch: 464 | Train Loss: 0.627475 | Val. Loss: 0.661786 |\n","| Epoch: 465 | Train Loss: 0.617841 | Val. Loss: 0.661666 |\n","| Epoch: 466 | Train Loss: 0.622685 | Val. Loss: 0.661546 |\n","| Epoch: 467 | Train Loss: 0.624507 | Val. Loss: 0.661440 |\n","| Epoch: 468 | Train Loss: 0.621603 | Val. Loss: 0.661347 |\n","| Epoch: 469 | Train Loss: 0.620270 | Val. Loss: 0.661241 |\n","| Epoch: 470 | Train Loss: 0.618380 | Val. Loss: 0.661220 |\n","| Epoch: 471 | Train Loss: 0.625915 | Val. Loss: 0.661273 |\n","| Epoch: 472 | Train Loss: 0.624083 | Val. Loss: 0.661221 |\n","| Epoch: 473 | Train Loss: 0.621778 | Val. Loss: 0.661171 |\n","| Epoch: 474 | Train Loss: 0.623535 | Val. Loss: 0.661135 |\n","| Epoch: 475 | Train Loss: 0.626890 | Val. Loss: 0.661108 |\n","| Epoch: 476 | Train Loss: 0.624699 | Val. Loss: 0.661185 |\n","| Epoch: 477 | Train Loss: 0.617164 | Val. Loss: 0.661177 |\n","| Epoch: 478 | Train Loss: 0.621912 | Val. Loss: 0.661125 |\n","| Epoch: 479 | Train Loss: 0.618288 | Val. Loss: 0.661057 |\n","| Epoch: 480 | Train Loss: 0.616352 | Val. Loss: 0.661048 |\n","| Epoch: 481 | Train Loss: 0.613850 | Val. Loss: 0.660940 |\n","| Epoch: 482 | Train Loss: 0.611492 | Val. Loss: 0.660702 |\n","| Epoch: 483 | Train Loss: 0.626302 | Val. Loss: 0.660588 |\n","| Epoch: 484 | Train Loss: 0.616577 | Val. Loss: 0.660447 |\n","| Epoch: 485 | Train Loss: 0.619968 | Val. Loss: 0.660298 |\n","| Epoch: 486 | Train Loss: 0.619363 | Val. Loss: 0.660218 |\n","| Epoch: 487 | Train Loss: 0.617122 | Val. Loss: 0.660500 |\n","| Epoch: 488 | Train Loss: 0.622460 | Val. Loss: 0.661529 |\n","| Epoch: 489 | Train Loss: 0.621336 | Val. Loss: 0.662539 |\n","| Epoch: 490 | Train Loss: 0.622230 | Val. Loss: 0.663306 |\n","| Epoch: 491 | Train Loss: 0.625666 | Val. Loss: 0.663790 |\n","| Epoch: 492 | Train Loss: 0.623623 | Val. Loss: 0.663681 |\n","| Epoch: 493 | Train Loss: 0.623754 | Val. Loss: 0.662702 |\n","| Epoch: 494 | Train Loss: 0.619801 | Val. Loss: 0.661327 |\n","| Epoch: 495 | Train Loss: 0.617433 | Val. Loss: 0.659909 |\n","| Epoch: 496 | Train Loss: 0.622778 | Val. Loss: 0.659065 |\n","| Epoch: 497 | Train Loss: 0.618847 | Val. Loss: 0.658923 |\n","| Epoch: 498 | Train Loss: 0.624151 | Val. Loss: 0.659612 |\n","| Epoch: 499 | Train Loss: 0.619259 | Val. Loss: 0.660601 |\n","| Epoch: 500 | Train Loss: 0.623988 | Val. Loss: 0.661932 |\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"epdlvHpkI3mX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"executionInfo":{"status":"ok","timestamp":1594123769138,"user_tz":-60,"elapsed":915,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"7c2a4818-bf88-4873-e7e2-502604fbb33c"},"source":["ebd = model.embedding.weight.data\n","print(ebd)"],"execution_count":182,"outputs":[{"output_type":"stream","text":["tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.8684, -0.4288,  0.9058,  ...,  1.3469, -1.8879, -0.2659],\n","        [-0.5214,  0.2853, -0.0157,  ...,  0.8558,  1.1096,  1.1314],\n","        ...,\n","        [ 0.6183,  1.8114, -0.0081,  ..., -0.1997,  0.6177,  0.4655],\n","        [ 1.8379,  0.6741, -0.6462,  ..., -2.3496,  0.6110, -0.5293],\n","        [ 1.2835,  0.9197, -1.3363,  ..., -0.4337,  0.7760, -0.2058]],\n","       device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pY8Ls24aUauh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593580609986,"user_tz":-60,"elapsed":12268,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"08aa70fb-7ebb-4b80-ac7d-f2a67d4e6270"},"source":["# FFNN for regression task\n","# Reset the seed before every model construction for reproducible results\n","fix_seed()\n","\n","# we will train for N epochs (The model will see the corpus N times)\n","EPOCHS = 200\n","\n","# Learning rate is initially set to 0.145\n","LRATE = 0.145\n","\n","# we define our embedding dimension (dimensionality of the output of the first layer)\n","EMBEDDING_DIM = 300\n","\n","# dimensionality of the output of the second hidden layer\n","HIDDEN_DIM_1 = 100\n","\n","# dimensionality of the output of the third hidden layer\n","HIDDEN_DIM_2 = 50\n","\n","# dimensionality of the output of the fourth hidden layer\n","HIDDEN_DIM_3 = 10\n","\n","# Construct the model\n","model = TwoInputsNN(EMBEDDING_DIM, HIDDEN_DIM_1, HIDDEN_DIM_2, HIDDEN_DIM_3, len(word2idx))\n","\n","# Print the model\n","print(model)\n","\n","model = model.to(DEVICE)\n","\n","# we use the stochastic gradient descent (SGD) optimizer\n","#optimizer = optim.SGD(model.parameters(), lr=LRATE)\n","\n","LRATE = 1e-1\n","optimizer = optim.Adam(model.parameters(), lr=LRATE)\n","\n","# schedule learning rate using scheduler\n","steps = 50\n","scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, steps)\n","\n","# Input and label tensors for training\n","x_feature = origin_tensor.to(DEVICE)\n","y_feature = new_tensor.to(DEVICE)\n","target = label_tensor.to(DEVICE)\n","\n","# Input and label tensors for validation\n","valid_x_feature = valid_origin_tensor.to(DEVICE)\n","valid_y_feature = valid_new_tensor.to(DEVICE)\n","valid_target = valid_label_tensor.to(DEVICE)\n","\n","\n","################\n","# Start training\n","################\n","print(f'Will train for {EPOCHS} epochs')\n","for epoch in range(1, EPOCHS + 1):\n","  # to ensure the dropout (explained later) is \"turned on\" while training\n","  # good practice to include even if do not use here\n","  model.train()\n","  \n","  # we zero the gradients as they are not removed automatically\n","  optimizer.zero_grad()\n"," \n","  # squeeze is needed as the predictions will have the shape (batch size, 1)\n","  # and we need to remove the dimension of size 1\n","  predictions = model(x_feature, y_feature).squeeze(1)\n","\n","  # Compute here the RMSE loss\n","  loss = torch.sqrt(((predictions - target)**2).mean())\n","  train_loss = loss.item()\n","\n","  # calculate the gradient of each parameter\n","  loss.backward()\n","\n","  # update the parameters using the gradients and optimizer algorithm \n","  optimizer.step()\n","  \n","  # update the learning rate\n","  scheduler.step()\n","\n","  # this puts the model in \"evaluation mode\" (turns off dropout and batch normalization)\n","  # good practise to include even if we do not use them right now\n","  model.eval()\n","\n","  # we do not compute gradients within this block, i.e. no training\n","  with torch.no_grad():\n","    valid_predictions = model(valid_x_feature, valid_y_feature).squeeze(1)\n","    valid_loss = torch.sqrt(((valid_predictions - valid_target)**2).mean()).item()\n","  \n","  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.6f} | Val. Loss: {valid_loss:.6f} |')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TwoInputsNN(\n","  (embedding): Embedding(11722, 300, padding_idx=0)\n","  (fc1): Linear(in_features=300, out_features=100, bias=True)\n","  (fc2): Linear(in_features=100, out_features=50, bias=True)\n","  (relu1): ReLU()\n","  (fc3): Linear(in_features=50, out_features=10, bias=True)\n",")\n","Will train for 200 epochs\n","| Epoch: 01 | Train Loss: 1.050 | Val. Loss: 22464.941 |\n","| Epoch: 02 | Train Loss: 18924.811 | Val. Loss: 9.176 |\n","| Epoch: 03 | Train Loss: 6.613 | Val. Loss: 1.084 |\n","| Epoch: 04 | Train Loss: 1.088 | Val. Loss: 1.086 |\n","| Epoch: 05 | Train Loss: 1.090 | Val. Loss: 1.058 |\n","| Epoch: 06 | Train Loss: 1.062 | Val. Loss: 1.014 |\n","| Epoch: 07 | Train Loss: 1.018 | Val. Loss: 0.962 |\n","| Epoch: 08 | Train Loss: 0.965 | Val. Loss: 0.906 |\n","| Epoch: 09 | Train Loss: 0.910 | Val. Loss: 0.850 |\n","| Epoch: 10 | Train Loss: 0.855 | Val. Loss: 0.798 |\n","| Epoch: 11 | Train Loss: 0.802 | Val. Loss: 0.750 |\n","| Epoch: 12 | Train Loss: 0.755 | Val. Loss: 0.708 |\n","| Epoch: 13 | Train Loss: 0.713 | Val. Loss: 0.672 |\n","| Epoch: 14 | Train Loss: 0.677 | Val. Loss: 0.643 |\n","| Epoch: 15 | Train Loss: 0.648 | Val. Loss: 0.620 |\n","| Epoch: 16 | Train Loss: 0.625 | Val. Loss: 0.602 |\n","| Epoch: 17 | Train Loss: 0.608 | Val. Loss: 0.591 |\n","| Epoch: 18 | Train Loss: 0.596 | Val. Loss: 0.583 |\n","| Epoch: 19 | Train Loss: 0.588 | Val. Loss: 0.579 |\n","| Epoch: 20 | Train Loss: 0.585 | Val. Loss: 0.578 |\n","| Epoch: 21 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 22 | Train Loss: 0.585 | Val. Loss: 0.582 |\n","| Epoch: 23 | Train Loss: 0.587 | Val. Loss: 0.585 |\n","| Epoch: 24 | Train Loss: 0.590 | Val. Loss: 0.588 |\n","| Epoch: 25 | Train Loss: 0.593 | Val. Loss: 0.592 |\n","| Epoch: 26 | Train Loss: 0.597 | Val. Loss: 0.595 |\n","| Epoch: 27 | Train Loss: 0.600 | Val. Loss: 0.598 |\n","| Epoch: 28 | Train Loss: 0.603 | Val. Loss: 0.601 |\n","| Epoch: 29 | Train Loss: 0.606 | Val. Loss: 0.603 |\n","| Epoch: 30 | Train Loss: 0.608 | Val. Loss: 0.605 |\n","| Epoch: 31 | Train Loss: 0.610 | Val. Loss: 0.607 |\n","| Epoch: 32 | Train Loss: 0.612 | Val. Loss: 0.608 |\n","| Epoch: 33 | Train Loss: 0.613 | Val. Loss: 0.609 |\n","| Epoch: 34 | Train Loss: 0.614 | Val. Loss: 0.610 |\n","| Epoch: 35 | Train Loss: 0.615 | Val. Loss: 0.610 |\n","| Epoch: 36 | Train Loss: 0.615 | Val. Loss: 0.611 |\n","| Epoch: 37 | Train Loss: 0.615 | Val. Loss: 0.611 |\n","| Epoch: 38 | Train Loss: 0.616 | Val. Loss: 0.611 |\n","| Epoch: 39 | Train Loss: 0.616 | Val. Loss: 0.611 |\n","| Epoch: 40 | Train Loss: 0.616 | Val. Loss: 0.611 |\n","| Epoch: 41 | Train Loss: 0.616 | Val. Loss: 0.611 |\n","| Epoch: 42 | Train Loss: 0.615 | Val. Loss: 0.611 |\n","| Epoch: 43 | Train Loss: 0.615 | Val. Loss: 0.611 |\n","| Epoch: 44 | Train Loss: 0.615 | Val. Loss: 0.611 |\n","| Epoch: 45 | Train Loss: 0.615 | Val. Loss: 0.610 |\n","| Epoch: 46 | Train Loss: 0.615 | Val. Loss: 0.610 |\n","| Epoch: 47 | Train Loss: 0.615 | Val. Loss: 0.610 |\n","| Epoch: 48 | Train Loss: 0.615 | Val. Loss: 0.610 |\n","| Epoch: 49 | Train Loss: 0.615 | Val. Loss: 0.610 |\n","| Epoch: 50 | Train Loss: 0.615 | Val. Loss: 0.610 |\n","| Epoch: 51 | Train Loss: 0.615 | Val. Loss: 0.610 |\n","| Epoch: 52 | Train Loss: 0.615 | Val. Loss: 0.610 |\n","| Epoch: 53 | Train Loss: 0.615 | Val. Loss: 0.610 |\n","| Epoch: 54 | Train Loss: 0.615 | Val. Loss: 0.610 |\n","| Epoch: 55 | Train Loss: 0.615 | Val. Loss: 0.610 |\n","| Epoch: 56 | Train Loss: 0.615 | Val. Loss: 0.610 |\n","| Epoch: 57 | Train Loss: 0.615 | Val. Loss: 0.610 |\n","| Epoch: 58 | Train Loss: 0.615 | Val. Loss: 0.610 |\n","| Epoch: 59 | Train Loss: 0.614 | Val. Loss: 0.609 |\n","| Epoch: 60 | Train Loss: 0.614 | Val. Loss: 0.609 |\n","| Epoch: 61 | Train Loss: 0.614 | Val. Loss: 0.609 |\n","| Epoch: 62 | Train Loss: 0.613 | Val. Loss: 0.608 |\n","| Epoch: 63 | Train Loss: 0.613 | Val. Loss: 0.608 |\n","| Epoch: 64 | Train Loss: 0.612 | Val. Loss: 0.607 |\n","| Epoch: 65 | Train Loss: 0.612 | Val. Loss: 0.606 |\n","| Epoch: 66 | Train Loss: 0.611 | Val. Loss: 0.605 |\n","| Epoch: 67 | Train Loss: 0.610 | Val. Loss: 0.604 |\n","| Epoch: 68 | Train Loss: 0.609 | Val. Loss: 0.603 |\n","| Epoch: 69 | Train Loss: 0.608 | Val. Loss: 0.602 |\n","| Epoch: 70 | Train Loss: 0.606 | Val. Loss: 0.600 |\n","| Epoch: 71 | Train Loss: 0.605 | Val. Loss: 0.599 |\n","| Epoch: 72 | Train Loss: 0.604 | Val. Loss: 0.597 |\n","| Epoch: 73 | Train Loss: 0.602 | Val. Loss: 0.596 |\n","| Epoch: 74 | Train Loss: 0.601 | Val. Loss: 0.594 |\n","| Epoch: 75 | Train Loss: 0.599 | Val. Loss: 0.593 |\n","| Epoch: 76 | Train Loss: 0.598 | Val. Loss: 0.591 |\n","| Epoch: 77 | Train Loss: 0.596 | Val. Loss: 0.590 |\n","| Epoch: 78 | Train Loss: 0.595 | Val. Loss: 0.588 |\n","| Epoch: 79 | Train Loss: 0.593 | Val. Loss: 0.587 |\n","| Epoch: 80 | Train Loss: 0.592 | Val. Loss: 0.586 |\n","| Epoch: 81 | Train Loss: 0.591 | Val. Loss: 0.584 |\n","| Epoch: 82 | Train Loss: 0.589 | Val. Loss: 0.583 |\n","| Epoch: 83 | Train Loss: 0.588 | Val. Loss: 0.582 |\n","| Epoch: 84 | Train Loss: 0.587 | Val. Loss: 0.581 |\n","| Epoch: 85 | Train Loss: 0.586 | Val. Loss: 0.581 |\n","| Epoch: 86 | Train Loss: 0.586 | Val. Loss: 0.580 |\n","| Epoch: 87 | Train Loss: 0.585 | Val. Loss: 0.579 |\n","| Epoch: 88 | Train Loss: 0.585 | Val. Loss: 0.579 |\n","| Epoch: 89 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 90 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 91 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 92 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 93 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 94 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 95 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 96 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 97 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 98 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 99 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 100 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 101 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 102 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 103 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 104 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 105 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 106 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 107 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 108 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 109 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 110 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 111 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 112 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 113 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 114 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 115 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 116 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 117 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 118 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 119 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 120 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 121 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 122 | Train Loss: 0.584 | Val. Loss: 0.579 |\n","| Epoch: 123 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 124 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 125 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 126 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 127 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 128 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 129 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 130 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 131 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 132 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 133 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 134 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 135 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 136 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 137 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 138 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 139 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 140 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 141 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 142 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 143 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 144 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 145 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 146 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 147 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 148 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 149 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 150 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 151 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 152 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 153 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 154 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 155 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 156 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 157 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 158 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 159 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 160 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 161 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 162 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 163 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 164 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 165 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 166 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 167 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 168 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 169 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 170 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 171 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 172 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 173 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 174 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 175 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 176 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 177 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 178 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 179 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 180 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 181 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 182 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 183 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 184 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 185 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 186 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 187 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 188 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 189 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 190 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 191 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 192 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 193 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 194 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 195 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 196 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 197 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 198 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 199 | Train Loss: 0.584 | Val. Loss: 0.578 |\n","| Epoch: 200 | Train Loss: 0.584 | Val. Loss: 0.578 |\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"v--HYhF6HMBm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":340},"executionInfo":{"status":"ok","timestamp":1593580093495,"user_tz":-60,"elapsed":1457,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"741cf07d-d527-4115-bc92-85c17ef81cd6"},"source":["# run on the test corpus\n","model.eval()\n","\n","test_x_feature = test_origin_tensor.to(DEVICE)\n","test_y_feature = test_new_tensor.to(DEVICE)\n","test_target = test_label_tensor.to(DEVICE)\n","\n","with torch.no_grad():\n","  test_predictions = model(test_x_feature, test_y_feature).squeeze(1)\n","  test_loss = torch.sqrt(((test_predictions - test_target)**2).mean()).item()\n","\n","print(f'| Test Loss: {test_loss:.3f} |')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["test_origin_tensor:\n","tensor([[  87, 2816,  234,  ...,    0,    0,    0],\n","        [ 392, 1532,  425,  ...,    0,    0,    0],\n","        [ 212,    2, 7535,  ...,    0,    0,    0],\n","        ...,\n","        [ 538,  234,  224,  ...,    0,    0,    0],\n","        [4808, 2153, 5571,  ...,    0,    0,    0],\n","        [  58,  429, 1988,  ...,    0,    0,    0]])\n","test_new_tensor:\n","tensor([[  87, 2816,  234,  ...,    0,    0,    0],\n","        [ 392, 1532,  773,  ...,    0,    0,    0],\n","        [ 212,    2, 7535,  ...,    0,    0,    0],\n","        ...,\n","        [ 538,  234,  224,  ...,    0,    0,    0],\n","        [4808, 2153, 5571,  ...,    0,    0,    0],\n","        [  58,  429, 1988,  ...,    0,    0,    0]])\n","test_label_tensor:\n","tensor([1.2000, 0.4000, 1.0000,  ..., 0.4000, 0.0000, 0.8000])\n","| Test Loss: 0.575 |\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ideUIaCPIIj2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1594115768578,"user_tz":-60,"elapsed":685,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"e8d6eb39-6e56-4204-f5dd-12c576fd746e"},"source":["def write_predictions(predictions, test_data_frame, out_loc):\n","    test_data_frame['pred'] = predictions\n","    output = test_data_frame[['id','pred']]\n","    output.to_csv(out_loc, index=False)\n","        \n","    print('Output file created:\\n\\t- '+os.path.abspath(out_loc))\n","\n","\n","# write the predictions for the dev data into 'task-1-output.csv'\n","out_loc = 'gdrive/My Drive/subtask-1/task-1-output.csv'\n","write_predictions(test_predictions, test, out_loc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Output file created:\n","\t- /content/gdrive/My Drive/subtask-1/task-1-output.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oKFbPuoeLd0I","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594115769411,"user_tz":-60,"elapsed":449,"user":{"displayName":"Ziyang Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWwwaBdySTvYwZCvEawSaLjD2UKbeEMt4k7o5U=s64","userId":"08796101430824867754"}},"outputId":"c35398c4-7b07-447f-9d1e-34a0600b402b"},"source":["def score(truth_loc, prediction_loc):\n","    truth = pd.read_csv(truth_loc, usecols=['id','meanGrade'])\n","    pred = pd.read_csv(prediction_loc, usecols=['id','pred'])\n","    \n","    assert(sorted(truth.id) == sorted(pred.id)),\"ID mismatch between ground truth and prediction!\"\n","    \n","    data = pd.merge(truth,pred)\n","    rmse = np.sqrt(np.mean((data['meanGrade'] - data['pred'])**2))\n","    \n","    print(\"RMSE = %.6f\" % rmse)    \n","\n","# print RMSE\n","truth_loc = 'gdrive/My Drive/subtask-1/test.csv'\n","prediction_loc = 'gdrive/My Drive/subtask-1/task-1-output.csv'\n","score(truth_loc, prediction_loc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["RMSE = 0.575020\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AEGy9Bt2WVxz","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}